nohup: ignoring input
[Taichi] mode=release
[Taichi] mode=release
[Taichi] preparing sandbox at /tmp/taichi-8nhafpck
[Taichi] preparing sandbox at /tmp/taichi-19vxrrdv
[Taichi] version 0.7.26, llvm 10.0.0, commit e37bdb5e, linux, python 3.7.11
[Taichi] version 0.7.26, llvm 10.0.0, commit e37bdb5e, linux, python 3.7.11
Loaded compiled 3D CUDA chamfer distance
Loaded compiled 3D CUDA chamfer distance
Message from 1:  	 DEBUG GPU CORE>>>>>>> cuda:1
Message from 0:  	 DEBUG GPU CORE>>>>>>> cuda:0
[Taichi] Starting on arch=cuda
[Taichi] Starting on arch=cuda
Building primitive
action:
  dim: 3
  scale: (0.01, 0.01, 0.01)
color: (0.8, 0.8, 0.8)
friction: 0.9
init_pos: (0.22, 0.015, 0.82)
init_rot: (1.0, 0.0, 0.0, 0.0)
lower_bound: (0.0, 0.0, 0.0)
radius: 0.03
shape: Sphere
upper_bound: (1.0, 1.0, 1.0)
variations: None
Building primitive
action:
  dim: 3
  scale: (0.01, 0.01, 0.01)
color: (0.8, 0.8, 0.8)
friction: 0.9
init_pos: (0.78, 0.015, 0.82)
init_rot: (1.0, 0.0, 0.0, 0.0)
lower_bound: (0.0, 0.0, 0.0)
radius: 0.03
shape: Sphere
upper_bound: (1.0, 1.0, 1.0)
variations: None
Building primitive
action:
  dim: 0
  scale: ()
color: (0.3, 0.3, 0.3)
friction: 0.9
h: 0.1
init_pos: (0.3919300650726247, 0, 0.4990770359432596)
init_rot: (1.0, 0.0, 0.0, 0.0)
lower_bound: (0.0, 0.0, 0.0)
r: 0.2
shape: Cylinder
upper_bound: (1.0, 1.0, 1.0)
variations: None
{'width': (0.6, 0.06, 0.06), 'init_pos': (0.5, 0.03, 0.73), 'color': 38400, 'n_particles': 8192}
Number of particles:  8192
Initialize Renderer
bake_size: 6  
camera_pos: (0.5, 2.5, 2.0)  
camera_rot: (1.0, 0.0)  
dx: 0.006666666666666667  
image_res: (256, 256)  
light_direction: (2.0, 1.0, 0.7)  
max_num_particles: 1000000  
max_ray_depth: 2  
sdf_threshold: 0.20720000000000002  
spp: 50  
target_res: (64, 64, 64)  
use_directional_light: False  
use_roulette: False  
voxel_res: (168, 168, 168)
Building primitive
action:
  dim: 3
  scale: (0.01, 0.01, 0.01)
color: (0.8, 0.8, 0.8)
friction: 0.9
init_pos: (0.22, 0.015, 0.82)
init_rot: (1.0, 0.0, 0.0, 0.0)
lower_bound: (0.0, 0.0, 0.0)
radius: 0.03
shape: Sphere
upper_bound: (1.0, 1.0, 1.0)
variations: None
Building primitive
action:
  dim: 3
  scale: (0.01, 0.01, 0.01)
color: (0.8, 0.8, 0.8)
friction: 0.9
init_pos: (0.78, 0.015, 0.82)
init_rot: (1.0, 0.0, 0.0, 0.0)
lower_bound: (0.0, 0.0, 0.0)
radius: 0.03
shape: Sphere
upper_bound: (1.0, 1.0, 1.0)
variations: None
Building primitive
action:
  dim: 0
  scale: ()
color: (0.3, 0.3, 0.3)
friction: 0.9
h: 0.1
init_pos: (0.3919300650726247, 0, 0.4990770359432596)
init_rot: (1.0, 0.0, 0.0, 0.0)
lower_bound: (0.0, 0.0, 0.0)
r: 0.2
shape: Cylinder
upper_bound: (1.0, 1.0, 1.0)
variations: None
{'width': (0.6, 0.06, 0.06), 'init_pos': (0.5, 0.03, 0.73), 'color': 38400, 'n_particles': 8192}
Number of particles:  8192
Initialize Renderer
bake_size: 6  
camera_pos: (0.5, 2.5, 2.0)  
camera_rot: (1.0, 0.0)  
dx: 0.006666666666666667  
image_res: (256, 256)  
light_direction: (2.0, 1.0, 0.7)  
max_num_particles: 1000000  
max_ray_depth: 2  
sdf_threshold: 0.20720000000000002  
spp: 50  
target_res: (64, 64, 64)  
use_directional_light: False  
use_roulette: False  
voxel_res: (168, 168, 168)
Message from 1:  	 TaichiEnv Number of Particles:8192
Message from 0:  	 TaichiEnv Number of Particles:8192
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@100] [$8906] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@100] [$8908] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@100] [$8910] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@100] [$8912] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@100] [$8914] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@100] [$8916] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@100] [$8918] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@100] [$8920] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@100] [$8922] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.251 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@100] [$8906] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@100] [$8908] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@100] [$8910] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@100] [$8912] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@100] [$8914] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@100] [$8916] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@100] [$8918] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@100] [$8920] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@100] [$8922] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:26.862 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 19:48:32.659 22359] [type_check.cpp:visit@100] [$35471] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:32.659 22359] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 93, in collide
    dist = self.sdf(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 59, in sdf
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 19:48:32.659 22359] [type_check.cpp:visit@100] [$35473] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:32.659 22359] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 93, in collide
    dist = self.sdf(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 59, in sdf
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 19:48:32.659 22359] [type_check.cpp:visit@100] [$35475] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:32.659 22359] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 93, in collide
    dist = self.sdf(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 59, in sdf
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 19:48:32.659 22359] [type_check.cpp:visit@100] [$35887] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:32.659 22359] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 79, in normal
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 19:48:32.659 22359] [type_check.cpp:visit@100] [$35889] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:32.659 22359] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 79, in normal
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 19:48:32.659 22359] [type_check.cpp:visit@100] [$35891] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:32.659 22359] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 79, in normal
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 19:48:32.659 22359] [type_check.cpp:visit@100] [$35951] Local store may lose precision (target = i32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:32.659 22359] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 80, in normal
    return qrot(self.rotation[f], self._normal(f, grid_pos))
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primitives.py", line 178, in _normal
    f = ti.cast(d[0] > d[1], self.dtype)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 135, in wrapped
    return imp_foo(a, b)
[m
[33m[1m[W 09/10/21 19:48:33.064 22358] [type_check.cpp:visit@100] [$35471] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:33.064 22358] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 93, in collide
    dist = self.sdf(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 59, in sdf
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 19:48:33.064 22358] [type_check.cpp:visit@100] [$35473] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:33.064 22358] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 93, in collide
    dist = self.sdf(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 59, in sdf
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 19:48:33.064 22358] [type_check.cpp:visit@100] [$35475] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:33.064 22358] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 93, in collide
    dist = self.sdf(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 59, in sdf
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 19:48:33.064 22358] [type_check.cpp:visit@100] [$35887] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:33.064 22358] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 79, in normal
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 19:48:33.064 22358] [type_check.cpp:visit@100] [$35889] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:33.064 22358] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 79, in normal
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 19:48:33.064 22358] [type_check.cpp:visit@100] [$35891] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:33.064 22358] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 79, in normal
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 19:48:33.064 22358] [type_check.cpp:visit@100] [$35951] Local store may lose precision (target = i32, value = f64) at[m
[33m[1m[W 09/10/21 19:48:33.064 22358] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 80, in normal
    return qrot(self.rotation[f], self._normal(f, grid_pos))
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primitives.py", line 178, in _normal
    f = ti.cast(d[0] > d[1], self.dtype)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 135, in wrapped
    return imp_foo(a, b)
[m
Message from 0:  	 Batch:0, loss:0.045400284230709076
Message from 0:  	 Batch:1, loss:0.04237883538007736
Message from 0:  	 Batch:2, loss:0.05291919410228729
Message from 0:  	 Batch:3, loss:0.0384254977107048
Message from 0:  	 Batch:4, loss:0.039670828729867935
Message from 0:  	 Batch:5, loss:0.04052010551095009
Message from 0:  	 Batch:6, loss:0.05121122673153877
Message from 0:  	 Batch:7, loss:0.044405482709407806
Message from 0:  	 Batch:8, loss:0.044303566217422485
Message from 0:  	 Batch:9, loss:0.03740384429693222
Message from 0:  	 Batch:10, loss:0.08878156542778015
Message from 0:  	 Batch:11, loss:0.037655092775821686
Message from 0:  	 Batch:12, loss:0.04142902046442032
Message from 0:  	 Batch:13, loss:0.04730747640132904
Message from 0:  	 Batch:14, loss:0.04578603804111481
Message from 0:  	 Batch:15, loss:0.04589269310235977
Message from 0:  	 Batch:16, loss:0.041738055646419525
Message from 0:  	 Batch:17, loss:0.051753029227256775
Message from 0:  	 Batch:18, loss:0.0547427237033844
Message from 0:  	 Batch:19, loss:0.05212932452559471
Message from 0:  	 Batch:20, loss:0.05627623200416565
Message from 0:  	 Batch:21, loss:0.07129479944705963
Message from 0:  	 Batch:22, loss:0.048745542764663696
Message from 0:  	 Batch:23, loss:0.04821699857711792
Message from 0:  	 Batch:24, loss:0.06089125946164131
Message from 0:  	 Batch:25, loss:0.0413828119635582
Message from 0:  	 Batch:26, loss:0.03644508123397827
Message from 0:  	 Batch:27, loss:0.06418824195861816
Message from 0:  	 Batch:28, loss:0.032753050327301025
Message from 0:  	 Batch:29, loss:0.04483925551176071
Message from 0:  	 Batch:30, loss:0.03369951993227005
Message from 0:  	 Batch:31, loss:0.04747498035430908
Message from 0:  	 Batch:32, loss:0.042181096971035004
Message from 0:  	 Batch:33, loss:0.031452618539333344
Message from 0:  	 Batch:34, loss:0.0491202287375927
Message from 0:  	 Batch:35, loss:0.03284015506505966
Message from 0:  	 Batch:36, loss:0.04516157507896423
Message from 0:  	 Batch:37, loss:0.043261803686618805
Message from 0:  	 Batch:38, loss:0.04490232095122337
Message from 0:  	 Batch:39, loss:0.054260119795799255
Message from 0:  	 Batch:40, loss:0.05452769622206688
Message from 0:  	 Batch:41, loss:0.04900331050157547
Message from 0:  	 Batch:42, loss:0.04829132556915283
Message from 0:  	 Batch:43, loss:0.041998520493507385
Message from 0:  	 Batch:44, loss:0.042241379618644714
Message from 0:  	 Batch:45, loss:0.04433372616767883
Message from 0:  	 Batch:46, loss:0.051105014979839325
Message from 0:  	 Batch:47, loss:0.04811522737145424
Message from 0:  	 Batch:48, loss:0.046667784452438354
Message from 0:  	 Batch:49, loss:0.0617721863090992
Message from 0:  	 Batch:50, loss:0.03825695067644119
Message from 0:  	 Batch:51, loss:0.0458201989531517
Message from 0:  	 Batch:52, loss:0.053618744015693665
Message from 0:  	 Batch:53, loss:0.043926820158958435
Message from 0:  	 Batch:54, loss:0.03620760142803192
Message from 0:  	 Batch:55, loss:0.049485452473163605
Message from 0:  	 Batch:56, loss:0.04201453924179077
Message from 0:  	 Batch:57, loss:0.049253810197114944
Message from 0:  	 Batch:58, loss:0.04592939466238022
Message from 0:  	 Batch:59, loss:0.053609449416399
Message from 0:  	 Batch:60, loss:0.04540693014860153
Message from 0:  	 Batch:61, loss:0.04169221967458725
Message from 0:  	 Batch:62, loss:0.050026677548885345
Message from 0:  	 Batch:63, loss:0.05808102339506149
Message from 0:  	 Batch:64, loss:0.04558642953634262
Message from 0:  	 Batch:65, loss:0.05549883097410202
Message from 0:  	 Batch:66, loss:0.05126585438847542
Message from 0:  	 Batch:67, loss:0.044552914798259735
Message from 0:  	 Batch:68, loss:0.064535953104496
Message from 0:  	 Batch:69, loss:0.042691342532634735
Message from 0:  	 Batch:70, loss:0.046867914497852325
Message from 0:  	 Batch:71, loss:0.034567855298519135
Message from 0:  	 Batch:72, loss:0.048955291509628296
Message from 0:  	 Batch:73, loss:0.0605446919798851
Message from 0:  	 Batch:74, loss:0.056746236979961395
Message from 0:  	 Batch:75, loss:0.06165824458003044
Message from 0:  	 Batch:76, loss:0.05278259888291359
Message from 0:  	 Batch:77, loss:0.04595935344696045
Message from 0:  	 Batch:78, loss:0.04542020335793495
Message from 0:  	 Batch:79, loss:0.047555066645145416
Message from 0:  	 Batch:80, loss:0.034596607089042664
Message from 0:  	 Batch:81, loss:0.05374393239617348
Message from 0:  	 Batch:82, loss:0.04596421867609024
Message from 0:  	 Batch:83, loss:0.051363829523324966
Message from 0:  	 Batch:84, loss:0.04596428945660591
Message from 0:  	 Batch:85, loss:0.050360873341560364
Message from 0:  	 Batch:86, loss:0.04850166290998459
Message from 0:  	 Batch:87, loss:0.04803360626101494
Message from 0:  	 Batch:88, loss:0.045005373656749725
Message from 0:  	 Batch:89, loss:0.047066960483789444
Message from 0:  	 Batch:90, loss:0.047814760357141495
Message from 0:  	 Batch:91, loss:0.05027637630701065
Message from 0:  	 Batch:92, loss:0.03579266369342804
Message from 0:  	 Batch:93, loss:0.04905987158417702
Message from 0:  	 Batch:94, loss:0.03603381663560867
Message from 0:  	 Batch:95, loss:0.05126051604747772
Message from 0:  	 Batch:96, loss:0.04822611063718796
Message from 0:  	 Batch:97, loss:0.04036978632211685
Message from 0:  	 Batch:98, loss:0.046066150069236755
Message from 0:  	 Batch:99, loss:0.046340636909008026
Message from 0:  	 Batch:100, loss:0.04863516986370087
Message from 0:  	 Batch:101, loss:0.04516714811325073
Message from 0:  	 Batch:102, loss:0.048914894461631775
Message from 0:  	 Batch:103, loss:0.06263820827007294
Message from 0:  	 Batch:104, loss:0.035376161336898804
Message from 0:  	 Batch:105, loss:0.0721932053565979
Message from 0:  	 Batch:106, loss:0.046181902289390564
Message from 0:  	 Batch:107, loss:0.0461430698633194
Message from 0:  	 Batch:108, loss:0.043239496648311615
Message from 0:  	 Batch:109, loss:0.030335072427988052
Message from 0:  	 Batch:110, loss:0.0425306111574173
Message from 0:  	 Batch:111, loss:0.08157207816839218
Message from 0:  	 Batch:112, loss:0.05254483222961426
Message from 0:  	 Batch:113, loss:0.07386988401412964
Message from 0:  	 Batch:114, loss:0.03683430701494217
Message from 0:  	 Batch:115, loss:0.03782215714454651
Message from 0:  	 Batch:116, loss:0.044194307178258896
Message from 0:  	 Batch:117, loss:0.042718373239040375
Message from 0:  	 Batch:118, loss:0.0628431886434555
Message from 0:  	 Batch:119, loss:0.04553348943591118
Message from 0:  	 Batch:120, loss:0.06468451023101807
Message from 0:  	 Batch:121, loss:0.04719720408320427
Message from 0:  	 Batch:122, loss:0.04766286164522171
Message from 0:  	 Batch:123, loss:0.042816564440727234
Message from 0:  	 Batch:124, loss:0.07142230123281479
Message from 0:  	 Batch:125, loss:0.05832261964678764
Message from 0:  	 Batch:126, loss:0.04861924424767494
Message from 0:  	 Batch:127, loss:0.05870852246880531
Message from 0:  	 Batch:128, loss:0.043887991458177567
Message from 0:  	 Batch:129, loss:0.053992729634046555
Message from 0:  	 Batch:130, loss:0.04421810805797577
Message from 0:  	 Batch:131, loss:0.05333544313907623
Message from 0:  	 Batch:132, loss:0.050751179456710815
Message from 0:  	 Batch:133, loss:0.04073842614889145
Message from 0:  	 Batch:134, loss:0.03216340392827988
Message from 0:  	 Batch:135, loss:0.03997171297669411
Message from 0:  	 Batch:136, loss:0.04634271562099457
Message from 0:  	 Batch:137, loss:0.03834817185997963
Message from 0:  	 Batch:138, loss:0.06380325555801392
Message from 0:  	 Batch:139, loss:0.04635561630129814
Message from 0:  	 Batch:140, loss:0.04901512712240219
Message from 0:  	 Batch:141, loss:0.06052659824490547
Message from 0:  	 Batch:142, loss:0.04853750765323639
Message from 0:  	 Batch:143, loss:0.04909207299351692
Message from 0:  	 Batch:144, loss:0.05236057937145233
Message from 0:  	 Batch:145, loss:0.0871582180261612
Message from 0:  	 Batch:146, loss:0.03590528666973114
Message from 0:  	 Batch:147, loss:0.057908497750759125
Message from 0:  	 Batch:148, loss:0.04613310098648071
Message from 0:  	 Batch:149, loss:0.06835643947124481
Message from 0:  	 Batch:150, loss:0.04782263934612274
Message from 0:  	 Batch:151, loss:0.04918321222066879
Message from 0:  	 Batch:152, loss:0.08248721063137054
Message from 0:  	 Batch:153, loss:0.04452686384320259
Message from 0:  	 Batch:154, loss:0.06403136253356934
Message from 0:  	 Batch:155, loss:0.033753179013729095
Message from 0:  	 Batch:156, loss:0.048109397292137146
Message from 0:  	 Batch:157, loss:0.06470668315887451
Message from 0:  	 Batch:158, loss:0.043049946427345276
Message from 0:  	 Batch:159, loss:0.045816220343112946
Message from 0:  	 Batch:160, loss:0.059521548449993134
Message from 0:  	 Batch:161, loss:0.040705606341362
Message from 0:  	 Batch:162, loss:0.03376534581184387
Message from 0:  	 Batch:163, loss:0.042173415422439575
Message from 0:  	 Batch:164, loss:0.043284378945827484
Message from 0:  	 Batch:165, loss:0.04146592319011688
Message from 0:  	 Batch:166, loss:0.05485282093286514
Message from 0:  	 Batch:167, loss:0.044048041105270386
Message from 0:  	 Batch:168, loss:0.0433584600687027
Message from 0:  	 Batch:169, loss:0.03975778445601463
Message from 0:  	 Batch:170, loss:0.04783332347869873
Message from 0:  	 Batch:171, loss:0.047300685197114944
Message from 0:  	 Batch:172, loss:0.07326225936412811
Message from 0:  	 Batch:173, loss:0.03628259897232056
Message from 0:  	 Batch:174, loss:0.0493566058576107
Message from 0:  	 Batch:175, loss:0.04337689280509949
Message from 0:  	 Batch:176, loss:0.03820746764540672
Message from 0:  	 Batch:177, loss:0.04324902966618538
Message from 0:  	 Batch:178, loss:0.04253317415714264
Message from 0:  	 Batch:179, loss:0.043590471148490906
Message from 0:  	 Batch:180, loss:0.04418861120939255
Message from 0:  	 Batch:181, loss:0.04834602028131485
Message from 0:  	 Batch:182, loss:0.03340783342719078
Message from 0:  	 Batch:183, loss:0.05687309056520462
Message from 0:  	 Batch:184, loss:0.043221354484558105
Message from 0:  	 Batch:185, loss:0.053884588181972504
Message from 0:  	 Batch:186, loss:0.03368230536580086
Message from 0:  	 Batch:187, loss:0.03654078021645546
Message from 0:  	 Batch:188, loss:0.04416259005665779
Message from 0:  	 Batch:189, loss:0.06859974563121796
Message from 0:  	 Batch:190, loss:0.04492362216114998
Message from 0:  	 Batch:191, loss:0.04622487723827362
Message from 0:  	 Batch:192, loss:0.047099508345127106
Message from 0:  	 Batch:193, loss:0.046926967799663544
Message from 0:  	 Batch:194, loss:0.05040258169174194
Message from 0:  	 Batch:195, loss:0.044846102595329285
Message from 0:  	 Batch:196, loss:0.05449919402599335
Message from 0:  	 Batch:197, loss:0.0700022354722023
Message from 0:  	 Batch:198, loss:0.03616862744092941
Message from 0:  	 Batch:199, loss:0.05316215753555298
Message from 0:  	 Batch:200, loss:0.06831569969654083
Message from 0:  	 Batch:201, loss:0.060404788702726364
Message from 0:  	 Batch:202, loss:0.04851248115301132
Message from 0:  	 Batch:203, loss:0.047437191009521484
Message from 0:  	 Batch:204, loss:0.04519215226173401
Message from 0:  	 Batch:205, loss:0.04115278273820877
Message from 0:  	 Batch:206, loss:0.03740224614739418
Message from 0:  	 Batch:207, loss:0.04036886990070343
Message from 0:  	 Batch:208, loss:0.04023246467113495
Message from 0:  	 Batch:209, loss:0.04774327948689461
Message from 0:  	 Batch:210, loss:0.04897923767566681
Message from 0:  	 Batch:211, loss:0.047800615429878235
Message from 0:  	 Batch:212, loss:0.05265398323535919
Message from 0:  	 Batch:213, loss:0.03849424421787262
Message from 0:  	 Batch:214, loss:0.03675169125199318
Message from 0:  	 Batch:215, loss:0.045960683375597
Message from 0:  	 Batch:216, loss:0.073410764336586
Message from 0:  	 Batch:217, loss:0.0358310230076313
Message from 0:  	 Batch:218, loss:0.047818802297115326
Message from 0:  	 Batch:219, loss:0.07747408747673035
Message from 0:  	 Batch:220, loss:0.043629325926303864
Message from 0:  	 Batch:221, loss:0.038403742015361786
Message from 0:  	 Batch:222, loss:0.07309205830097198
Message from 0:  	 Batch:223, loss:0.0322367362678051
Message from 0:  	 Batch:224, loss:0.03389503061771393
Message from 0:  	 Batch:225, loss:0.042168982326984406
Message from 0:  	 Batch:226, loss:0.04601706191897392
Message from 0:  	 Batch:227, loss:0.03913096711039543
Message from 0:  	 Batch:228, loss:0.04272399842739105
Message from 0:  	 Batch:229, loss:0.05271371081471443
Message from 0:  	 Batch:230, loss:0.04992249235510826
Message from 0:  	 Batch:231, loss:0.051514819264411926
Message from 0:  	 Batch:232, loss:0.040615782141685486
Message from 0:  	 Batch:233, loss:0.05186837539076805
Message from 0:  	 Batch:234, loss:0.04976293444633484
Message from 0:  	 Batch:235, loss:0.044467851519584656
Message from 0:  	 Batch:236, loss:0.04357136785984039
Message from 0:  	 Batch:237, loss:0.06775660812854767
Message from 0:  	 Batch:238, loss:0.08018054068088531
Message from 0:  	 Batch:239, loss:0.04200958088040352
Message from 0:  	 Batch:240, loss:0.052091192454099655
Message from 0:  	 Batch:241, loss:0.043687012046575546
Message from 0:  	 Batch:242, loss:0.04033809155225754
Message from 0:  	 Batch:243, loss:0.04910171031951904
Message from 0:  	 Batch:244, loss:0.05490892380475998
Message from 0:  	 Batch:245, loss:0.04269406944513321
Message from 0:  	 Batch:246, loss:0.04425975680351257
Message from 0:  	 Batch:247, loss:0.049937937408685684
Message from 0:  	 Batch:248, loss:0.04492264986038208
Message from 0:  	 Batch:249, loss:0.035371262580156326
Message from 0:  	 Batch:250, loss:0.05391228199005127
Message from 0:  	 Batch:251, loss:0.04606998711824417
Message from 0:  	 Batch:252, loss:0.04673841595649719
Message from 0:  	 Batch:253, loss:0.04590675234794617
Message from 0:  	 Batch:254, loss:0.052823010832071304
Message from 0:  	 Batch:255, loss:0.05507666617631912
Message from 0:  	 Batch:256, loss:0.04730478301644325
Message from 0:  	 Batch:257, loss:0.047332148998975754
Message from 0:  	 Batch:258, loss:0.029614288359880447
Message from 0:  	 Batch:259, loss:0.050449274480342865
Message from 0:  	 Batch:260, loss:0.07780016958713531
Message from 0:  	 Batch:261, loss:0.04887785762548447
Message from 0:  	 Batch:262, loss:0.05539042875170708
Message from 0:  	 Batch:263, loss:0.04642947018146515
Message from 0:  	 Batch:264, loss:0.033809009939432144
Message from 0:  	 Batch:265, loss:0.06234370917081833
Message from 0:  	 Batch:266, loss:0.04712498560547829
Message from 0:  	 Batch:267, loss:0.03882703185081482
Message from 0:  	 Batch:268, loss:0.04816393554210663
Message from 0:  	 Batch:269, loss:0.0697154551744461
Message from 0:  	 Batch:270, loss:0.03832927718758583
Message from 0:  	 Batch:271, loss:0.047472987323999405
Message from 0:  	 Batch:272, loss:0.04910959303379059
Message from 0:  	 Batch:273, loss:0.05843237787485123
Message from 0:  	 Batch:274, loss:0.07490291446447372
Message from 0:  	 Batch:275, loss:0.045755185186862946
Message from 0:  	 Batch:276, loss:0.08061327785253525
Message from 0:  	 Batch:277, loss:0.03653563931584358
Message from 0:  	 Batch:278, loss:0.058030106127262115
Message from 0:  	 Batch:279, loss:0.09423510730266571
Message from 0:  	 Batch:280, loss:0.06311679631471634
Message from 0:  	 Batch:281, loss:0.051347579807043076
Message from 0:  	 Batch:282, loss:0.04576883465051651
Message from 0:  	 Batch:283, loss:0.04219222813844681
Message from 0:  	 Batch:284, loss:0.052794165909290314
Message from 0:  	 Batch:285, loss:0.050720129162073135
Message from 0:  	 Batch:286, loss:0.04136497527360916
Message from 0:  	 Batch:287, loss:0.0458315834403038
Message from 0:  	 Batch:288, loss:0.05890972167253494
Message from 0:  	 Batch:289, loss:0.04445281997323036
Message from 0:  	 Batch:290, loss:0.042221635580062866
Message from 0:  	 Batch:291, loss:0.04680916666984558
Message from 0:  	 Batch:292, loss:0.06437403708696365
Message from 0:  	 Batch:293, loss:0.050255075097084045
Message from 0:  	 Batch:294, loss:0.047893501818180084
Message from 0:  	 Batch:295, loss:0.050490304827690125
Message from 0:  	 Batch:296, loss:0.0671139732003212
Message from 0:  	 Batch:297, loss:0.06387002766132355
Message from 0:  	 Batch:298, loss:0.05270155519247055
Message from 0:  	 Batch:299, loss:0.05448180064558983
Message from 0:  	 Batch:300, loss:0.04528766870498657
Message from 0:  	 Batch:301, loss:0.0479726567864418
Message from 0:  	 Batch:302, loss:0.03323753550648689
Message from 0:  	 Batch:303, loss:0.0519818440079689
Message from 0:  	 Batch:304, loss:0.04108380153775215
Message from 0:  	 Batch:305, loss:0.07990121096372604
Message from 0:  	 Batch:306, loss:0.04957097768783569
Message from 0:  	 Batch:307, loss:0.05970132350921631
Message from 0:  	 Batch:308, loss:0.05740681290626526
Message from 0:  	 Batch:309, loss:0.04416058585047722
Message from 0:  	 Batch:310, loss:0.0424003079533577
Message from 0:  	 Batch:311, loss:0.0567341223359108
Message from 0:  	 Batch:312, loss:0.05952584743499756
Message from 0:  	 Batch:313, loss:0.05017760396003723
Message from 0:  	 Batch:314, loss:0.06237570196390152
Message from 0:  	 Batch:315, loss:0.021811828017234802
Message from 0:  	 Batch:316, loss:0.049489449709653854
Message from 0:  	 Batch:317, loss:0.04687567800283432
Message from 0:  	 Batch:318, loss:0.06080026924610138
Message from 0:  	 Batch:319, loss:0.0493159294128418
Message from 0:  	 Batch:320, loss:0.04904206842184067
Message from 0:  	 Batch:321, loss:0.03396967798471451
Message from 0:  	 Batch:322, loss:0.03515259176492691
Message from 0:  	 Batch:323, loss:0.0466928631067276
Message from 0:  	 Batch:324, loss:0.04426015168428421
Message from 0:  	 Batch:325, loss:0.051124900579452515
Message from 0:  	 Batch:326, loss:0.04385370388627052
Message from 0:  	 Batch:327, loss:0.044355250895023346
Message from 0:  	 Batch:328, loss:0.057894837111234665
Message from 0:  	 Batch:329, loss:0.050558313727378845
Message from 0:  	 Batch:330, loss:0.04556562379002571
Message from 0:  	 Batch:331, loss:0.041303884238004684
Message from 0:  	 Batch:332, loss:0.04193420708179474
Message from 0:  	 Batch:333, loss:0.040964819490909576
Message from 0:  	 Batch:334, loss:0.05908823013305664
Message from 0:  	 Batch:335, loss:0.049156930297613144
Message from 0:  	 Batch:336, loss:0.05999322235584259
Message from 0:  	 Batch:337, loss:0.04897456616163254
Message from 0:  	 Batch:338, loss:0.055365413427352905
Message from 0:  	 Batch:339, loss:0.04458729550242424
Message from 0:  	 Batch:340, loss:0.05491696670651436
Message from 0:  	 Batch:341, loss:0.059657223522663116
Message from 0:  	 Batch:342, loss:0.03919766843318939
Message from 0:  	 Batch:343, loss:0.06133363023400307
Message from 0:  	 Batch:344, loss:0.03383354842662811
Message from 0:  	 Batch:345, loss:0.047483526170253754
Message from 0:  	 Batch:346, loss:0.043905794620513916
Message from 0:  	 Batch:347, loss:0.06784110516309738
Message from 0:  	 Batch:348, loss:0.06344380229711533
Message from 0:  	 Batch:349, loss:0.05524282157421112
Message from 0:  	 Batch:350, loss:0.04594915360212326
Message from 0:  	 Batch:351, loss:0.04680676385760307
Message from 0:  	 Batch:352, loss:0.04268472641706467
Message from 0:  	 Batch:353, loss:0.051346808671951294
Message from 0:  	 Batch:354, loss:0.03902697563171387
Message from 0:  	 Batch:355, loss:0.06152639538049698
Message from 0:  	 Batch:356, loss:0.04713845252990723
Message from 0:  	 Batch:357, loss:0.045313477516174316
Message from 0:  	 Batch:358, loss:0.04698888212442398
Message from 0:  	 Batch:359, loss:0.04949362576007843
Message from 0:  	 Batch:360, loss:0.05466639995574951
Message from 0:  	 Batch:361, loss:0.0514330118894577
Message from 0:  	 Batch:362, loss:0.04342829808592796
Message from 0:  	 Batch:363, loss:0.06138160452246666
Message from 0:  	 Batch:364, loss:0.05269031226634979
Message from 0:  	 Batch:365, loss:0.07016460597515106
Message from 0:  	 Batch:366, loss:0.04849311709403992
Message from 0:  	 Batch:367, loss:0.0781729593873024
Message from 0:  	 Batch:368, loss:0.039236679673194885
Message from 0:  	 Batch:369, loss:0.08955244719982147
Message from 0:  	 Batch:370, loss:0.04672696068882942
Message from 0:  	 Batch:371, loss:0.04228173941373825
Message from 0:  	 Batch:372, loss:0.04293340444564819
Message from 0:  	 Batch:373, loss:0.042100511491298676
Message from 0:  	 Batch:374, loss:0.05105636268854141
Message from 0:  	 Batch:375, loss:0.04610223323106766
Message from 0:  	 Batch:376, loss:0.04427892714738846
Message from 0:  	 Batch:377, loss:0.04266301542520523
Message from 0:  	 Batch:378, loss:0.04371366649866104
Message from 0:  	 Batch:379, loss:0.05018031969666481
Message from 0:  	 Batch:380, loss:0.04617314040660858
Message from 0:  	 Batch:381, loss:0.049452006816864014
Message from 0:  	 Batch:382, loss:0.0422259084880352
Message from 0:  	 Batch:383, loss:0.041099898517131805
Message from 0:  	 Batch:384, loss:0.04806678369641304
Message from 0:  	 Batch:385, loss:0.04643075913190842
Message from 0:  	 Batch:386, loss:0.04021229222416878
Message from 0:  	 Batch:387, loss:0.04782307893037796
Message from 0:  	 Batch:388, loss:0.06571322679519653
Message from 0:  	 Batch:389, loss:0.046690456569194794
Message from 0:  	 Batch:390, loss:0.04602644592523575
Message from 0:  	 Batch:391, loss:0.047137703746557236
Message from 0:  	 Batch:392, loss:0.03209357708692551
Message from 0:  	 Batch:393, loss:0.04545212909579277
Message from 0:  	 Batch:394, loss:0.1002078577876091
Message from 0:  	 Batch:395, loss:0.044831566512584686
Message from 0:  	 Batch:396, loss:0.0384729765355587
Message from 0:  	 Batch:397, loss:0.046797435730695724
Message from 0:  	 Batch:398, loss:0.0538337379693985
Message from 0:  	 Batch:399, loss:0.043470919132232666
Message from 0:  	 Batch:400, loss:0.04931974783539772
Message from 0:  	 Batch:401, loss:0.0787864625453949
Message from 0:  	 Batch:402, loss:0.03316961228847504
Message from 0:  	 Batch:403, loss:0.05924133211374283
Message from 0:  	 Batch:404, loss:0.04711805284023285
Message from 0:  	 Batch:405, loss:0.04542161524295807
Message from 0:  	 Batch:406, loss:0.0416162870824337
Message from 0:  	 Batch:407, loss:0.08159588277339935
Message from 0:  	 Batch:408, loss:0.0515180341899395
Message from 0:  	 Batch:409, loss:0.03614562004804611
Message from 0:  	 Batch:410, loss:0.06380447745323181
Message from 0:  	 Batch:411, loss:0.04282209277153015
Message from 0:  	 Batch:412, loss:0.048554033041000366
Message from 0:  	 Batch:413, loss:0.05508438125252724
Message from 0:  	 Batch:414, loss:0.04959786310791969
Message from 0:  	 Batch:415, loss:0.04450619965791702
Message from 0:  	 Batch:416, loss:0.048418790102005005
Message from 0:  	 Batch:417, loss:0.04876411333680153
Message from 0:  	 Batch:418, loss:0.05164577066898346
Message from 0:  	 Batch:419, loss:0.07065097242593765
Message from 0:  	 Batch:420, loss:0.05759318917989731
Message from 0:  	 Batch:421, loss:0.04721590504050255
Message from 0:  	 Batch:422, loss:0.05247682332992554
Message from 0:  	 Batch:423, loss:0.060982078313827515
Message from 0:  	 Batch:424, loss:0.05050137639045715
Message from 0:  	 Batch:425, loss:0.046994343400001526
Message from 0:  	 Batch:426, loss:0.03694457560777664
Message from 0:  	 Batch:427, loss:0.039399974048137665
Message from 0:  	 Batch:428, loss:0.04678471013903618
Message from 0:  	 Batch:429, loss:0.05152331665158272
Message from 0:  	 Batch:430, loss:0.04750856012105942
Message from 0:  	 Batch:431, loss:0.04347490519285202
Message from 0:  	 Batch:432, loss:0.06292001903057098
Message from 0:  	 Batch:433, loss:0.05037187039852142
Message from 0:  	 Batch:434, loss:0.045472294092178345
Message from 0:  	 Batch:435, loss:0.06404100358486176
Message from 0:  	 Batch:436, loss:0.03757328540086746
Message from 0:  	 Batch:437, loss:0.054293423891067505
Message from 0:  	 Batch:438, loss:0.0335085354745388
Message from 0:  	 Batch:439, loss:0.052912868559360504
Message from 0:  	 Batch:440, loss:0.05936238914728165
Message from 0:  	 Batch:441, loss:0.07363463938236237
Message from 0:  	 Batch:442, loss:0.04215684533119202
Message from 0:  	 Batch:443, loss:0.06039082258939743
Message from 0:  	 Batch:444, loss:0.0454680509865284
Message from 0:  	 Batch:445, loss:0.04893534258008003
Message from 0:  	 Batch:446, loss:0.04935114085674286
Message from 0:  	 Batch:447, loss:0.03287791460752487
Message from 0:  	 Batch:448, loss:0.041067346930503845
Message from 0:  	 Batch:449, loss:0.04703846573829651
Message from 0:  	 Batch:450, loss:0.0393076166510582
Message from 0:  	 Batch:451, loss:0.04608776420354843
Message from 0:  	 Batch:452, loss:0.041643913835287094
Message from 0:  	 Batch:453, loss:0.04870028793811798
Message from 0:  	 Batch:454, loss:0.04601048678159714
Message from 0:  	 Batch:455, loss:0.058663323521614075
Message from 0:  	 Batch:456, loss:0.04743564501404762
Message from 0:  	 Batch:457, loss:0.08017410337924957
Message from 0:  	 Batch:458, loss:0.04573363810777664
Message from 0:  	 Batch:459, loss:0.047129176557064056
Message from 0:  	 Batch:460, loss:0.06231114640831947
Message from 0:  	 Batch:461, loss:0.046720001846551895
Message from 0:  	 Batch:462, loss:0.04689342901110649
Message from 0:  	 Batch:463, loss:0.05758613348007202
Message from 0:  	 Batch:464, loss:0.047352589666843414
Message from 0:  	 Batch:465, loss:0.03899196535348892
Message from 0:  	 Batch:466, loss:0.045628879219293594
Message from 0:  	 Batch:467, loss:0.04887227341532707
Message from 0:  	 Batch:468, loss:0.03310481086373329
Message from 0:  	 Batch:469, loss:0.04971788078546524
Message from 0:  	 Batch:470, loss:0.0644851103425026
Message from 0:  	 Batch:471, loss:0.05105702579021454
Message from 0:  	 Batch:472, loss:0.04337479919195175
Message from 0:  	 Batch:473, loss:0.0420081652700901
Message from 0:  	 Batch:474, loss:0.06009192392230034
Message from 0:  	 Batch:475, loss:0.04328703135251999
Message from 0:  	 Batch:476, loss:0.054843656718730927
Message from 0:  	 Batch:477, loss:0.03411788120865822
Message from 0:  	 Batch:478, loss:0.0430854931473732
Message from 0:  	 Batch:479, loss:0.03473138064146042
Message from 0:  	 Batch:480, loss:0.04949649050831795
Message from 0:  	 Batch:481, loss:0.04171375185251236
Message from 0:  	 Batch:482, loss:0.05335277318954468
Message from 0:  	 Batch:483, loss:0.042564842849969864
Message from 0:  	 Batch:484, loss:0.045215725898742676
Message from 0:  	 Batch:485, loss:0.033003102988004684
Message from 0:  	 Batch:486, loss:0.03381681442260742
Message from 0:  	 Batch:487, loss:0.04555074870586395
Message from 0:  	 Batch:488, loss:0.04548969119787216
Message from 0:  	 Batch:489, loss:0.05003410577774048
Message from 0:  	 Batch:490, loss:0.07888354361057281
Message from 0:  	 Batch:491, loss:0.05112454295158386
Message from 0:  	 Batch:492, loss:0.04633152484893799
Message from 0:  	 Batch:493, loss:0.04432830587029457
Message from 0:  	 Batch:494, loss:0.047664422541856766
Message from 0:  	 Batch:495, loss:0.07333500683307648
Message from 0:  	 Batch:496, loss:0.06167200952768326
Message from 0:  	 Batch:497, loss:0.08379867672920227
Message from 0:  	 Batch:498, loss:0.06609596312046051
Message from 0:  	 Batch:499, loss:0.05323609709739685
Message from 0:  	 Batch:500, loss:0.05804632604122162
Message from 0:  	 Batch:501, loss:0.08170700818300247
Message from 0:  	 Batch:502, loss:0.031522396951913834
Message from 0:  	 Batch:503, loss:0.044869475066661835
Message from 0:  	 Batch:504, loss:0.04384075477719307
Message from 0:  	 Batch:505, loss:0.03997883200645447
Message from 0:  	 Batch:506, loss:0.040293268859386444
Message from 0:  	 Batch:507, loss:0.04467314854264259
Message from 0:  	 Batch:508, loss:0.05440167710185051
Message from 0:  	 Batch:509, loss:0.043353527784347534
Message from 0:  	 Batch:510, loss:0.04639476537704468
Message from 0:  	 Batch:511, loss:0.08490258455276489
Message from 0:  	 Batch:512, loss:0.045721493661403656
Message from 0:  	 Batch:513, loss:0.05700518935918808
Message from 0:  	 Batch:514, loss:0.05868959054350853
Message from 0:  	 Batch:515, loss:0.04494796693325043
Message from 0:  	 Batch:516, loss:0.052170075476169586
Message from 0:  	 Batch:517, loss:0.043994151055812836
Message from 0:  	 Batch:518, loss:0.0574318990111351
Message from 0:  	 Batch:519, loss:0.0708654373884201
Message from 0:  	 Batch:520, loss:0.047517940402030945
Message from 0:  	 Batch:521, loss:0.04714898765087128
Message from 0:  	 Batch:522, loss:0.050239719450473785
Message from 0:  	 Batch:523, loss:0.054482147097587585
Message from 0:  	 Batch:524, loss:0.050450392067432404
Message from 0:  	 Batch:525, loss:0.036344245076179504
Message from 0:  	 Batch:526, loss:0.06695404648780823
Message from 0:  	 Batch:527, loss:0.045998714864254
Message from 0:  	 Batch:528, loss:0.04706776887178421
Message from 0:  	 Batch:529, loss:0.043886564671993256
Message from 0:  	 Batch:530, loss:0.04397758096456528
Message from 0:  	 Batch:531, loss:0.04118905961513519
Message from 0:  	 Batch:532, loss:0.047112688422203064
Message from 0:  	 Batch:533, loss:0.04144769534468651
Message from 0:  	 Batch:534, loss:0.04598206654191017
Message from 0:  	 Batch:535, loss:0.045837871730327606
Message from 0:  	 Batch:536, loss:0.04659202694892883
Message from 0:  	 Batch:537, loss:0.06202711910009384
Message from 0:  	 Batch:538, loss:0.0638720840215683
Message from 0:  	 Batch:539, loss:0.04831523075699806
Message from 0:  	 Batch:540, loss:0.04247153550386429
Message from 0:  	 Batch:541, loss:0.044970475137233734
Message from 0:  	 Batch:542, loss:0.04903455078601837
Message from 0:  	 Batch:543, loss:0.05608838051557541
Message from 0:  	 Batch:544, loss:0.04903966933488846
Message from 0:  	 Batch:545, loss:0.04976342245936394
Message from 0:  	 Batch:546, loss:0.03817819803953171
Message from 0:  	 Batch:547, loss:0.043751999735832214
Message from 0:  	 Batch:548, loss:0.04774435609579086
Message from 0:  	 Batch:549, loss:0.04709935933351517
Message from 0:  	 Batch:550, loss:0.06782739609479904
Message from 0:  	 Batch:551, loss:0.048609890043735504
Message from 0:  	 Batch:552, loss:0.04232290759682655
Message from 0:  	 Batch:553, loss:0.07020104676485062
Message from 0:  	 Batch:554, loss:0.05048000067472458
Message from 0:  	 Batch:555, loss:0.04573660343885422
Message from 0:  	 Batch:556, loss:0.03908182680606842
Message from 0:  	 Batch:557, loss:0.045943278819322586
Message from 0:  	 Batch:558, loss:0.04251781851053238
Message from 0:  	 Batch:559, loss:0.05122249573469162
Message from 0:  	 Batch:560, loss:0.046787556260824203
Message from 0:  	 Batch:561, loss:0.06256887316703796
Message from 0:  	 Batch:562, loss:0.052016593515872955
Message from 0:  	 Batch:563, loss:0.049180999398231506
Message from 0:  	 Batch:564, loss:0.026059824973344803
Message from 0:  	 Batch:565, loss:0.04372060298919678
Message from 0:  	 Batch:566, loss:0.048094186931848526
Message from 0:  	 Batch:567, loss:0.05357569456100464
Message from 0:  	 Batch:568, loss:0.04611140489578247
Message from 0:  	 Batch:569, loss:0.07158415764570236
Message from 0:  	 Batch:570, loss:0.044505830854177475
Message from 0:  	 Batch:571, loss:0.07170399278402328
Message from 0:  	 Batch:572, loss:0.04854005202651024
Message from 0:  	 Batch:573, loss:0.04513384401798248
Message from 0:  	 Batch:574, loss:0.045092277228832245
Message from 0:  	 Batch:575, loss:0.04722209274768829
Message from 0:  	 Batch:576, loss:0.043365851044654846
Message from 0:  	 Batch:577, loss:0.04815699905157089
Message from 0:  	 Batch:578, loss:0.042046017944812775
Message from 0:  	 Batch:579, loss:0.04266231879591942
Message from 0:  	 Batch:580, loss:0.06382660567760468
Message from 0:  	 Batch:581, loss:0.04012076556682587
Message from 0:  	 Batch:582, loss:0.0491713248193264
Message from 0:  	 Batch:583, loss:0.05076126754283905
Message from 0:  	 Batch:584, loss:0.06746146827936172
Message from 0:  	 Batch:585, loss:0.05100701376795769
Message from 0:  	 Batch:586, loss:0.05027100443840027
Message from 0:  	 Batch:587, loss:0.04820144176483154
Message from 0:  	 Batch:588, loss:0.08725938200950623
Message from 0:  	 Batch:589, loss:0.04851452261209488
Message from 0:  	 Batch:590, loss:0.0778014063835144
Message from 0:  	 Batch:591, loss:0.05190248414874077
Message from 0:  	 Batch:592, loss:0.049273550510406494
Message from 0:  	 Batch:593, loss:0.04594312608242035
Message from 0:  	 Batch:594, loss:0.05117945373058319
Message from 0:  	 Batch:595, loss:0.05076971277594566
Message from 0:  	 Batch:596, loss:0.04689037799835205
Message from 0:  	 Batch:597, loss:0.043491341173648834
Message from 0:  	 Batch:598, loss:0.0644846111536026
Message from 0:  	 Batch:599, loss:0.04503537714481354
Message from 0:  	 Batch:600, loss:0.042001381516456604
Message from 0:  	 Batch:601, loss:0.0737188309431076
Message from 0:  	 Batch:602, loss:0.04310908168554306
Message from 0:  	 Batch:603, loss:0.0545089915394783
Message from 0:  	 Batch:604, loss:0.03229718655347824
Message from 0:  	 Batch:605, loss:0.048881709575653076
Message from 0:  	 Batch:606, loss:0.04813356697559357
Message from 0:  	 Batch:607, loss:0.05448084697127342
Message from 0:  	 Batch:608, loss:0.045205987989902496
Message from 0:  	 Batch:609, loss:0.06997224688529968
Message from 0:  	 Batch:610, loss:0.04456036537885666
Message from 0:  	 Batch:611, loss:0.046431515365839005
Message from 0:  	 Batch:612, loss:0.05427088961005211
Message from 0:  	 Batch:613, loss:0.04301745072007179
Message from 0:  	 Batch:614, loss:0.04359837621450424
Message from 0:  	 Batch:615, loss:0.049886979162693024
Message from 0:  	 Batch:616, loss:0.0489281490445137
Message from 0:  	 Batch:617, loss:0.044503986835479736
Message from 0:  	 Batch:618, loss:0.048358507454395294
Message from 0:  	 Batch:619, loss:0.051536235958337784
Message from 0:  	 Batch:620, loss:0.058108851313591
Message from 0:  	 Batch:621, loss:0.03453867509961128
Message from 0:  	 Batch:622, loss:0.04612687975168228
Message from 0:  	 Batch:623, loss:0.04746271297335625
Message from 0:  	 Batch:624, loss:0.044365450739860535
Message from 0:  	 Batch:625, loss:0.04674895852804184
Message from 0:  	 Batch:626, loss:0.04205154627561569
Message from 0:  	 Batch:627, loss:0.02565499022603035
Message from 0:  	 Batch:628, loss:0.04673650115728378
Message from 0:  	 Batch:629, loss:0.045939650386571884
Message from 0:  	 Batch:630, loss:0.04457438364624977
Message from 0:  	 Batch:631, loss:0.04631002992391586
Message from 0:  	 Batch:632, loss:0.05522911995649338
Message from 0:  	 Batch:633, loss:0.06586064398288727
Message from 0:  	 Batch:634, loss:0.04530860483646393
Message from 0:  	 Batch:635, loss:0.0449976921081543
Message from 0:  	 Batch:636, loss:0.06331318616867065
Message from 0:  	 Batch:637, loss:0.044575560837984085
Message from 0:  	 Batch:638, loss:0.048111461102962494
Message from 0:  	 Batch:639, loss:0.04724949598312378
Message from 0:  	 Batch:640, loss:0.04774532467126846
Message from 0:  	 Batch:641, loss:0.048182085156440735
Message from 0:  	 Batch:642, loss:0.046479273587465286
Message from 0:  	 Batch:643, loss:0.04723428562283516
Message from 0:  	 Batch:644, loss:0.06415890157222748
Message from 0:  	 Batch:645, loss:0.048211537301540375
Message from 0:  	 Batch:646, loss:0.04367156699299812
Message from 0:  	 Batch:647, loss:0.05998281016945839
Message from 0:  	 Batch:648, loss:0.04449405521154404
Message from 0:  	 Batch:649, loss:0.04721144586801529
Message from 0:  	 Batch:650, loss:0.04580198973417282
Message from 0:  	 Batch:651, loss:0.05314578860998154
Message from 0:  	 Batch:652, loss:0.04544052481651306
Message from 0:  	 Batch:653, loss:0.045661360025405884
Message from 0:  	 Batch:654, loss:0.0462978333234787
Message from 0:  	 Batch:655, loss:0.09480646997690201
Message from 0:  	 Batch:656, loss:0.10811497271060944
Message from 0:  	 Batch:657, loss:0.07329341769218445
Message from 0:  	 Batch:658, loss:0.08564384281635284
Message from 0:  	 Batch:659, loss:0.04689592495560646
Message from 0:  	 Batch:660, loss:0.05609317123889923
Message from 0:  	 Batch:661, loss:0.04530638828873634
Message from 0:  	 Batch:662, loss:0.0473286435008049
Message from 0:  	 Batch:663, loss:0.04309283196926117
Message from 0:  	 Batch:664, loss:0.04432797059416771
Message from 0:  	 Batch:665, loss:0.06584152579307556
Message from 0:  	 Batch:666, loss:0.04989456385374069
Message from 0:  	 Batch:667, loss:0.05596911534667015
Message from 0:  	 Batch:668, loss:0.05764317139983177
Message from 0:  	 Batch:669, loss:0.04200654476881027
Message from 0:  	 Batch:670, loss:0.04289887100458145
Message from 0:  	 Batch:671, loss:0.05462248623371124
Message from 0:  	 Batch:672, loss:0.044825173914432526
Message from 0:  	 Batch:673, loss:0.046336859464645386
Message from 0:  	 Batch:674, loss:0.045267291367053986
Message from 0:  	 Batch:675, loss:0.04331124573945999
Message from 0:  	 Batch:676, loss:0.037954796105623245
Message from 0:  	 Batch:677, loss:0.042774468660354614
Message from 0:  	 Batch:678, loss:0.047206997871398926
Message from 0:  	 Batch:679, loss:0.06916444003582001
Message from 0:  	 Batch:680, loss:0.04515250772237778
Message from 0:  	 Batch:681, loss:0.06008971482515335
Message from 0:  	 Batch:682, loss:0.0543929785490036
Message from 0:  	 Batch:683, loss:0.048975870013237
Message from 0:  	 Batch:684, loss:0.04460100829601288
Message from 0:  	 Batch:685, loss:0.05893504619598389
Message from 0:  	 Batch:686, loss:0.04388826712965965
Message from 0:  	 Batch:687, loss:0.08293500542640686
Message from 0:  	 Batch:688, loss:0.06327786296606064
Message from 0:  	 Batch:689, loss:0.04401955008506775
Message from 0:  	 Batch:690, loss:0.05669708549976349
Message from 0:  	 Batch:691, loss:0.057341694831848145
Message from 0:  	 Batch:692, loss:0.04665525257587433
Message from 0:  	 Batch:693, loss:0.039471227675676346
Message from 0:  	 Batch:694, loss:0.057246990501880646
Message from 0:  	 Batch:695, loss:0.06296570599079132
Message from 0:  	 Batch:696, loss:0.06568091362714767
Message from 0:  	 Batch:697, loss:0.048522017896175385
Message from 0:  	 Batch:698, loss:0.04931625723838806
Message from 0:  	 Batch:699, loss:0.04444146156311035
Message from 0:  	 Batch:700, loss:0.0513022281229496
Message from 0:  	 Batch:701, loss:0.04625929147005081
Message from 0:  	 Batch:702, loss:0.0464826375246048
Message from 0:  	 Batch:703, loss:0.03201119974255562
Message from 0:  	 Batch:704, loss:0.0341775044798851
Message from 0:  	 Batch:705, loss:0.04297223314642906
Message from 0:  	 Batch:706, loss:0.03095872886478901
Message from 0:  	 Batch:707, loss:0.044401273131370544
Message from 0:  	 Batch:708, loss:0.04790891706943512
Message from 0:  	 Batch:709, loss:0.04551117122173309
Message from 0:  	 Batch:710, loss:0.048694342374801636
Message from 0:  	 Batch:711, loss:0.05677975341677666
Message from 0:  	 Batch:712, loss:0.06715159863233566
Message from 0:  	 Batch:713, loss:0.049493350088596344
Message from 0:  	 Batch:714, loss:0.0465116873383522
Message from 0:  	 Batch:715, loss:0.04520706087350845
Message from 0:  	 Batch:716, loss:0.055522412061691284
Message from 0:  	 Batch:717, loss:0.04496868699789047
Message from 0:  	 Batch:718, loss:0.0408492311835289
Message from 0:  	 Batch:719, loss:0.0448056235909462
Message from 0:  	 Batch:720, loss:0.04513201862573624
Message from 0:  	 Batch:721, loss:0.04741986095905304
Message from 0:  	 Batch:722, loss:0.05411085486412048
Message from 0:  	 Batch:723, loss:0.0935383215546608
Message from 0:  	 Batch:724, loss:0.04915975034236908
Message from 0:  	 Batch:725, loss:0.04382814094424248
Message from 0:  	 Batch:726, loss:0.044960979372262955
Message from 0:  	 Batch:727, loss:0.0340188704431057
Message from 0:  	 Batch:728, loss:0.05237315222620964
Message from 0:  	 Batch:729, loss:0.0499550886452198
Message from 0:  	 Batch:730, loss:0.06861896812915802
Message from 0:  	 Batch:731, loss:0.047454990446567535
Message from 0:  	 Batch:732, loss:0.04886734485626221
Message from 0:  	 Batch:733, loss:0.05606561154127121
Message from 0:  	 Batch:734, loss:0.048052430152893066
Message from 0:  	 Batch:735, loss:0.030923306941986084
Message from 0:  	 Batch:736, loss:0.05137748271226883
Message from 0:  	 Batch:737, loss:0.05772675201296806
Message from 0:  	 Batch:738, loss:0.04467702656984329
Message from 0:  	 Batch:739, loss:0.04144185781478882
Message from 0:  	 Batch:740, loss:0.05014131963253021
Message from 0:  	 Batch:741, loss:0.04484640061855316
Message from 0:  	 Batch:742, loss:0.04900837689638138
Message from 0:  	 Batch:743, loss:0.0782894715666771
Message from 0:  	 Batch:744, loss:0.057214487344026566
Message from 0:  	 Batch:745, loss:0.04515717178583145
Message from 0:  	 Batch:746, loss:0.03384482488036156
Message from 0:  	 Batch:747, loss:0.08322630822658539
Message from 0:  	 Batch:748, loss:0.03144817426800728
Message from 0:  	 Batch:749, loss:0.043825842440128326
Message from 0:  	 Batch:750, loss:0.0512956827878952
Message from 0:  	 Batch:751, loss:0.03349350392818451
Message from 0:  	 Batch:752, loss:0.04491442069411278
Message from 0:  	 Batch:753, loss:0.05082323029637337
Message from 0:  	 Batch:754, loss:0.08119868487119675
Message from 0:  	 Batch:755, loss:0.04579237103462219
Message from 0:  	 Batch:756, loss:0.052960969507694244
Message from 0:  	 Batch:757, loss:0.03161035478115082
Message from 0:  	 Batch:758, loss:0.046269845217466354
Message from 0:  	 Batch:759, loss:0.04429725930094719
Message from 0:  	 Batch:760, loss:0.047670699656009674
Message from 0:  	 Batch:761, loss:0.04784505069255829
Message from 0:  	 Batch:762, loss:0.05735576152801514
Message from 0:  	 Batch:763, loss:0.05177793651819229
Message from 0:  	 Batch:764, loss:0.045916374772787094
Message from 0:  	 Batch:765, loss:0.05935110151767731
Message from 0:  	 Batch:766, loss:0.065245121717453
Message from 0:  	 Batch:767, loss:0.0470460020005703
Message from 0:  	 Batch:768, loss:0.030075345188379288
Message from 0:  	 Batch:769, loss:0.055010534822940826
Message from 0:  	 Batch:770, loss:0.04355233535170555
Message from 0:  	 Batch:771, loss:0.03267347812652588
Message from 0:  	 Batch:772, loss:0.04302568733692169
Message from 0:  	 Batch:773, loss:0.04016129672527313
Message from 0:  	 Batch:774, loss:0.04434159770607948
Message from 0:  	 Batch:775, loss:0.046978920698165894
Message from 0:  	 Batch:776, loss:0.03858857601881027
Message from 0:  	 Batch:777, loss:0.05206570029258728
Message from 0:  	 Batch:778, loss:0.04193541407585144
Message from 0:  	 Batch:779, loss:0.04851735383272171
Message from 0:  	 Batch:780, loss:0.039706021547317505
Message from 0:  	 Batch:781, loss:0.04312816262245178
Message from 0:  	 Batch:782, loss:0.04391350969672203
Message from 0:  	 Batch:783, loss:0.0644783228635788
Message from 0:  	 Batch:784, loss:0.04854018986225128
Message from 0:  	 Batch:785, loss:0.06877827644348145
Message from 0:  	 Batch:786, loss:0.057685449719429016
Message from 0:  	 Batch:787, loss:0.05808121711015701
Message from 0:  	 Batch:788, loss:0.048110272735357285
Message from 0:  	 Batch:789, loss:0.05333808809518814
Message from 0:  	 Batch:790, loss:0.05437043309211731
Message from 0:  	 Batch:791, loss:0.046681493520736694
Message from 0:  	 Batch:792, loss:0.047987185418605804
Message from 0:  	 Batch:793, loss:0.06430162489414215
Message from 0:  	 Batch:794, loss:0.04249807819724083
Message from 0:  	 Batch:795, loss:0.049371324479579926
Message from 0:  	 Batch:796, loss:0.0465252548456192
Message from 0:  	 Batch:797, loss:0.03207390382885933
Message from 0:  	 Batch:798, loss:0.04005572944879532
Message from 0:  	 Batch:799, loss:0.048676855862140656
Message from 0:  	 Batch:800, loss:0.04113059490919113
Message from 0:  	 Batch:801, loss:0.05348152667284012
Message from 0:  	 Batch:802, loss:0.053448669612407684
Message from 0:  	 Batch:803, loss:0.044708140194416046
Message from 0:  	 Batch:804, loss:0.04596255347132683
Message from 0:  	 Batch:805, loss:0.04200149327516556
Message from 0:  	 Batch:806, loss:0.046332549303770065
Message from 0:  	 Batch:807, loss:0.06064518541097641
Message from 0:  	 Batch:808, loss:0.044765375554561615
Message from 0:  	 Batch:809, loss:0.04406294226646423
Message from 0:  	 Batch:810, loss:0.040104255080223083
Message from 0:  	 Batch:811, loss:0.047234438359737396
Message from 0:  	 Batch:812, loss:0.040151678025722504
Message from 0:  	 Batch:813, loss:0.044524773955345154
Message from 0:  	 Batch:814, loss:0.04263653978705406
Message from 0:  	 Batch:815, loss:0.049229975789785385
Message from 0:  	 Batch:816, loss:0.048288747668266296
Message from 0:  	 Batch:817, loss:0.05223684385418892
Message from 0:  	 Batch:818, loss:0.05432824417948723
Message from 0:  	 Batch:819, loss:0.05049417167901993
Message from 0:  	 Batch:820, loss:0.040788523852825165
Message from 0:  	 Batch:821, loss:0.05673141032457352
Message from 0:  	 Batch:822, loss:0.04983650520443916
Message from 0:  	 Batch:823, loss:0.050131119787693024
Message from 0:  	 Batch:824, loss:0.03677497059106827
Message from 0:  	 Batch:825, loss:0.045973751693964005
Message from 0:  	 Batch:826, loss:0.0432732068002224
Message from 0:  	 Batch:827, loss:0.06667641550302505
Message from 0:  	 Batch:828, loss:0.04367195814847946
Message from 0:  	 Batch:829, loss:0.052570030093193054
Message from 0:  	 Batch:830, loss:0.04686710983514786
Message from 0:  	 Batch:831, loss:0.050243671983480453
Message from 0:  	 Batch:832, loss:0.04500255733728409
Message from 0:  	 Batch:833, loss:0.04475732892751694
Message from 0:  	 Batch:834, loss:0.04661686718463898
Message from 0:  	 Batch:835, loss:0.04445352405309677
Message from 0:  	 Batch:836, loss:0.04575209692120552
Message from 0:  	 Batch:837, loss:0.047148529440164566
Message from 0:  	 Batch:838, loss:0.03918835520744324
Message from 0:  	 Batch:839, loss:0.03762153908610344
Message from 0:  	 Batch:840, loss:0.05921030789613724
Message from 0:  	 Batch:841, loss:0.04591812193393707
Message from 0:  	 Batch:842, loss:0.044682420790195465
Message from 0:  	 Batch:843, loss:0.04079216718673706
Message from 0:  	 Batch:844, loss:0.06551336497068405
Message from 0:  	 Batch:845, loss:0.06364721804857254
Message from 0:  	 Batch:846, loss:0.05476423352956772
Message from 0:  	 Batch:847, loss:0.03865528106689453
Message from 0:  	 Batch:848, loss:0.08201766014099121
Message from 0:  	 Batch:849, loss:0.046837400645017624
Message from 0:  	 Batch:850, loss:0.04488665983080864
Message from 0:  	 Batch:851, loss:0.04793284460902214
Message from 0:  	 Batch:852, loss:0.04609490931034088
Message from 0:  	 Batch:853, loss:0.05396737530827522
Message from 0:  	 Batch:854, loss:0.04535093158483505
Message from 0:  	 Batch:855, loss:0.05473242700099945
Message from 0:  	 Batch:856, loss:0.0450083464384079
Message from 0:  	 Batch:857, loss:0.03640451282262802
Message from 0:  	 Batch:858, loss:0.05005289614200592
Message from 0:  	 Batch:859, loss:0.04315507411956787
Message from 0:  	 Batch:860, loss:0.04047628492116928
Message from 0:  	 Batch:861, loss:0.04291200637817383
Message from 0:  	 Batch:862, loss:0.07270476967096329
Message from 0:  	 Batch:863, loss:0.048096172511577606
Message from 0:  	 Batch:864, loss:0.06347859650850296
Message from 0:  	 Batch:865, loss:0.0504988469183445
Message from 0:  	 Batch:866, loss:0.0430377796292305
Message from 0:  	 Batch:867, loss:0.04736698791384697
Message from 0:  	 Batch:868, loss:0.062362343072891235
Message from 0:  	 Batch:869, loss:0.06377546489238739
Message from 0:  	 Batch:870, loss:0.05266309157013893
Message from 0:  	 Batch:871, loss:0.029650716111063957
Message from 0:  	 Batch:872, loss:0.043059706687927246
Message from 0:  	 Batch:873, loss:0.07735639810562134
Message from 0:  	 Batch:874, loss:0.044037241488695145
Message from 0:  	 Batch:875, loss:0.03784054517745972
Message from 0:  	 Batch:876, loss:0.04770632088184357
Message from 0:  	 Batch:877, loss:0.04450032114982605
Message from 0:  	 Batch:878, loss:0.045194655656814575
Message from 0:  	 Batch:879, loss:0.05467541515827179
Message from 0:  	 Batch:880, loss:0.04414932429790497
Message from 0:  	 Batch:881, loss:0.0409858301281929
Message from 0:  	 Batch:882, loss:0.05506507679820061
Message from 0:  	 Batch:883, loss:0.03848303109407425
Message from 0:  	 Batch:884, loss:0.044470950961112976
Message from 0:  	 Batch:885, loss:0.04744723439216614
Message from 0:  	 Batch:886, loss:0.041045598685741425
Message from 0:  	 Batch:887, loss:0.05307210236787796
Message from 0:  	 Batch:888, loss:0.041894227266311646
Message from 0:  	 Batch:889, loss:0.04693884402513504
Message from 0:  	 Batch:890, loss:0.048647601157426834
Message from 0:  	 Batch:891, loss:0.03688156604766846
Message from 0:  	 Batch:892, loss:0.04774731770157814
Message from 0:  	 Batch:893, loss:0.049691468477249146
Message from 0:  	 Batch:894, loss:0.05367302522063255
Message from 0:  	 Batch:895, loss:0.04767869785428047
Message from 0:  	 Batch:896, loss:0.049777381122112274
Message from 0:  	 Batch:897, loss:0.04479481279850006
Message from 0:  	 Batch:898, loss:0.0498516783118248
Message from 0:  	 Batch:899, loss:0.04132206365466118
Message from 0:  	 Batch:900, loss:0.04489801824092865
Message from 0:  	 Batch:901, loss:0.050147585570812225
Message from 0:  	 Batch:902, loss:0.04551387578248978
Message from 0:  	 Batch:903, loss:0.03703834116458893
Message from 0:  	 Batch:904, loss:0.04598084092140198
Message from 0:  	 Batch:905, loss:0.04824052006006241
Message from 0:  	 Batch:906, loss:0.043264757841825485
Message from 0:  	 Batch:907, loss:0.04050281643867493
Message from 0:  	 Batch:908, loss:0.025462808087468147
Message from 0:  	 Batch:909, loss:0.04390464723110199
Message from 0:  	 Batch:910, loss:0.04821992665529251
Message from 0:  	 Batch:911, loss:0.05224059149622917
Message from 0:  	 Batch:912, loss:0.04544868320226669
Message from 0:  	 Batch:913, loss:0.04916144907474518
Message from 0:  	 Batch:914, loss:0.04407906532287598
Message from 0:  	 Batch:915, loss:0.048829033970832825
Message from 0:  	 Batch:916, loss:0.051397476345300674
Message from 0:  	 Batch:917, loss:0.043283604085445404
Message from 0:  	 Batch:918, loss:0.05930076166987419
Message from 0:  	 Batch:919, loss:0.03109634295105934
Message from 0:  	 Batch:920, loss:0.045542992651462555
Message from 0:  	 Batch:921, loss:0.06315943598747253
Message from 0:  	 Batch:922, loss:0.04474388062953949
Message from 0:  	 Batch:923, loss:0.04793907701969147
Message from 0:  	 Batch:924, loss:0.05046381801366806
Message from 0:  	 Batch:925, loss:0.05126351863145828
Message from 0:  	 Batch:926, loss:0.04449291527271271
Message from 0:  	 Batch:927, loss:0.06216536834836006
Message from 0:  	 Batch:928, loss:0.05146201327443123
Message from 0:  	 Batch:929, loss:0.035105735063552856
Message from 0:  	 Batch:930, loss:0.043653927743434906
Message from 0:  	 Batch:931, loss:0.037215858697891235
Message from 0:  	 Batch:932, loss:0.04691197723150253
Message from 0:  	 Batch:933, loss:0.07090974599123001
Message from 0:  	 Batch:934, loss:0.0682947114109993
Message from 0:  	 Batch:935, loss:0.04047476127743721
Message from 0:  	 Batch:936, loss:0.0414249524474144
Message from 0:  	 Batch:937, loss:0.03166483715176582
Message from 0:  	 Batch:938, loss:0.03835336118936539
Message from 0:  	 Batch:939, loss:0.033898092806339264
Message from 0:  	 Batch:940, loss:0.03385866433382034
Message from 0:  	 Batch:941, loss:0.05499858781695366
Message from 0:  	 Batch:942, loss:0.04381430149078369
Message from 0:  	 Batch:943, loss:0.05187513679265976
Message from 0:  	 Batch:944, loss:0.05208945274353027
Message from 0:  	 Batch:945, loss:0.04093167185783386
Message from 0:  	 Batch:946, loss:0.057081930339336395
Message from 0:  	 Batch:947, loss:0.04394233226776123
Message from 0:  	 Batch:948, loss:0.07887645065784454
Message from 0:  	 Batch:949, loss:0.06109892204403877
Message from 0:  	 Batch:950, loss:0.05077597498893738
Message from 0:  	 Batch:951, loss:0.05407450348138809
Message from 0:  	 Batch:952, loss:0.045504942536354065
Message from 0:  	 Batch:953, loss:0.07045681774616241
Message from 0:  	 Batch:954, loss:0.04853559285402298
Message from 0:  	 Batch:955, loss:0.04816874861717224
Message from 0:  	 Batch:956, loss:0.06010720133781433
Message from 0:  	 Batch:957, loss:0.04786980152130127
Message from 0:  	 Batch:958, loss:0.051427870988845825
Message from 0:  	 Batch:959, loss:0.04736565425992012
Message from 0:  	 Batch:960, loss:0.044406160712242126
Message from 0:  	 Batch:961, loss:0.04363656044006348
Message from 0:  	 Batch:962, loss:0.050374921411275864
Message from 0:  	 Batch:963, loss:0.03992592915892601
Message from 0:  	 Batch:964, loss:0.046753570437431335
Message from 0:  	 Batch:965, loss:0.04362088441848755
Message from 0:  	 Batch:966, loss:0.05087628215551376
Message from 0:  	 Batch:967, loss:0.04469238221645355
Message from 0:  	 Batch:968, loss:0.03626072779297829
Message from 0:  	 Batch:969, loss:0.055786848068237305
Message from 0:  	 Batch:970, loss:0.06939500570297241
Message from 0:  	 Batch:971, loss:0.04209939390420914
Message from 0:  	 Batch:972, loss:0.04326816648244858
Message from 0:  	 Batch:973, loss:0.056033775210380554
Message from 0:  	 Batch:974, loss:0.07358478009700775
Message from 0:  	 Batch:975, loss:0.07184886187314987
Message from 0:  	 Batch:976, loss:0.04814191162586212
Message from 0:  	 Batch:977, loss:0.04575992375612259
Message from 0:  	 Batch:978, loss:0.07652458548545837
Message from 0:  	 Batch:979, loss:0.04259075969457626
Message from 0:  	 Batch:980, loss:0.05615083500742912
Message from 0:  	 Batch:981, loss:0.050593793392181396
Message from 0:  	 Batch:982, loss:0.043341830372810364
Message from 0:  	 Batch:983, loss:0.04867967218160629
Message from 0:  	 Batch:984, loss:0.04979266971349716
Message from 0:  	 Batch:985, loss:0.04317491501569748
Message from 0:  	 Batch:986, loss:0.04488115385174751
Message from 0:  	 Batch:987, loss:0.047489769756793976
Message from 0:  	 Batch:988, loss:0.06095209717750549
Message from 0:  	 Batch:989, loss:0.048056453466415405
Message from 0:  	 Batch:990, loss:0.06058460846543312
Message from 0:  	 Batch:991, loss:0.0488489493727684
Message from 0:  	 Batch:992, loss:0.042007625102996826
Message from 0:  	 Batch:993, loss:0.05292124301195145
Message from 0:  	 Batch:994, loss:0.044521309435367584
Message from 0:  	 Batch:995, loss:0.06782002002000809
Message from 0:  	 Batch:996, loss:0.020222345367074013
Message from 0:  	 Batch:997, loss:0.04723811894655228
Message from 0:  	 Batch:998, loss:0.04619841277599335
Message from 0:  	 Batch:999, loss:0.044292181730270386
Message from 0:  	 Batch:1000, loss:0.04468715190887451
Message from 0:  	 Batch:1001, loss:0.07103309035301208
Message from 0:  	 Batch:1002, loss:0.04186267405748367
Message from 0:  	 Batch:1003, loss:0.048323918133974075
Message from 0:  	 Batch:1004, loss:0.046611636877059937
Message from 0:  	 Batch:1005, loss:0.046309471130371094
Message from 0:  	 Batch:1006, loss:0.031172584742307663
Message from 0:  	 Batch:1007, loss:0.04039239138364792
Message from 0:  	 Batch:1008, loss:0.0449107401072979
Message from 0:  	 Batch:1009, loss:0.039507195353507996
Message from 0:  	 Batch:1010, loss:0.06033598631620407
Message from 0:  	 Batch:1011, loss:0.046012360602617264
Message from 0:  	 Batch:1012, loss:0.047675374895334244
Message from 0:  	 Batch:1013, loss:0.04632791131734848
Message from 0:  	 Batch:1014, loss:0.05312450975179672
Message from 0:  	 Batch:1015, loss:0.05107651650905609
Message from 0:  	 Batch:1016, loss:0.0499693863093853
Message from 0:  	 Batch:1017, loss:0.051071181893348694
Message from 0:  	 Batch:1018, loss:0.054445572197437286
Message from 0:  	 Batch:1019, loss:0.03650161623954773
Message from 0:  	 Batch:1020, loss:0.039644915610551834
Message from 0:  	 Batch:1021, loss:0.04285343363881111
Message from 0:  	 Batch:1022, loss:0.05162074416875839
Message from 0:  	 Batch:1023, loss:0.04034285619854927
Message from 0:  	 Batch:1024, loss:0.042144373059272766
Message from 0:  	 Batch:1025, loss:0.049891818314790726
Message from 0:  	 Batch:1026, loss:0.05064252391457558
Message from 0:  	 Batch:1027, loss:0.040661245584487915
Message from 0:  	 Batch:1028, loss:0.05154865235090256
Message from 0:  	 Batch:1029, loss:0.03298026695847511
Message from 0:  	 Batch:1030, loss:0.04616314172744751
Message from 0:  	 Batch:1031, loss:0.05872718244791031
Message from 0:  	 Batch:1032, loss:0.04758874326944351
Message from 0:  	 Batch:1033, loss:0.05905609950423241
Message from 0:  	 Batch:1034, loss:0.0731801986694336
Message from 0:  	 Batch:1035, loss:0.04898525029420853
Message from 0:  	 Batch:1036, loss:0.05024632811546326
Message from 0:  	 Batch:1037, loss:0.08979563415050507
Message from 0:  	 Batch:1038, loss:0.050875864923000336
Message from 0:  	 Batch:1039, loss:0.03897333890199661
Message from 0:  	 Batch:1040, loss:0.050754472613334656
Message from 0:  	 Batch:1041, loss:0.04583423212170601
Message from 0:  	 Batch:1042, loss:0.047701843082904816
Message from 0:  	 Batch:1043, loss:0.03584156185388565
Message from 0:  	 Batch:1044, loss:0.048226211220026016
Message from 0:  	 Batch:1045, loss:0.049847088754177094
Message from 0:  	 Batch:1046, loss:0.042479008436203
Message from 0:  	 Batch:1047, loss:0.04250412434339523
Message from 0:  	 Batch:1048, loss:0.04363379627466202
Message from 0:  	 Batch:1049, loss:0.04246576130390167
Message from 0:  	 Batch:1050, loss:0.029191333800554276
Message from 0:  	 Batch:1051, loss:0.04373646900057793
Message from 0:  	 Batch:1052, loss:0.055733226239681244
Message from 0:  	 Batch:1053, loss:0.035025790333747864
Message from 0:  	 Batch:1054, loss:0.038873620331287384
Message from 0:  	 Batch:1055, loss:0.0435480922460556
Message from 0:  	 Batch:1056, loss:0.04735403507947922
Message from 0:  	 Batch:1057, loss:0.045264843851327896
Message from 0:  	 Batch:1058, loss:0.047520946711301804
Message from 0:  	 Batch:1059, loss:0.05845244228839874
Message from 0:  	 Batch:1060, loss:0.05635317042469978
Message from 0:  	 Batch:1061, loss:0.053612176328897476
Message from 0:  	 Batch:1062, loss:0.0463506281375885
Message from 0:  	 Batch:1063, loss:0.05352730676531792
Message from 0:  	 Batch:1064, loss:0.04389860853552818
Message from 0:  	 Batch:1065, loss:0.04303281009197235
Message from 0:  	 Batch:1066, loss:0.046496182680130005
Message from 0:  	 Batch:1067, loss:0.04647945612668991
Message from 0:  	 Batch:1068, loss:0.05081864073872566
Message from 0:  	 Batch:1069, loss:0.034653451293706894
Message from 0:  	 Batch:1070, loss:0.04723279923200607
Message from 0:  	 Batch:1071, loss:0.04492470622062683
Message from 0:  	 Batch:1072, loss:0.0909145250916481
Message from 0:  	 Batch:1073, loss:0.049316294491291046
Message from 0:  	 Batch:1074, loss:0.03451201692223549
Message from 0:  	 Batch:1075, loss:0.04228241369128227
Message from 0:  	 Batch:1076, loss:0.07296917587518692
Message from 0:  	 Batch:1077, loss:0.04038561135530472
Message from 0:  	 Batch:1078, loss:0.047154247760772705
Message from 0:  	 Batch:1079, loss:0.058078572154045105
Message from 0:  	 Batch:1080, loss:0.046738073229789734
Message from 0:  	 Batch:1081, loss:0.050502240657806396
Message from 0:  	 Batch:1082, loss:0.05229710042476654
Message from 0:  	 Batch:1083, loss:0.06370485574007034
Message from 0:  	 Batch:1084, loss:0.04386508837342262
Message from 0:  	 Batch:1085, loss:0.08294124901294708
Message from 0:  	 Batch:1086, loss:0.048030443489551544
Message from 0:  	 Batch:1087, loss:0.06670785695314407
Message from 0:  	 Batch:1088, loss:0.05043245106935501
Message from 0:  	 Batch:1089, loss:0.02775038406252861
Message from 0:  	 Batch:1090, loss:0.04637422785162926
Message from 0:  	 Batch:1091, loss:0.051361337304115295
Message from 0:  	 Batch:1092, loss:0.04340615123510361
Message from 0:  	 Batch:1093, loss:0.04624062404036522
Message from 0:  	 Batch:1094, loss:0.04382201284170151
Message from 0:  	 Batch:1095, loss:0.035819604992866516
Message from 0:  	 Batch:1096, loss:0.04532710462808609
Message from 0:  	 Batch:1097, loss:0.08169705420732498
Message from 0:  	 Batch:1098, loss:0.051698751747608185
Message from 0:  	 Batch:1099, loss:0.05600028485059738
Message from 0:  	 Batch:1100, loss:0.04251347482204437
Message from 0:  	 Batch:1101, loss:0.0440315380692482
Message from 0:  	 Batch:1102, loss:0.0531294047832489
Message from 0:  	 Batch:1103, loss:0.047239355742931366
Message from 0:  	 Batch:1104, loss:0.04967547208070755
Message from 0:  	 Batch:1105, loss:0.043883007019758224
Message from 0:  	 Batch:1106, loss:0.03624001517891884
Message from 0:  	 Batch:1107, loss:0.050779372453689575
Message from 0:  	 Batch:1108, loss:0.03839137405157089
Message from 0:  	 Batch:1109, loss:0.040607843548059464
Message from 0:  	 Batch:1110, loss:0.04297689348459244
Message from 0:  	 Batch:1111, loss:0.05202803388237953
Message from 0:  	 Batch:1112, loss:0.042037930339574814
Message from 0:  	 Batch:1113, loss:0.05032379925251007
Message from 0:  	 Batch:1114, loss:0.06174936145544052
Message from 0:  	 Batch:1115, loss:0.04024028778076172
Message from 0:  	 Batch:1116, loss:0.039716172963380814
Message from 0:  	 Batch:1117, loss:0.04356204718351364
Message from 0:  	 Batch:1118, loss:0.04161719232797623
Message from 0:  	 Batch:1119, loss:0.04866727069020271
Message from 0:  	 Batch:1120, loss:0.03436283767223358
Message from 0:  	 Batch:1121, loss:0.04797836393117905
Message from 0:  	 Batch:1122, loss:0.04424574226140976
Message from 0:  	 Batch:1123, loss:0.03252805024385452
Message from 0:  	 Batch:1124, loss:0.04695473611354828
Message from 0:  	 Batch:1125, loss:0.053485751152038574
Message from 0:  	 Batch:1126, loss:0.06445351243019104
Message from 0:  	 Batch:1127, loss:0.0555463507771492
Message from 0:  	 Batch:1128, loss:0.05818147212266922
Message from 0:  	 Batch:1129, loss:0.04929447919130325
Message from 0:  	 Batch:1130, loss:0.05661269277334213
Message from 0:  	 Batch:1131, loss:0.06255082786083221
Message from 0:  	 Batch:1132, loss:0.053634826093912125
Message from 0:  	 Batch:1133, loss:0.05275031924247742
Message from 0:  	 Batch:1134, loss:0.0695049837231636
Message from 0:  	 Batch:1135, loss:0.041177161037921906
Message from 0:  	 Batch:1136, loss:0.048430539667606354
Message from 0:  	 Batch:1137, loss:0.05825740844011307
Message from 0:  	 Batch:1138, loss:0.04588017240166664
Message from 0:  	 Batch:1139, loss:0.04577970877289772
Message from 0:  	 Batch:1140, loss:0.03732183575630188
Message from 0:  	 Batch:1141, loss:0.026014821603894234
Message from 0:  	 Batch:1142, loss:0.043200742453336716
Message from 0:  	 Batch:1143, loss:0.04219772666692734
Message from 0:  	 Batch:1144, loss:0.05128995701670647
Message from 0:  	 Batch:1145, loss:0.05107957124710083
Message from 0:  	 Batch:1146, loss:0.04333328455686569
Message from 0:  	 Batch:1147, loss:0.039449743926525116
Message from 0:  	 Batch:1148, loss:0.03747377544641495
Message from 0:  	 Batch:1149, loss:0.0459994301199913
Message from 0:  	 Batch:1150, loss:0.04877471923828125
Message from 0:  	 Batch:1151, loss:0.04573710262775421
Message from 0:  	 Batch:1152, loss:0.04184497520327568
Message from 0:  	 Batch:1153, loss:0.047987550497055054
Message from 0:  	 Batch:1154, loss:0.07051711529493332
Message from 0:  	 Batch:1155, loss:0.040137000381946564
Message from 0:  	 Batch:1156, loss:0.0659082755446434
Message from 0:  	 Batch:1157, loss:0.04949671030044556
Message from 0:  	 Batch:1158, loss:0.0504302941262722
Message from 0:  	 Batch:1159, loss:0.047918930649757385
Message from 0:  	 Batch:1160, loss:0.03580666705965996
Message from 0:  	 Batch:1161, loss:0.04357542842626572
Message from 0:  	 Batch:1162, loss:0.050238125026226044
Message from 0:  	 Batch:1163, loss:0.051204513758420944
Message from 0:  	 Batch:1164, loss:0.03528185188770294
Message from 0:  	 Batch:1165, loss:0.04220064729452133
Message from 0:  	 Batch:1166, loss:0.03684180974960327
Message from 0:  	 Batch:1167, loss:0.0418219193816185
Message from 0:  	 Batch:1168, loss:0.047921210527420044
Message from 0:  	 Batch:1169, loss:0.04532583802938461
Message from 0:  	 Batch:1170, loss:0.05072425305843353
Message from 0:  	 Batch:1171, loss:0.0387897863984108
Message from 0:  	 Batch:1172, loss:0.03467496857047081
Message from 0:  	 Batch:1173, loss:0.05134230852127075
Message from 0:  	 Batch:1174, loss:0.04415207356214523
Message from 0:  	 Batch:1175, loss:0.05147293210029602
Message from 0:  	 Batch:1176, loss:0.03933416306972504
Message from 0:  	 Batch:1177, loss:0.08703067898750305
Message from 0:  	 Batch:1178, loss:0.04510204493999481
Message from 0:  	 Batch:1179, loss:0.03533829376101494
Message from 0:  	 Batch:1180, loss:0.043286655098199844
Message from 0:  	 Batch:1181, loss:0.05699394643306732
Message from 0:  	 Batch:1182, loss:0.04829958826303482
Message from 0:  	 Batch:1183, loss:0.04768996685743332
Message from 0:  	 Batch:1184, loss:0.04284462332725525
Message from 0:  	 Batch:1185, loss:0.043070029467344284
Message from 0:  	 Batch:1186, loss:0.03861282020807266
Message from 0:  	 Batch:1187, loss:0.05603726953268051
Message from 0:  	 Batch:1188, loss:0.04709751531481743
Message from 0:  	 Batch:1189, loss:0.05193816125392914
Message from 0:  	 Batch:1190, loss:0.04560172185301781
Message from 0:  	 Batch:1191, loss:0.04768141359090805
Message from 0:  	 Batch:1192, loss:0.04937310516834259
Message from 0:  	 Batch:1193, loss:0.03250672295689583
Message from 0:  	 Batch:1194, loss:0.05091128498315811
Message from 0:  	 Batch:1195, loss:0.04576275497674942
Message from 0:  	 Batch:1196, loss:0.06114733964204788
Message from 0:  	 Batch:1197, loss:0.04531578719615936
Message from 0:  	 Batch:1198, loss:0.04530131444334984
Message from 0:  	 Batch:1199, loss:0.058387354016304016
Message from 0:  	 Batch:1200, loss:0.033245719969272614
Message from 0:  	 Batch:1201, loss:0.0371079295873642
Message from 0:  	 Batch:1202, loss:0.05114170163869858
Message from 0:  	 Batch:1203, loss:0.03433360531926155
Message from 0:  	 Batch:1204, loss:0.04593556374311447
Message from 0:  	 Batch:1205, loss:0.04644906520843506
Message from 0:  	 Batch:1206, loss:0.04313318431377411
Message from 0:  	 Batch:1207, loss:0.047948967665433884
Message from 0:  	 Batch:1208, loss:0.04853469133377075
Message from 0:  	 Batch:1209, loss:0.04456645995378494
Message from 0:  	 Batch:1210, loss:0.0473603680729866
Message from 0:  	 Batch:1211, loss:0.04896078258752823
Message from 0:  	 Batch:1212, loss:0.04476325958967209
Message from 0:  	 Batch:1213, loss:0.045506738126277924
Message from 0:  	 Batch:1214, loss:0.039815038442611694
Message from 0:  	 Batch:1215, loss:0.048567041754722595
Message from 0:  	 Batch:1216, loss:0.04710468649864197
Message from 0:  	 Batch:1217, loss:0.04989029839634895
Message from 0:  	 Batch:1218, loss:0.06508643180131912
Message from 0:  	 Batch:1219, loss:0.0629824697971344
Message from 0:  	 Batch:1220, loss:0.05096813291311264
Message from 0:  	 Batch:1221, loss:0.04690127819776535
Message from 0:  	 Batch:1222, loss:0.041395217180252075
Message from 0:  	 Batch:1223, loss:0.05486731976270676
Message from 0:  	 Batch:1224, loss:0.04144143685698509
Message from 0:  	 Batch:1225, loss:0.051443614065647125
Message from 0:  	 Batch:1226, loss:0.04762497544288635
Message from 0:  	 Batch:1227, loss:0.052383825182914734
Message from 0:  	 Batch:1228, loss:0.05842110514640808
Message from 0:  	 Batch:1229, loss:0.044956497848033905
Message from 0:  	 Batch:1230, loss:0.03368958830833435
Message from 0:  	 Batch:1231, loss:0.04763510823249817
Message from 0:  	 Batch:1232, loss:0.03742113336920738
Message from 0:  	 Batch:1233, loss:0.034271806478500366
Message from 0:  	 Batch:1234, loss:0.05382462590932846
Message from 0:  	 Batch:1235, loss:0.04362320154905319
Message from 0:  	 Batch:1236, loss:0.0490306057035923
Message from 0:  	 Batch:1237, loss:0.045051030814647675
Message from 0:  	 Batch:1238, loss:0.05010311305522919
Message from 0:  	 Batch:1239, loss:0.04841569438576698
Message from 0:  	 Batch:1240, loss:0.04928046464920044
Message from 0:  	 Batch:1241, loss:0.05585165694355965
Message from 0:  	 Batch:1242, loss:0.05395665764808655
Message from 0:  	 Batch:1243, loss:0.0649154782295227
Message from 0:  	 Batch:1244, loss:0.045035868883132935
Message from 0:  	 Batch:1245, loss:0.044077616184949875
Message from 0:  	 Batch:1246, loss:0.06338649243116379
Message from 0:  	 Batch:1247, loss:0.0598796084523201
Message from 0:  	 Batch:1248, loss:0.045092254877090454
Message from 0:  	 Batch:1249, loss:0.04690892621874809
Message from 0:  	 Batch:1250, loss:0.04234182462096214
Message from 0:  	 Batch:1251, loss:0.045854710042476654
Message from 0:  	 Batch:1252, loss:0.04658431559801102
Message from 0:  	 Batch:1253, loss:0.04077088087797165
Message from 0:  	 Batch:1254, loss:0.05414176732301712
Message from 0:  	 Batch:1255, loss:0.045914605259895325
Message from 0:  	 Batch:1256, loss:0.04075902700424194
Message from 0:  	 Batch:1257, loss:0.04543464630842209
Message from 0:  	 Batch:1258, loss:0.0546988807618618
Message from 0:  	 Batch:1259, loss:0.05070296674966812
Message from 0:  	 Batch:1260, loss:0.06645330786705017
Message from 0:  	 Batch:1261, loss:0.04898545891046524
Message from 0:  	 Batch:1262, loss:0.04468706250190735
Message from 0:  	 Batch:1263, loss:0.03597210347652435
Message from 0:  	 Batch:1264, loss:0.04374292865395546
Message from 0:  	 Batch:1265, loss:0.037875112146139145
Message from 0:  	 Batch:1266, loss:0.03811441361904144
Message from 0:  	 Batch:1267, loss:0.05093175545334816
Message from 0:  	 Batch:1268, loss:0.051026422530412674
Message from 0:  	 Batch:1269, loss:0.047316983342170715
Message from 0:  	 Batch:1270, loss:0.046323053538799286
Message from 0:  	 Batch:1271, loss:0.04821592569351196
Message from 0:  	 Batch:1272, loss:0.043658092617988586
Message from 0:  	 Batch:1273, loss:0.04033587872982025
Message from 0:  	 Batch:1274, loss:0.04650375247001648
Message from 0:  	 Batch:1275, loss:0.06941148638725281
Message from 0:  	 Batch:1276, loss:0.07430686056613922
Message from 0:  	 Batch:1277, loss:0.055316973477602005
Message from 0:  	 Batch:1278, loss:0.04421830549836159
Message from 0:  	 Batch:1279, loss:0.055471763014793396
Message from 0:  	 Batch:1280, loss:0.04586152732372284
Message from 0:  	 Batch:1281, loss:0.047262128442525864
Message from 0:  	 Batch:1282, loss:0.04243079572916031
Message from 0:  	 Batch:1283, loss:0.043811410665512085
Message from 0:  	 Batch:1284, loss:0.049194447696208954
Message from 0:  	 Batch:1285, loss:0.04296257719397545
Message from 0:  	 Batch:1286, loss:0.06204631179571152
Message from 0:  	 Batch:1287, loss:0.04013371840119362
Message from 0:  	 Batch:1288, loss:0.07248874008655548
Message from 0:  	 Batch:1289, loss:0.0435333177447319
Message from 0:  	 Batch:1290, loss:0.049786992371082306
Message from 0:  	 Batch:1291, loss:0.08418861031532288
Message from 0:  	 Batch:1292, loss:0.04426797106862068
Message from 0:  	 Batch:1293, loss:0.056716371327638626
Message from 0:  	 Batch:1294, loss:0.04852309077978134
Message from 0:  	 Batch:1295, loss:0.06092769652605057
Message from 0:  	 Batch:1296, loss:0.046059802174568176
Message from 0:  	 Batch:1297, loss:0.04588183015584946
Message from 0:  	 Batch:1298, loss:0.033667370676994324
Message from 0:  	 Batch:1299, loss:0.04918094724416733
Message from 0:  	 Batch:1300, loss:0.05724310502409935
Message from 0:  	 Batch:1301, loss:0.0682370513677597
Message from 0:  	 Batch:1302, loss:0.04522603750228882
Message from 0:  	 Batch:1303, loss:0.04706905782222748
Message from 0:  	 Batch:1304, loss:0.03287884593009949
Message from 0:  	 Batch:1305, loss:0.045182399451732635
Message from 0:  	 Batch:1306, loss:0.0451219379901886
Message from 0:  	 Batch:1307, loss:0.07681621611118317
Message from 0:  	 Batch:1308, loss:0.058442823588848114
Message from 0:  	 Batch:1309, loss:0.0448659285902977
Message from 0:  	 Batch:1310, loss:0.045501772314310074
Message from 0:  	 Batch:1311, loss:0.05464872717857361
Message from 0:  	 Batch:1312, loss:0.04658157750964165
Message from 0:  	 Batch:1313, loss:0.07423614710569382
Message from 0:  	 Batch:1314, loss:0.053138695657253265
Message from 0:  	 Batch:1315, loss:0.052195534110069275
Message from 0:  	 Batch:1316, loss:0.0535331666469574
Message from 0:  	 Batch:1317, loss:0.04628001153469086
Message from 0:  	 Batch:1318, loss:0.060813628137111664
Message from 0:  	 Batch:1319, loss:0.04257946461439133
Message from 0:  	 Batch:1320, loss:0.04172990471124649
Message from 0:  	 Batch:1321, loss:0.036163460463285446
Message from 0:  	 Batch:1322, loss:0.04195043444633484
Message from 0:  	 Batch:1323, loss:0.042477820068597794
Message from 0:  	 Batch:1324, loss:0.0418749563395977
Message from 0:  	 Batch:1325, loss:0.048952896147966385
Message from 0:  	 Batch:1326, loss:0.050978802144527435
Message from 0:  	 Batch:1327, loss:0.05024411901831627
Message from 0:  	 Batch:1328, loss:0.03405040502548218
Message from 0:  	 Batch:1329, loss:0.05225864797830582
Message from 0:  	 Batch:1330, loss:0.04680241644382477
Message from 0:  	 Batch:1331, loss:0.047807477414608
Message from 0:  	 Batch:1332, loss:0.05085242539644241
Message from 0:  	 Batch:1333, loss:0.04637806862592697
Message from 0:  	 Batch:1334, loss:0.05352950841188431
Message from 0:  	 Batch:1335, loss:0.04823242872953415
Message from 0:  	 Batch:1336, loss:0.033230073750019073
Message from 0:  	 Batch:1337, loss:0.04071326181292534
Message from 0:  	 Batch:1338, loss:0.04493347927927971
Message from 0:  	 Batch:1339, loss:0.05902564525604248
Message from 0:  	 Batch:1340, loss:0.05134574696421623
Message from 0:  	 Batch:1341, loss:0.05387003719806671
Message from 0:  	 Batch:1342, loss:0.04792024940252304
Message from 0:  	 Batch:1343, loss:0.060699205845594406
Message from 0:  	 Batch:1344, loss:0.04886624962091446
Message from 0:  	 Batch:1345, loss:0.04726279526948929
Message from 0:  	 Batch:1346, loss:0.04784664511680603
Message from 0:  	 Batch:1347, loss:0.044487111270427704
Message from 0:  	 Batch:1348, loss:0.05729566141963005
Message from 0:  	 Batch:1349, loss:0.0434737354516983
Message from 0:  	 Batch:1350, loss:0.04386136680841446
Message from 0:  	 Batch:1351, loss:0.04909095913171768
Message from 0:  	 Batch:1352, loss:0.04417186975479126
Message from 0:  	 Batch:1353, loss:0.05348964035511017
Message from 0:  	 Batch:1354, loss:0.0632595494389534
Message from 0:  	 Batch:1355, loss:0.03500378131866455
Message from 0:  	 Batch:1356, loss:0.04797846078872681
Message from 0:  	 Batch:1357, loss:0.045027900487184525
Message from 0:  	 Batch:1358, loss:0.05376908928155899
Message from 0:  	 Batch:1359, loss:0.048559583723545074
Message from 0:  	 Batch:1360, loss:0.05334312096238136
Message from 0:  	 Batch:1361, loss:0.0664643719792366
Message from 0:  	 Batch:1362, loss:0.06656359881162643
Message from 0:  	 Batch:1363, loss:0.04407122731208801
Message from 0:  	 Batch:1364, loss:0.04711911827325821
Message from 0:  	 Batch:1365, loss:0.0522741861641407
Message from 0:  	 Batch:1366, loss:0.04068969935178757
Message from 0:  	 Batch:1367, loss:0.046762775629758835
Message from 0:  	 Batch:1368, loss:0.06550590693950653
Message from 0:  	 Batch:1369, loss:0.03916456922888756
Message from 0:  	 Batch:1370, loss:0.0609687939286232
Message from 0:  	 Batch:1371, loss:0.048958905041217804
Message from 0:  	 Batch:1372, loss:0.054239146411418915
Message from 0:  	 Batch:1373, loss:0.04055338352918625
Message from 0:  	 Batch:1374, loss:0.04969555139541626
Message from 0:  	 Batch:1375, loss:0.04471351206302643
Message from 0:  	 Batch:1376, loss:0.036358073353767395
Message from 0:  	 Batch:1377, loss:0.0418698713183403
Message from 0:  	 Batch:1378, loss:0.06690126657485962
Message from 0:  	 Batch:1379, loss:0.049101006239652634
Message from 0:  	 Batch:1380, loss:0.043441291898489
Message from 0:  	 Batch:1381, loss:0.036406684666872025
Message from 0:  	 Batch:1382, loss:0.030850108712911606
Message from 0:  	 Batch:1383, loss:0.043710075318813324
Message from 0:  	 Batch:1384, loss:0.040464915335178375
Message from 0:  	 Batch:1385, loss:0.04901295155286789
Message from 0:  	 Batch:1386, loss:0.06538145244121552
Message from 0:  	 Batch:1387, loss:0.06542772799730301
Message from 0:  	 Batch:1388, loss:0.05011579021811485
Message from 0:  	 Batch:1389, loss:0.038370780646800995
Message from 0:  	 Batch:1390, loss:0.04155496135354042
Message from 0:  	 Batch:1391, loss:0.036196816712617874
Message from 0:  	 Batch:1392, loss:0.04387705773115158
Message from 0:  	 Batch:1393, loss:0.039846017956733704
Message from 0:  	 Batch:1394, loss:0.02977459877729416
Message from 0:  	 Batch:1395, loss:0.050507813692092896
Message from 0:  	 Batch:1396, loss:0.04458725452423096
Message from 0:  	 Batch:1397, loss:0.035036277025938034
Message from 0:  	 Batch:1398, loss:0.04099593684077263
Message from 0:  	 Batch:1399, loss:0.03582876920700073
Message from 0:  	 Batch:1400, loss:0.04767029359936714
Message from 0:  	 Batch:1401, loss:0.06512634456157684
Message from 0:  	 Batch:1402, loss:0.04561392217874527
Message from 0:  	 Batch:1403, loss:0.04296421632170677
Message from 0:  	 Batch:1404, loss:0.030193589627742767
Message from 0:  	 Batch:1405, loss:0.040867168456315994
Message from 0:  	 Batch:1406, loss:0.054107584059238434
Message from 0:  	 Batch:1407, loss:0.06808051466941833
Message from 0:  	 Batch:1408, loss:0.040701135993003845
Message from 0:  	 Batch:1409, loss:0.04983741417527199
Message from 0:  	 Batch:1410, loss:0.04393717646598816
Message from 0:  	 Batch:1411, loss:0.05552331358194351
Message from 0:  	 Batch:1412, loss:0.06119810417294502
Message from 0:  	 Batch:1413, loss:0.06906064599752426
Message from 0:  	 Batch:1414, loss:0.0501624271273613
Message from 0:  	 Batch:1415, loss:0.050407975912094116
Message from 0:  	 Batch:1416, loss:0.058840952813625336
Message from 0:  	 Batch:1417, loss:0.04371290281414986
Message from 0:  	 Batch:1418, loss:0.04934556037187576
Message from 0:  	 Batch:1419, loss:0.05005154758691788
Message from 0:  	 Batch:1420, loss:0.034030672162771225
Message from 0:  	 Batch:1421, loss:0.03561348468065262
Message from 0:  	 Batch:1422, loss:0.04791367053985596
Message from 0:  	 Batch:1423, loss:0.062152083963155746
Message from 0:  	 Batch:1424, loss:0.04649137333035469
Message from 0:  	 Batch:1425, loss:0.037064552307128906
Message from 0:  	 Batch:1426, loss:0.07052932679653168
Message from 0:  	 Batch:1427, loss:0.07195807248353958
Message from 0:  	 Batch:1428, loss:0.04050813987851143
Message from 0:  	 Batch:1429, loss:0.049585938453674316
Message from 0:  	 Batch:1430, loss:0.03734665736556053
Message from 0:  	 Batch:1431, loss:0.062385089695453644
Message from 0:  	 Batch:1432, loss:0.04104505479335785
Message from 0:  	 Batch:1433, loss:0.04142967611551285
Message from 0:  	 Batch:1434, loss:0.04953664168715477
Message from 0:  	 Batch:1435, loss:0.047964684665203094
Message from 0:  	 Batch:1436, loss:0.04573880881071091
Message from 0:  	 Batch:1437, loss:0.04277535527944565
Message from 0:  	 Batch:1438, loss:0.04905208945274353
Message from 0:  	 Batch:1439, loss:0.048986949026584625
Message from 0:  	 Batch:1440, loss:0.054044172167778015
Message from 0:  	 Batch:1441, loss:0.04949804022908211
Message from 0:  	 Batch:1442, loss:0.041837699711322784
Message from 0:  	 Batch:1443, loss:0.04159855097532272
Message from 0:  	 Batch:1444, loss:0.049472928047180176
Message from 0:  	 Batch:1445, loss:0.04489409551024437
Message from 0:  	 Batch:1446, loss:0.07259920984506607
Message from 0:  	 Batch:1447, loss:0.05097884684801102
Message from 0:  	 Batch:1448, loss:0.07274597138166428
Message from 0:  	 Batch:1449, loss:0.057137854397296906
Message from 0:  	 Batch:1450, loss:0.062497228384017944
Message from 0:  	 Batch:1451, loss:0.049281973391771317
Message from 0:  	 Batch:1452, loss:0.046178750693798065
Message from 0:  	 Batch:1453, loss:0.06357823312282562
Message from 0:  	 Batch:1454, loss:0.05237100273370743
Message from 0:  	 Batch:1455, loss:0.04342846944928169
Message from 0:  	 Batch:1456, loss:0.04990062862634659
Message from 0:  	 Batch:1457, loss:0.044925786554813385
Message from 0:  	 Batch:1458, loss:0.04554447531700134
Message from 0:  	 Batch:1459, loss:0.055068954825401306
Message from 0:  	 Batch:1460, loss:0.0358043909072876
Message from 0:  	 Batch:1461, loss:0.06372034549713135
Message from 0:  	 Batch:1462, loss:0.036122433841228485
Message from 0:  	 Batch:1463, loss:0.06565698981285095
Message from 0:  	 Batch:1464, loss:0.0793556272983551
Message from 0:  	 Batch:1465, loss:0.06078862398862839
Message from 0:  	 Batch:1466, loss:0.04382628947496414
Message from 0:  	 Batch:1467, loss:0.040817487984895706
Message from 0:  	 Batch:1468, loss:0.04124651104211807
Message from 0:  	 Batch:1469, loss:0.06433354318141937
Message from 0:  	 Batch:1470, loss:0.04490312188863754
Message from 0:  	 Batch:1471, loss:0.05536159873008728
Message from 0:  	 Batch:1472, loss:0.047329869121313095
Message from 0:  	 Batch:1473, loss:0.046281665563583374
Message from 0:  	 Batch:1474, loss:0.034330613911151886
Message from 0:  	 Batch:1475, loss:0.03883569687604904
Message from 0:  	 Batch:1476, loss:0.03437428921461105
Message from 0:  	 Batch:1477, loss:0.05300803482532501
Message from 0:  	 Batch:1478, loss:0.05204878747463226
Message from 0:  	 Batch:1479, loss:0.04469997435808182
Message from 0:  	 Batch:1480, loss:0.03548077493906021
Message from 0:  	 Batch:1481, loss:0.049067769199609756
Message from 0:  	 Batch:1482, loss:0.04570538550615311
Message from 0:  	 Batch:1483, loss:0.03826076537370682
Message from 0:  	 Batch:1484, loss:0.04126737266778946
Message from 0:  	 Batch:1485, loss:0.06555874645709991
Message from 0:  	 Batch:1486, loss:0.0357213020324707
Message from 0:  	 Batch:1487, loss:0.04186359792947769
Message from 0:  	 Batch:1488, loss:0.057831697165966034
Message from 0:  	 Batch:1489, loss:0.048839569091796875
Message from 0:  	 Batch:1490, loss:0.06421907246112823
Message from 0:  	 Batch:1491, loss:0.04662225395441055
Message from 0:  	 Batch:1492, loss:0.047967828810214996
Message from 0:  	 Batch:1493, loss:0.032983601093292236
Message from 0:  	 Batch:1494, loss:0.03966769576072693
Message from 0:  	 Batch:1495, loss:0.06974238157272339
Message from 0:  	 Batch:1496, loss:0.06907323002815247
Message from 0:  	 Batch:1497, loss:0.04471602290868759
Message from 0:  	 Batch:1498, loss:0.028832482174038887
Message from 0:  	 Batch:1499, loss:0.05219856649637222
Message from 0:  	 Batch:1500, loss:0.03481448069214821
Message from 0:  	 Batch:1501, loss:0.04180854558944702
Message from 0:  	 Batch:1502, loss:0.04531653970479965
Message from 0:  	 Batch:1503, loss:0.04401274397969246
Message from 0:  	 Batch:1504, loss:0.0494803711771965
Message from 0:  	 Batch:1505, loss:0.04966331645846367
Message from 0:  	 Batch:1506, loss:0.03992181643843651
Message from 0:  	 Batch:1507, loss:0.048628345131874084
Message from 0:  	 Batch:1508, loss:0.0329672209918499
Message from 0:  	 Batch:1509, loss:0.04711168259382248
Message from 0:  	 Batch:1510, loss:0.04912455379962921
Message from 0:  	 Batch:1511, loss:0.062077004462480545
Message from 0:  	 Batch:1512, loss:0.043564654886722565
Message from 0:  	 Batch:1513, loss:0.04576268047094345
Message from 0:  	 Batch:1514, loss:0.0727921649813652
Message from 0:  	 Batch:1515, loss:0.046653494238853455
Message from 0:  	 Batch:1516, loss:0.04070177301764488
Message from 0:  	 Batch:1517, loss:0.034569501876831055
Message from 0:  	 Batch:1518, loss:0.050734441727399826
Message from 0:  	 Batch:1519, loss:0.03955464065074921
Message from 0:  	 Batch:1520, loss:0.038410939276218414
Message from 0:  	 Batch:1521, loss:0.0361996591091156
Message from 0:  	 Batch:1522, loss:0.07018697261810303
Message from 0:  	 Batch:1523, loss:0.052359096705913544
Message from 0:  	 Batch:1524, loss:0.02275092527270317
Message from 0:  	 Batch:1525, loss:0.04432394355535507
Message from 0:  	 Batch:1526, loss:0.04744614660739899
Message from 0:  	 Batch:1527, loss:0.044271841645240784
Message from 0:  	 Batch:1528, loss:0.03397195041179657
Message from 0:  	 Batch:1529, loss:0.04522502422332764
Message from 0:  	 Batch:1530, loss:0.04677861928939819
Message from 0:  	 Batch:1531, loss:0.04784276336431503
Message from 0:  	 Batch:1532, loss:0.04379838705062866
Message from 0:  	 Batch:1533, loss:0.04384101182222366
Message from 0:  	 Batch:1534, loss:0.04429009556770325
Message from 0:  	 Batch:1535, loss:0.04895033687353134
Message from 0:  	 Batch:1536, loss:0.02324259653687477
Message from 0:  	 Batch:1537, loss:0.06066926568746567
Message from 0:  	 Batch:1538, loss:0.05106189101934433
Message from 0:  	 Batch:1539, loss:0.04554910957813263
Message from 0:  	 Batch:1540, loss:0.045432448387145996
Message from 0:  	 Batch:1541, loss:0.048116497695446014
Message from 0:  	 Batch:1542, loss:0.06404295563697815
Message from 0:  	 Batch:1543, loss:0.04403996467590332
Message from 0:  	 Batch:1544, loss:0.05134538561105728
Message from 0:  	 Batch:1545, loss:0.037518732249736786
Message from 0:  	 Batch:1546, loss:0.04907897859811783
Message from 0:  	 Batch:1547, loss:0.05189201235771179
Message from 0:  	 Batch:1548, loss:0.046910110861063004
Message from 0:  	 Batch:1549, loss:0.056673046201467514
Message from 0:  	 Batch:1550, loss:0.05561274662613869
Message from 0:  	 Batch:1551, loss:0.047866109758615494
Message from 0:  	 Batch:1552, loss:0.04839961975812912
Message from 0:  	 Batch:1553, loss:0.06505100429058075
Message from 0:  	 Batch:1554, loss:0.05288565158843994
Message from 0:  	 Batch:1555, loss:0.04594821482896805
Message from 0:  	 Batch:1556, loss:0.05185627564787865
Message from 0:  	 Batch:1557, loss:0.04834578186273575
Message from 0:  	 Batch:1558, loss:0.050433769822120667
Message from 0:  	 Batch:1559, loss:0.06746716797351837
Message from 0:  	 Batch:1560, loss:0.0465828999876976
Message from 0:  	 Batch:1561, loss:0.04513515532016754
Message from 0:  	 Batch:1562, loss:0.04549528658390045
Message from 0:  	 Batch:1563, loss:0.04284481704235077
Message from 0:  	 Batch:1564, loss:0.04261504113674164
Message from 0:  	 Batch:1565, loss:0.03487510234117508
Message from 0:  	 Batch:1566, loss:0.054238468408584595
Message from 0:  	 Batch:1567, loss:0.043804313987493515
Message from 0:  	 Batch:1568, loss:0.0531744547188282
Message from 0:  	 Batch:1569, loss:0.04589982330799103
Message from 0:  	 Batch:1570, loss:0.04715031012892723
Message from 0:  	 Batch:1571, loss:0.04707618057727814
Message from 0:  	 Batch:1572, loss:0.04737790673971176
Message from 0:  	 Batch:1573, loss:0.037850238382816315
Message from 0:  	 Batch:1574, loss:0.04473824054002762
Message from 0:  	 Batch:1575, loss:0.05178910493850708
Message from 0:  	 Batch:1576, loss:0.03359914571046829
Message from 0:  	 Batch:1577, loss:0.03629282861948013
Message from 0:  	 Batch:1578, loss:0.043085742741823196
Message from 0:  	 Batch:1579, loss:0.04270084947347641
Message from 0:  	 Batch:1580, loss:0.05111963301897049
Message from 0:  	 Batch:1581, loss:0.04377565532922745
Message from 0:  	 Batch:1582, loss:0.04673053324222565
Message from 0:  	 Batch:1583, loss:0.0510408990085125
Message from 0:  	 Batch:1584, loss:0.03831086307764053
Message from 0:  	 Batch:1585, loss:0.0511665903031826
Message from 0:  	 Batch:1586, loss:0.06452466547489166
Message from 0:  	 Batch:1587, loss:0.0482945591211319
Message from 0:  	 Batch:1588, loss:0.04669013246893883
Message from 0:  	 Batch:1589, loss:0.08388673514127731
Message from 0:  	 Batch:1590, loss:0.030329128727316856
Message from 0:  	 Batch:1591, loss:0.038615308701992035
Message from 0:  	 Batch:1592, loss:0.04394974559545517
Message from 0:  	 Batch:1593, loss:0.04041937366127968
Message from 0:  	 Batch:1594, loss:0.0460987351834774
Message from 0:  	 Batch:1595, loss:0.036947160959243774
Message from 0:  	 Batch:1596, loss:0.04417473077774048
Message from 0:  	 Batch:1597, loss:0.048564452677965164
Message from 0:  	 Batch:1598, loss:0.0451497808098793
Message from 0:  	 Batch:1599, loss:0.044161610305309296
Message from 0:  	 Batch:1600, loss:0.04083884507417679
Message from 0:  	 Batch:1601, loss:0.04522138833999634
Message from 0:  	 Batch:1602, loss:0.04490510746836662
Message from 0:  	 Batch:1603, loss:0.045128416270017624
Message from 0:  	 Batch:1604, loss:0.04613196849822998
Message from 0:  	 Batch:1605, loss:0.05087938532233238
Message from 0:  	 Batch:1606, loss:0.04775933176279068
Message from 0:  	 Batch:1607, loss:0.049178916960954666
Message from 0:  	 Batch:1608, loss:0.04976367950439453
Message from 0:  	 Batch:1609, loss:0.04336996376514435
Message from 0:  	 Batch:1610, loss:0.05061754211783409
Message from 0:  	 Batch:1611, loss:0.06205478310585022
Message from 0:  	 Batch:1612, loss:0.044235631823539734
Message from 0:  	 Batch:1613, loss:0.049454666674137115
Message from 0:  	 Batch:1614, loss:0.046661704778671265
Message from 0:  	 Batch:1615, loss:0.04906591400504112
Message from 0:  	 Batch:1616, loss:0.06714166700839996
Message from 0:  	 Batch:1617, loss:0.0438225120306015
Message from 0:  	 Batch:1618, loss:0.040411919355392456
Message from 0:  	 Batch:1619, loss:0.048608288168907166
Message from 0:  	 Batch:1620, loss:0.05081888288259506
Message from 0:  	 Batch:1621, loss:0.04889512434601784
Message from 0:  	 Batch:1622, loss:0.03791246563196182
Message from 0:  	 Batch:1623, loss:0.06081085279583931
Message from 0:  	 Batch:1624, loss:0.04272724315524101
Message from 0:  	 Batch:1625, loss:0.04932490736246109
Message from 0:  	 Batch:1626, loss:0.04101862758398056
Message from 0:  	 Batch:1627, loss:0.04938935488462448
Message from 0:  	 Batch:1628, loss:0.04641830176115036
Message from 0:  	 Batch:1629, loss:0.05567005276679993
Message from 0:  	 Batch:1630, loss:0.04161258786916733
Message from 0:  	 Batch:1631, loss:0.0615386962890625
Message from 0:  	 Batch:1632, loss:0.05699802190065384
Message from 0:  	 Batch:1633, loss:0.05678655579686165
Message from 0:  	 Batch:1634, loss:0.04401890188455582
Message from 0:  	 Batch:1635, loss:0.04517608880996704
Message from 0:  	 Batch:1636, loss:0.04779001325368881
Message from 0:  	 Batch:1637, loss:0.03253631293773651
Message from 0:  	 Batch:1638, loss:0.04786718264222145
Message from 0:  	 Batch:1639, loss:0.049831997603178024
Message from 0:  	 Batch:1640, loss:0.0452403649687767
Message from 0:  	 Batch:1641, loss:0.042642056941986084
Message from 0:  	 Batch:1642, loss:0.0481380969285965
Message from 0:  	 Batch:1643, loss:0.04914779216051102
Message from 0:  	 Batch:1644, loss:0.03316270932555199
Message from 0:  	 Batch:1645, loss:0.04788009449839592
Message from 0:  	 Batch:1646, loss:0.044894590973854065
Message from 0:  	 Batch:1647, loss:0.03869117051362991
Message from 0:  	 Batch:1648, loss:0.07916881144046783
Message from 0:  	 Batch:1649, loss:0.0534975528717041
Message from 0:  	 Batch:1650, loss:0.05040166527032852
Message from 0:  	 Batch:1651, loss:0.04366953670978546
Message from 0:  	 Batch:1652, loss:0.047018785029649734
Message from 0:  	 Batch:1653, loss:0.04627978801727295
Message from 0:  	 Batch:1654, loss:0.035904839634895325
Message from 0:  	 Batch:1655, loss:0.04389781504869461
Message from 0:  	 Batch:1656, loss:0.04204437881708145
Message from 0:  	 Batch:1657, loss:0.02865559607744217
Message from 0:  	 Batch:1658, loss:0.04642634093761444
Message from 0:  	 Batch:1659, loss:0.05160525441169739
Message from 0:  	 Batch:1660, loss:0.047407712787389755
Message from 0:  	 Batch:1661, loss:0.05251204967498779
Message from 0:  	 Batch:1662, loss:0.04857508838176727
Message from 0:  	 Batch:1663, loss:0.043856095522642136
Message from 0:  	 Batch:1664, loss:0.044451482594013214
Message from 0:  	 Batch:1665, loss:0.05127383768558502
Message from 0:  	 Batch:1666, loss:0.044832371175289154
Message from 0:  	 Batch:1667, loss:0.04166886955499649
Message from 0:  	 Batch:1668, loss:0.03428710997104645
Message from 0:  	 Batch:1669, loss:0.059240445494651794
Message from 0:  	 Batch:1670, loss:0.04756902530789375
Message from 0:  	 Batch:1671, loss:0.03886256366968155
Message from 0:  	 Batch:1672, loss:0.041289567947387695
Message from 0:  	 Batch:1673, loss:0.04762972891330719
Message from 0:  	 Batch:1674, loss:0.03562938794493675
Message from 0:  	 Batch:1675, loss:0.07135283946990967
Message from 0:  	 Batch:1676, loss:0.04716630280017853
Message from 0:  	 Batch:1677, loss:0.046566739678382874
Message from 0:  	 Batch:1678, loss:0.042328935116529465
Message from 0:  	 Batch:1679, loss:0.06208103895187378
Message from 0:  	 Batch:1680, loss:0.06906965374946594
Message from 0:  	 Batch:1681, loss:0.051236022263765335
Message from 0:  	 Batch:1682, loss:0.059112921357154846
Message from 0:  	 Batch:1683, loss:0.04758322611451149
Message from 0:  	 Batch:1684, loss:0.03224979713559151
Message from 0:  	 Batch:1685, loss:0.04485392943024635
Message from 0:  	 Batch:1686, loss:0.050466328859329224
Message from 0:  	 Batch:1687, loss:0.05178392678499222
Message from 0:  	 Batch:1688, loss:0.05205123871564865
Message from 0:  	 Batch:1689, loss:0.03588929772377014
Message from 0:  	 Batch:1690, loss:0.055070940405130386
Message from 0:  	 Batch:1691, loss:0.060408562421798706
Message from 0:  	 Batch:1692, loss:0.06810538470745087
Message from 0:  	 Batch:1693, loss:0.03418164327740669
Message from 0:  	 Batch:1694, loss:0.05428434908390045
Message from 0:  	 Batch:1695, loss:0.05495716631412506
Message from 0:  	 Batch:1696, loss:0.04904398322105408
Message from 0:  	 Batch:1697, loss:0.04635100066661835
Message from 0:  	 Batch:1698, loss:0.05208008363842964
Message from 0:  	 Batch:1699, loss:0.042678385972976685
Message from 0:  	 Batch:1700, loss:0.0337350033223629
Message from 0:  	 Batch:1701, loss:0.04839261621236801
Message from 0:  	 Batch:1702, loss:0.05241234228014946
Message from 0:  	 Batch:1703, loss:0.05949346721172333
Message from 0:  	 Batch:1704, loss:0.07255901396274567
Message from 0:  	 Batch:1705, loss:0.03425498306751251
Message from 0:  	 Batch:1706, loss:0.05044134706258774
Message from 0:  	 Batch:1707, loss:0.04331502318382263
Message from 0:  	 Batch:1708, loss:0.05379767343401909
Message from 0:  	 Batch:1709, loss:0.04643716663122177
Message from 0:  	 Batch:1710, loss:0.05107393115758896
Message from 0:  	 Batch:1711, loss:0.040482960641384125
Message from 0:  	 Batch:1712, loss:0.04478705674409866
Message from 0:  	 Batch:1713, loss:0.04917965829372406
Message from 0:  	 Batch:1714, loss:0.043418027460575104
Message from 0:  	 Batch:1715, loss:0.0775315910577774
Message from 0:  	 Batch:1716, loss:0.05837012827396393
Message from 0:  	 Batch:1717, loss:0.05239744484424591
Message from 0:  	 Batch:1718, loss:0.04516526684165001
Message from 0:  	 Batch:1719, loss:0.04696492850780487
Message from 0:  	 Batch:1720, loss:0.03492732346057892
Message from 0:  	 Batch:1721, loss:0.04615428298711777
Message from 0:  	 Batch:1722, loss:0.04745197296142578
Message from 0:  	 Batch:1723, loss:0.04387081414461136
Message from 0:  	 Batch:1724, loss:0.047567218542099
Message from 0:  	 Batch:1725, loss:0.028805028647184372
Message from 0:  	 Batch:1726, loss:0.05391518771648407
Message from 0:  	 Batch:1727, loss:0.05923368036746979
Message from 0:  	 Batch:1728, loss:0.03616371005773544
Message from 0:  	 Batch:1729, loss:0.055485233664512634
Message from 0:  	 Batch:1730, loss:0.03951685130596161
Message from 0:  	 Batch:1731, loss:0.03959231823682785
Message from 0:  	 Batch:1732, loss:0.05074889957904816
Message from 0:  	 Batch:1733, loss:0.05880395323038101
Message from 0:  	 Batch:1734, loss:0.0478639230132103
Message from 0:  	 Batch:1735, loss:0.05217956006526947
Message from 0:  	 Batch:1736, loss:0.04420442506670952
Message from 0:  	 Batch:1737, loss:0.05457421392202377
Message from 0:  	 Batch:1738, loss:0.059198521077632904
Message from 0:  	 Batch:1739, loss:0.05018267780542374
Message from 0:  	 Batch:1740, loss:0.04614416882395744
Message from 0:  	 Batch:1741, loss:0.03402938321232796
Message from 0:  	 Batch:1742, loss:0.053395308554172516
Message from 0:  	 Batch:1743, loss:0.05084392428398132
Message from 0:  	 Batch:1744, loss:0.0332932248711586
Message from 0:  	 Batch:1745, loss:0.0772823765873909
Message from 0:  	 Batch:1746, loss:0.059425827115774155
Message from 0:  	 Batch:1747, loss:0.05671359971165657
Message from 0:  	 Batch:1748, loss:0.05106624215841293
Message from 0:  	 Batch:1749, loss:0.0440843291580677
Message from 0:  	 Batch:1750, loss:0.046247195452451706
Message from 0:  	 Batch:1751, loss:0.06707358360290527
Message from 0:  	 Batch:1752, loss:0.050394296646118164
Message from 0:  	 Batch:1753, loss:0.052922897040843964
Message from 0:  	 Batch:1754, loss:0.1018226146697998
Message from 0:  	 Batch:1755, loss:0.047026343643665314
Message from 0:  	 Batch:1756, loss:0.0485241636633873
Message from 0:  	 Batch:1757, loss:0.03215852752327919
Message from 0:  	 Batch:1758, loss:0.04813925176858902
Message from 0:  	 Batch:1759, loss:0.034159038215875626
Message from 0:  	 Batch:1760, loss:0.03419320285320282
Message from 0:  	 Batch:1761, loss:0.04527907818555832
Message from 0:  	 Batch:1762, loss:0.04253849387168884
Message from 0:  	 Batch:1763, loss:0.04814400523900986
Message from 0:  	 Batch:1764, loss:0.045981742441654205
Message from 0:  	 Batch:1765, loss:0.02349284291267395
Message from 0:  	 Batch:1766, loss:0.05032791197299957
Message from 0:  	 Batch:1767, loss:0.03869201987981796
Message from 0:  	 Batch:1768, loss:0.04504835605621338
Message from 0:  	 Batch:1769, loss:0.04609464481472969
Message from 0:  	 Batch:1770, loss:0.050468817353248596
Message from 0:  	 Batch:1771, loss:0.025084273889660835
Message from 0:  	 Batch:1772, loss:0.04726453125476837
Message from 0:  	 Batch:1773, loss:0.048786066472530365
Message from 0:  	 Batch:1774, loss:0.05844613164663315
Message from 0:  	 Batch:1775, loss:0.048127636313438416
Message from 0:  	 Batch:1776, loss:0.05133228003978729
Message from 0:  	 Batch:1777, loss:0.04210399463772774
Message from 0:  	 Batch:1778, loss:0.042684972286224365
Message from 0:  	 Batch:1779, loss:0.03849726915359497
Message from 0:  	 Batch:1780, loss:0.07411696016788483
Message from 0:  	 Batch:1781, loss:0.06643930077552795
Message from 0:  	 Batch:1782, loss:0.04273474961519241
Message from 0:  	 Batch:1783, loss:0.03851712495088577
Message from 0:  	 Batch:1784, loss:0.057631708681583405
Message from 0:  	 Batch:1785, loss:0.053656503558158875
Message from 0:  	 Batch:1786, loss:0.03783578425645828
Message from 0:  	 Batch:1787, loss:0.04607813060283661
Message from 0:  	 Batch:1788, loss:0.05281086638569832
Message from 0:  	 Batch:1789, loss:0.04895322024822235
Message from 0:  	 Batch:1790, loss:0.0372948944568634
Message from 0:  	 Batch:1791, loss:0.0478743351995945
Message from 0:  	 Batch:1792, loss:0.04431293159723282
Message from 0:  	 Batch:1793, loss:0.03711041808128357
Message from 0:  	 Batch:1794, loss:0.0564248152077198
Message from 0:  	 Batch:1795, loss:0.0516994446516037
Message from 0:  	 Batch:1796, loss:0.048750147223472595
Message from 0:  	 Batch:1797, loss:0.04364537447690964
Message from 0:  	 Batch:1798, loss:0.05029549077153206
Message from 0:  	 Batch:1799, loss:0.06270242482423782
Message from 0:  	 Batch:1800, loss:0.04525286704301834
Message from 0:  	 Batch:1801, loss:0.036684583872556686
Message from 0:  	 Batch:1802, loss:0.044876545667648315
Message from 0:  	 Batch:1803, loss:0.05020175129175186
Message from 0:  	 Batch:1804, loss:0.06303355097770691
Message from 0:  	 Batch:1805, loss:0.05861864238977432
Message from 0:  	 Batch:1806, loss:0.04494337737560272
Message from 0:  	 Batch:1807, loss:0.04515615850687027
Message from 0:  	 Batch:1808, loss:0.04963618144392967
Message from 0:  	 Batch:1809, loss:0.04880198836326599
Message from 0:  	 Batch:1810, loss:0.04080352187156677
Message from 0:  	 Batch:1811, loss:0.05572479963302612
Message from 0:  	 Batch:1812, loss:0.047344427555799484
Message from 0:  	 Batch:1813, loss:0.04756680876016617
Message from 0:  	 Batch:1814, loss:0.04027785360813141
Message from 0:  	 Batch:1815, loss:0.04812808334827423
Message from 0:  	 Batch:1816, loss:0.052035264670848846
Message from 0:  	 Batch:1817, loss:0.0426592156291008
Message from 0:  	 Batch:1818, loss:0.03733059763908386
Message from 0:  	 Batch:1819, loss:0.04054439067840576
Message from 0:  	 Batch:1820, loss:0.04823896288871765
Message from 0:  	 Batch:1821, loss:0.03803730756044388
Message from 0:  	 Batch:1822, loss:0.05255141481757164
Message from 0:  	 Batch:1823, loss:0.053538162261247635
Message from 0:  	 Batch:1824, loss:0.0464395135641098
Message from 0:  	 Batch:1825, loss:0.031749092042446136
Message from 0:  	 Batch:1826, loss:0.0433088093996048
Message from 0:  	 Batch:1827, loss:0.04385185241699219
Message from 0:  	 Batch:1828, loss:0.04774757847189903
Message from 0:  	 Batch:1829, loss:0.04435563087463379
Message from 0:  	 Batch:1830, loss:0.046189043670892715
Message from 0:  	 Batch:1831, loss:0.04302101582288742
Message from 0:  	 Batch:1832, loss:0.043184682726860046
Message from 0:  	 Batch:1833, loss:0.05545256659388542
Message from 0:  	 Batch:1834, loss:0.04665893316268921
Message from 0:  	 Batch:1835, loss:0.02292238548398018
Message from 0:  	 Batch:1836, loss:0.04612860828638077
Message from 0:  	 Batch:1837, loss:0.041739944368600845
Message from 0:  	 Batch:1838, loss:0.038126081228256226
Message from 0:  	 Batch:1839, loss:0.044304393231868744
Message from 0:  	 Batch:1840, loss:0.04439681023359299
Message from 0:  	 Batch:1841, loss:0.04979289323091507
Message from 0:  	 Batch:1842, loss:0.040630415081977844
Message from 0:  	 Batch:1843, loss:0.05228415131568909
Message from 0:  	 Batch:1844, loss:0.04994148761034012
Message from 0:  	 Batch:1845, loss:0.04365786537528038
Message from 0:  	 Batch:1846, loss:0.042843349277973175
Message from 0:  	 Batch:1847, loss:0.05119708180427551
Message from 0:  	 Batch:1848, loss:0.03897339850664139
Message from 0:  	 Batch:1849, loss:0.04965119808912277
Message from 0:  	 Batch:1850, loss:0.056652382016181946
Message from 0:  	 Batch:1851, loss:0.046591997146606445
Message from 0:  	 Batch:1852, loss:0.04018130898475647
Message from 0:  	 Batch:1853, loss:0.044497597962617874
Message from 0:  	 Batch:1854, loss:0.041591234505176544
Message from 0:  	 Batch:1855, loss:0.057141490280628204
Message from 0:  	 Batch:1856, loss:0.041458792984485626
Message from 0:  	 Batch:1857, loss:0.03338541463017464
Message from 0:  	 Batch:1858, loss:0.03164852410554886
Message from 0:  	 Batch:1859, loss:0.07356934249401093
Message from 0:  	 Batch:1860, loss:0.04835277050733566
Message from 0:  	 Batch:1861, loss:0.05281268060207367
Message from 0:  	 Batch:1862, loss:0.04755011200904846
Message from 0:  	 Batch:1863, loss:0.05184607207775116
Message from 0:  	 Batch:1864, loss:0.035501547157764435
Message from 0:  	 Batch:1865, loss:0.0531308576464653
Message from 0:  	 Batch:1866, loss:0.051456332206726074
Message from 0:  	 Batch:1867, loss:0.04625915735960007
Message from 0:  	 Batch:1868, loss:0.05034150928258896
Message from 0:  	 Batch:1869, loss:0.04397521913051605
Message from 0:  	 Batch:1870, loss:0.04425418749451637
Message from 0:  	 Batch:1871, loss:0.051999107003211975
Message from 0:  	 Batch:1872, loss:0.04733050614595413
Message from 0:  	 Batch:1873, loss:0.0434105321764946
Message from 0:  	 Batch:1874, loss:0.05064799636602402
Message from 0:  	 Batch:1875, loss:0.03181388974189758
Message from 0:  	 Batch:1876, loss:0.04990365356206894
Message from 0:  	 Batch:1877, loss:0.040036238729953766
Message from 0:  	 Batch:1878, loss:0.03633466362953186
Message from 0:  	 Batch:1879, loss:0.050122447311878204
Message from 0:  	 Batch:1880, loss:0.041978731751441956
Message from 0:  	 Batch:1881, loss:0.05670306459069252
Message from 0:  	 Batch:1882, loss:0.04357624799013138
Message from 0:  	 Batch:1883, loss:0.054263968020677567
Message from 0:  	 Batch:1884, loss:0.05908471345901489
Message from 0:  	 Batch:1885, loss:0.04140008985996246
Message from 0:  	 Batch:1886, loss:0.05973399430513382
Message from 0:  	 Batch:1887, loss:0.03688555210828781
Message from 0:  	 Batch:1888, loss:0.047167710959911346
Message from 0:  	 Batch:1889, loss:0.044646255671978
Message from 0:  	 Batch:1890, loss:0.04149913042783737
Message from 0:  	 Batch:1891, loss:0.04459086060523987
Message from 0:  	 Batch:1892, loss:0.045278578996658325
Message from 0:  	 Batch:1893, loss:0.03446924686431885
Message from 0:  	 Batch:1894, loss:0.03904258832335472
Message from 0:  	 Batch:1895, loss:0.05458875000476837
Message from 0:  	 Batch:1896, loss:0.04984661191701889
Message from 0:  	 Batch:1897, loss:0.04683159291744232
Message from 0:  	 Batch:1898, loss:0.053990885615348816
Message from 0:  	 Batch:1899, loss:0.04427731782197952
Message from 0:  	 Batch:1900, loss:0.06916443258523941
Message from 0:  	 Batch:1901, loss:0.042189739644527435
Message from 0:  	 Batch:1902, loss:0.06964601576328278
Message from 0:  	 Batch:1903, loss:0.04822038114070892
Message from 0:  	 Batch:1904, loss:0.05884251743555069
Message from 0:  	 Batch:1905, loss:0.07356039434671402
Message from 0:  	 Batch:1906, loss:0.04634071886539459
Message from 0:  	 Batch:1907, loss:0.04233954846858978
Message from 0:  	 Batch:1908, loss:0.04525730758905411
Message from 0:  	 Batch:1909, loss:0.03620140254497528
Message from 0:  	 Batch:1910, loss:0.0649130791425705
Message from 0:  	 Batch:1911, loss:0.04535559564828873
Message from 0:  	 Batch:1912, loss:0.04895397648215294
Message from 0:  	 Batch:1913, loss:0.036278534680604935
Message from 0:  	 Batch:1914, loss:0.05098795145750046
Message from 0:  	 Batch:1915, loss:0.0462561771273613
Message from 0:  	 Batch:1916, loss:0.04746953397989273
Message from 0:  	 Batch:1917, loss:0.047879908233881
Message from 0:  	 Batch:1918, loss:0.0626721978187561
Message from 0:  	 Batch:1919, loss:0.04989554360508919
Message from 0:  	 Batch:1920, loss:0.045845214277505875
Message from 0:  	 Batch:1921, loss:0.05668383836746216
Message from 0:  	 Batch:1922, loss:0.05391862243413925
Message from 0:  	 Batch:1923, loss:0.050066642463207245
Message from 0:  	 Batch:1924, loss:0.047728367149829865
Message from 0:  	 Batch:1925, loss:0.046099573373794556
Message from 0:  	 Batch:1926, loss:0.03013133630156517
Message from 0:  	 Batch:1927, loss:0.035877324640750885
Message from 0:  	 Batch:1928, loss:0.05198521912097931
Message from 0:  	 Batch:1929, loss:0.054170265793800354
Message from 0:  	 Batch:1930, loss:0.042578794062137604
Message from 0:  	 Batch:1931, loss:0.0644938051700592
Message from 0:  	 Batch:1932, loss:0.05354011803865433
Message from 0:  	 Batch:1933, loss:0.047876495867967606
Message from 0:  	 Batch:1934, loss:0.0456877276301384
Message from 0:  	 Batch:1935, loss:0.04522983729839325
Message from 0:  	 Batch:1936, loss:0.04572131484746933
Message from 0:  	 Batch:1937, loss:0.04920978099107742
Message from 0:  	 Batch:1938, loss:0.04366098344326019
Message from 0:  	 Batch:1939, loss:0.03959481418132782
Message from 0:  	 Batch:1940, loss:0.038021594285964966
Message from 0:  	 Batch:1941, loss:0.054357610642910004
Message from 0:  	 Batch:1942, loss:0.04492414742708206
Message from 0:  	 Batch:1943, loss:0.05292439088225365
Message from 0:  	 Batch:1944, loss:0.04269162192940712
Message from 0:  	 Batch:1945, loss:0.050189584493637085
Message from 0:  	 Batch:1946, loss:0.05312474071979523
Message from 0:  	 Batch:1947, loss:0.050524089485406876
Message from 0:  	 Batch:1948, loss:0.03940678387880325
Message from 0:  	 Batch:1949, loss:0.05706720054149628
Message from 0:  	 Batch:1950, loss:0.044780146330595016
Message from 0:  	 Batch:1951, loss:0.050917848944664
Message from 0:  	 Batch:1952, loss:0.03429131582379341
Message from 0:  	 Batch:1953, loss:0.04590699449181557
Message from 0:  	 Batch:1954, loss:0.0284293033182621
Message from 0:  	 Batch:1955, loss:0.03396042063832283
Message from 0:  	 Batch:1956, loss:0.04629819840192795
Message from 0:  	 Batch:1957, loss:0.046766430139541626
Message from 0:  	 Batch:1958, loss:0.04083114117383957
Message from 0:  	 Batch:1959, loss:0.048220694065093994
Message from 0:  	 Batch:1960, loss:0.051708441227674484
Message from 0:  	 Batch:1961, loss:0.047470711171627045
Message from 0:  	 Batch:1962, loss:0.046116940677165985
Message from 0:  	 Batch:1963, loss:0.07072214782238007
Message from 0:  	 Batch:1964, loss:0.07415741682052612
Message from 0:  	 Batch:1965, loss:0.03270331025123596
Message from 0:  	 Batch:1966, loss:0.06643379479646683
Message from 0:  	 Batch:1967, loss:0.06483325362205505
Message from 0:  	 Batch:1968, loss:0.040944233536720276
Message from 0:  	 Batch:1969, loss:0.06556190550327301
Message from 0:  	 Batch:1970, loss:0.03224695473909378
Message from 0:  	 Batch:1971, loss:0.06009320169687271
Message from 0:  	 Batch:1972, loss:0.04871046170592308
Message from 0:  	 Batch:1973, loss:0.04029584676027298
Message from 0:  	 Batch:1974, loss:0.06547223776578903
Message from 0:  	 Batch:1975, loss:0.04048483073711395
Message from 0:  	 Batch:1976, loss:0.05170489847660065
Message from 0:  	 Batch:1977, loss:0.06975635886192322
Message from 0:  	 Batch:1978, loss:0.046109702438116074
Message from 0:  	 Batch:1979, loss:0.04173444211483002
Message from 0:  	 Batch:1980, loss:0.03758896887302399
Message from 0:  	 Batch:1981, loss:0.04461926594376564
Message from 0:  	 Batch:1982, loss:0.04532768577337265
Message from 0:  	 Batch:1983, loss:0.047020990401506424
Message from 0:  	 Batch:1984, loss:0.06499631702899933
Message from 0:  	 Batch:1985, loss:0.0629362091422081
Message from 0:  	 Batch:1986, loss:0.04536235332489014
Message from 0:  	 Batch:1987, loss:0.04126059263944626
Message from 0:  	 Batch:1988, loss:0.045364394783973694
Message from 0:  	 Batch:1989, loss:0.064236581325531
Message from 0:  	 Batch:1990, loss:0.03701940178871155
Message from 0:  	 Batch:1991, loss:0.05750880390405655
Message from 0:  	 Batch:1992, loss:0.05500968173146248
Message from 0:  	 Batch:1993, loss:0.04269862174987793
Message from 0:  	 Batch:1994, loss:0.043020643293857574
Message from 0:  	 Batch:1995, loss:0.06126895174384117
Message from 0:  	 Batch:1996, loss:0.0484900027513504
Message from 0:  	 Batch:1997, loss:0.05239187926054001
Message from 0:  	 Batch:1998, loss:0.04417373612523079
Message from 0:  	 Batch:1999, loss:0.045219264924526215
Message from 0:  	 Batch:2000, loss:0.044026847928762436
Message from 0:  	 Batch:2001, loss:0.06057598441839218
Message from 0:  	 Batch:2002, loss:0.04566558450460434
Message from 0:  	 Batch:2003, loss:0.046251118183135986
Message from 0:  	 Batch:2004, loss:0.06452101469039917
Message from 0:  	 Batch:2005, loss:0.05416989326477051
Message from 0:  	 Batch:2006, loss:0.03548278659582138
Message from 0:  	 Batch:2007, loss:0.0478828065097332
Message from 0:  	 Batch:2008, loss:0.037173792719841
Message from 0:  	 Batch:2009, loss:0.0474201925098896
Message from 0:  	 Batch:2010, loss:0.03810993582010269
Message from 0:  	 Batch:2011, loss:0.06742502003908157
Message from 0:  	 Batch:2012, loss:0.04913916066288948
Message from 0:  	 Batch:2013, loss:0.04445486515760422
Message from 0:  	 Batch:2014, loss:0.05175873637199402
Message from 0:  	 Batch:2015, loss:0.07319387793540955
Message from 0:  	 Batch:2016, loss:0.04621846228837967
Message from 0:  	 Batch:2017, loss:0.05097145587205887
Message from 0:  	 Batch:2018, loss:0.04884558916091919
Message from 0:  	 Batch:2019, loss:0.03863653540611267
Message from 0:  	 Batch:2020, loss:0.04561721161007881
Message from 0:  	 Batch:2021, loss:0.041192688047885895
Message from 0:  	 Batch:2022, loss:0.044799335300922394
Message from 0:  	 Batch:2023, loss:0.04732799530029297
Message from 0:  	 Batch:2024, loss:0.056251347064971924
Message from 0:  	 Batch:2025, loss:0.044364407658576965
Message from 0:  	 Batch:2026, loss:0.05039477348327637
Message from 0:  	 Batch:2027, loss:0.04604633152484894
Message from 0:  	 Batch:2028, loss:0.04382313787937164
Message from 0:  	 Batch:2029, loss:0.04101503640413284
Message from 0:  	 Batch:2030, loss:0.04241760075092316
Message from 0:  	 Batch:2031, loss:0.0481911227107048
Message from 0:  	 Batch:2032, loss:0.04622601717710495
Message from 0:  	 Batch:2033, loss:0.06469645351171494
Message from 0:  	 Batch:2034, loss:0.03261260315775871
Message from 0:  	 Batch:2035, loss:0.04549126327037811
Message from 0:  	 Batch:2036, loss:0.04540945589542389
Message from 0:  	 Batch:2037, loss:0.04461653158068657
Message from 0:  	 Batch:2038, loss:0.04258938878774643
Message from 0:  	 Batch:2039, loss:0.045462556183338165
Message from 0:  	 Batch:2040, loss:0.04833325743675232
Message from 0:  	 Batch:2041, loss:0.04598652571439743
Message from 0:  	 Batch:2042, loss:0.04497753828763962
Message from 0:  	 Batch:2043, loss:0.046946555376052856
Message from 0:  	 Batch:2044, loss:0.07185213267803192
Message from 0:  	 Batch:2045, loss:0.0492895171046257
Message from 0:  	 Batch:2046, loss:0.046501271426677704
Message from 0:  	 Batch:2047, loss:0.049582019448280334
Message from 0:  	 Batch:2048, loss:0.04877559840679169
Message from 0:  	 Batch:2049, loss:0.030908450484275818
Message from 0:  	 Batch:2050, loss:0.05718519538640976
Message from 0:  	 Batch:2051, loss:0.04876170679926872
Message from 0:  	 Batch:2052, loss:0.04832804948091507
Message from 0:  	 Batch:2053, loss:0.04220841825008392
Message from 0:  	 Batch:2054, loss:0.031174995005130768
Message from 0:  	 Batch:2055, loss:0.0471414178609848
Message from 0:  	 Batch:2056, loss:0.04599691927433014
Message from 0:  	 Batch:2057, loss:0.05140172690153122
Message from 0:  	 Batch:2058, loss:0.04256577789783478
Message from 0:  	 Batch:2059, loss:0.03357769176363945
Message from 0:  	 Batch:2060, loss:0.034704744815826416
Message from 0:  	 Batch:2061, loss:0.04810696840286255
Message from 0:  	 Batch:2062, loss:0.05394759029150009
Message from 0:  	 Batch:2063, loss:0.04465640336275101
Message from 0:  	 Batch:2064, loss:0.042933594435453415
Message from 0:  	 Batch:2065, loss:0.04404667392373085
Message from 0:  	 Batch:2066, loss:0.06749048829078674
Message from 0:  	 Batch:2067, loss:0.04436715692281723
Message from 0:  	 Batch:2068, loss:0.04747049883008003
Message from 0:  	 Batch:2069, loss:0.02883537858724594
Message from 0:  	 Batch:2070, loss:0.035957179963588715
Message from 0:  	 Batch:2071, loss:0.043868593871593475
Message from 0:  	 Batch:2072, loss:0.04715476930141449
Message from 0:  	 Batch:2073, loss:0.056892164051532745
Message from 0:  	 Batch:2074, loss:0.05199547857046127
Message from 0:  	 Batch:2075, loss:0.04907694458961487
Message from 0:  	 Batch:2076, loss:0.04515954479575157
Message from 0:  	 Batch:2077, loss:0.043113578110933304
Message from 0:  	 Batch:2078, loss:0.04993923008441925
Message from 0:  	 Batch:2079, loss:0.04686662554740906
Message from 0:  	 Batch:2080, loss:0.05913376808166504
Message from 0:  	 Batch:2081, loss:0.06610725820064545
Message from 0:  	 Batch:2082, loss:0.044784218072891235
Message from 0:  	 Batch:2083, loss:0.08424696326255798
Message from 0:  	 Batch:2084, loss:0.046215951442718506
Message from 0:  	 Batch:2085, loss:0.034916482865810394
Message from 0:  	 Batch:2086, loss:0.05077876150608063
Message from 0:  	 Batch:2087, loss:0.04412302374839783
Message from 0:  	 Batch:2088, loss:0.03848148137331009
Message from 0:  	 Batch:2089, loss:0.04860353469848633
Message from 0:  	 Batch:2090, loss:0.039999667555093765
Message from 0:  	 Batch:2091, loss:0.046306855976581573
Message from 0:  	 Batch:2092, loss:0.044033389538526535
Message from 0:  	 Batch:2093, loss:0.04264276847243309
Message from 0:  	 Batch:2094, loss:0.047569580376148224
Message from 0:  	 Batch:2095, loss:0.046877436339855194
Message from 0:  	 Batch:2096, loss:0.05388534069061279
Message from 0:  	 Batch:2097, loss:0.05605752766132355
Message from 0:  	 Batch:2098, loss:0.038208041340112686
Message from 0:  	 Batch:2099, loss:0.04942617565393448
Message from 0:  	 Batch:2100, loss:0.06290803104639053
Message from 0:  	 Batch:2101, loss:0.03617952764034271
Message from 0:  	 Batch:2102, loss:0.03352534770965576
Message from 0:  	 Batch:2103, loss:0.04834271967411041
Message from 0:  	 Batch:2104, loss:0.06496323645114899
Message from 0:  	 Batch:2105, loss:0.06392387300729752
Message from 0:  	 Batch:2106, loss:0.05883907154202461
Message from 0:  	 Batch:2107, loss:0.04520602524280548
Message from 0:  	 Batch:2108, loss:0.0426461361348629
Message from 0:  	 Batch:2109, loss:0.05026963725686073
Message from 0:  	 Batch:2110, loss:0.03898084908723831
Message from 0:  	 Batch:2111, loss:0.04215061664581299
Message from 0:  	 Batch:2112, loss:0.04755014181137085
Message from 0:  	 Batch:2113, loss:0.03923493251204491
Message from 0:  	 Batch:2114, loss:0.04199941083788872
Message from 0:  	 Batch:2115, loss:0.040554292500019073
Message from 0:  	 Batch:2116, loss:0.05116922780871391
Message from 0:  	 Batch:2117, loss:0.04195374622941017
Message from 0:  	 Batch:2118, loss:0.05039139837026596
Message from 0:  	 Batch:2119, loss:0.04140709713101387
Message from 0:  	 Batch:2120, loss:0.04383472353219986
Message from 0:  	 Batch:2121, loss:0.043326400220394135
Message from 0:  	 Batch:2122, loss:0.04591239243745804
Message from 0:  	 Batch:2123, loss:0.02447475492954254
Message from 0:  	 Batch:2124, loss:0.0457739531993866
Message from 0:  	 Batch:2125, loss:0.0589209645986557
Message from 0:  	 Batch:2126, loss:0.05869351327419281
Message from 0:  	 Batch:2127, loss:0.05160478129982948
Message from 0:  	 Batch:2128, loss:0.04813959077000618
Message from 0:  	 Batch:2129, loss:0.06840820610523224
Message from 0:  	 Batch:2130, loss:0.04778140038251877
Message from 0:  	 Batch:2131, loss:0.051058270037174225
Message from 0:  	 Batch:2132, loss:0.04677470400929451
Message from 0:  	 Batch:2133, loss:0.04332263395190239
Message from 0:  	 Batch:2134, loss:0.042780518531799316
Message from 0:  	 Batch:2135, loss:0.04832072556018829
Message from 0:  	 Batch:2136, loss:0.04183308780193329
Message from 0:  	 Batch:2137, loss:0.04651239141821861
Message from 0:  	 Batch:2138, loss:0.04829759895801544
Message from 0:  	 Batch:2139, loss:0.03761632740497589
Message from 0:  	 Batch:2140, loss:0.04837464541196823
Message from 0:  	 Batch:2141, loss:0.049403585493564606
Message from 0:  	 Batch:2142, loss:0.051197975873947144
Message from 0:  	 Batch:2143, loss:0.035423897206783295
Message from 0:  	 Batch:2144, loss:0.04749032482504845
Message from 0:  	 Batch:2145, loss:0.04645116627216339
Message from 0:  	 Batch:2146, loss:0.05612379312515259
Message from 0:  	 Batch:2147, loss:0.03805946558713913
Message from 0:  	 Batch:2148, loss:0.038252927362918854
Message from 0:  	 Batch:2149, loss:0.0629083514213562
Message from 0:  	 Batch:2150, loss:0.04619930684566498
Message from 0:  	 Batch:2151, loss:0.05287756025791168
Message from 0:  	 Batch:2152, loss:0.05154229700565338
Message from 0:  	 Batch:2153, loss:0.020144058391451836
Message from 0:  	 Batch:2154, loss:0.05892418324947357
Message from 0:  	 Batch:2155, loss:0.05342244356870651
Message from 0:  	 Batch:2156, loss:0.046573080122470856
Message from 0:  	 Batch:2157, loss:0.03279545158147812
Message from 0:  	 Batch:2158, loss:0.04490097612142563
Message from 0:  	 Batch:2159, loss:0.06261108070611954
Message from 0:  	 Batch:2160, loss:0.08004162460565567
Message from 0:  	 Batch:2161, loss:0.043327052146196365
Message from 0:  	 Batch:2162, loss:0.04979921504855156
Message from 0:  	 Batch:2163, loss:0.04899156466126442
Message from 0:  	 Batch:2164, loss:0.08485545217990875
Message from 0:  	 Batch:2165, loss:0.04166831821203232
Message from 0:  	 Batch:2166, loss:0.04387480393052101
Message from 0:  	 Batch:2167, loss:0.04095477610826492
Message from 0:  	 Batch:2168, loss:0.05872102826833725
Message from 0:  	 Batch:2169, loss:0.047165483236312866
Message from 0:  	 Batch:2170, loss:0.052792251110076904
Message from 0:  	 Batch:2171, loss:0.04734613001346588
Message from 0:  	 Batch:2172, loss:0.03154429793357849
Message from 0:  	 Batch:2173, loss:0.037067100405693054
Message from 0:  	 Batch:2174, loss:0.04419770836830139
Message from 0:  	 Batch:2175, loss:0.04087712615728378
Message from 0:  	 Batch:2176, loss:0.07254419475793839
Message from 0:  	 Batch:2177, loss:0.05133264139294624
Message from 0:  	 Batch:2178, loss:0.044313400983810425
Message from 0:  	 Batch:2179, loss:0.04558631032705307
Message from 0:  	 Batch:2180, loss:0.04696068912744522
Message from 0:  	 Batch:2181, loss:0.04334552586078644
Message from 0:  	 Batch:2182, loss:0.046745665371418
Message from 0:  	 Batch:2183, loss:0.07674272358417511
Message from 0:  	 Batch:2184, loss:0.043865084648132324
Message from 0:  	 Batch:2185, loss:0.0429193340241909
Message from 0:  	 Batch:2186, loss:0.058986712247133255
Message from 0:  	 Batch:2187, loss:0.0381675660610199
Message from 0:  	 Batch:2188, loss:0.05348264053463936
Message from 0:  	 Batch:2189, loss:0.04314402863383293
Message from 0:  	 Batch:2190, loss:0.10051283240318298
Message from 0:  	 Batch:2191, loss:0.047073617577552795
Message from 0:  	 Batch:2192, loss:0.04641316086053848
Message from 0:  	 Batch:2193, loss:0.051018450409173965
Message from 0:  	 Batch:2194, loss:0.03814416378736496
Message from 0:  	 Batch:2195, loss:0.041116055101156235
Message from 0:  	 Batch:2196, loss:0.047911569476127625
Message from 0:  	 Batch:2197, loss:0.0467817597091198
Message from 0:  	 Batch:2198, loss:0.05588490888476372
Message from 0:  	 Batch:2199, loss:0.045478954911231995
Message from 0:  	 Batch:2200, loss:0.0458475798368454
Message from 0:  	 Batch:2201, loss:0.047157879918813705
Message from 0:  	 Batch:2202, loss:0.05451228469610214
Message from 0:  	 Batch:2203, loss:0.048575688153505325
Message from 0:  	 Batch:2204, loss:0.03765903785824776
Message from 0:  	 Batch:2205, loss:0.0382673516869545
Message from 0:  	 Batch:2206, loss:0.05165059119462967
Message from 0:  	 Batch:2207, loss:0.046335358172655106
Message from 0:  	 Batch:2208, loss:0.0404345728456974
Message from 0:  	 Batch:2209, loss:0.020080439746379852
Message from 0:  	 Batch:2210, loss:0.042394667863845825
Message from 0:  	 Batch:2211, loss:0.05404362082481384
Message from 0:  	 Batch:2212, loss:0.04323812946677208
Message from 0:  	 Batch:2213, loss:0.032738544046878815
Message from 0:  	 Batch:2214, loss:0.07011783868074417
Message from 0:  	 Batch:2215, loss:0.049889709800481796
Message from 0:  	 Batch:2216, loss:0.05392860248684883
Message from 0:  	 Batch:2217, loss:0.029778767377138138
Message from 0:  	 Batch:2218, loss:0.05808015167713165
Message from 0:  	 Batch:2219, loss:0.04780936986207962
Message from 0:  	 Batch:2220, loss:0.06579906493425369
Message from 0:  	 Batch:2221, loss:0.04173710197210312
Message from 0:  	 Batch:2222, loss:0.04595975950360298
Message from 0:  	 Batch:2223, loss:0.0492122583091259
Message from 0:  	 Batch:2224, loss:0.04614553600549698
Message from 0:  	 Batch:2225, loss:0.05398401618003845
Message from 0:  	 Batch:2226, loss:0.04515068233013153
Message from 0:  	 Batch:2227, loss:0.0489228293299675
Message from 0:  	 Batch:2228, loss:0.039136022329330444
Message from 0:  	 Batch:2229, loss:0.04699157550930977
Message from 0:  	 Batch:2230, loss:0.04542861133813858
Message from 0:  	 Batch:2231, loss:0.07181528210639954
Message from 0:  	 Batch:2232, loss:0.058305397629737854
Message from 0:  	 Batch:2233, loss:0.12729576230049133
Message from 0:  	 Batch:2234, loss:0.046806883066892624
Message from 0:  	 Batch:2235, loss:0.04255349189043045
Message from 0:  	 Batch:2236, loss:0.054917000234127045
Message from 0:  	 Batch:2237, loss:0.04506654292345047
Message from 0:  	 Batch:2238, loss:0.04601995646953583
Message from 0:  	 Batch:2239, loss:0.027439724653959274
Message from 0:  	 Batch:2240, loss:0.043392911553382874
Message from 0:  	 Batch:2241, loss:0.0465165376663208
Message from 0:  	 Batch:2242, loss:0.04491472244262695
Message from 0:  	 Batch:2243, loss:0.04782279580831528
Message from 0:  	 Batch:2244, loss:0.03422991931438446
Message from 0:  	 Batch:2245, loss:0.03503996506333351
Message from 0:  	 Batch:2246, loss:0.06209447234869003
Message from 0:  	 Batch:2247, loss:0.03874391317367554
Message from 0:  	 Batch:2248, loss:0.03478836268186569
Message from 0:  	 Batch:2249, loss:0.04154197871685028
Message from 0:  	 Batch:2250, loss:0.051033321768045425
Message from 0:  	 Batch:2251, loss:0.04500875249505043
Message from 0:  	 Batch:2252, loss:0.07320555299520493
Message from 0:  	 Batch:2253, loss:0.045511335134506226
Message from 0:  	 Batch:2254, loss:0.04778209701180458
Message from 0:  	 Batch:2255, loss:0.04831577092409134
Message from 0:  	 Batch:2256, loss:0.051392652094364166
Message from 0:  	 Batch:2257, loss:0.07466046512126923
Message from 0:  	 Batch:2258, loss:0.041566506028175354
Message from 0:  	 Batch:2259, loss:0.05539649724960327
Message from 0:  	 Batch:2260, loss:0.05529634654521942
Message from 0:  	 Batch:2261, loss:0.04750930145382881
Message from 0:  	 Batch:2262, loss:0.04888582229614258
Message from 0:  	 Batch:2263, loss:0.0655793845653534
Message from 0:  	 Batch:2264, loss:0.04910116270184517
Message from 0:  	 Batch:2265, loss:0.05345892161130905
Message from 0:  	 Batch:2266, loss:0.04125238209962845
Message from 0:  	 Batch:2267, loss:0.04274154081940651
Message from 0:  	 Batch:2268, loss:0.04613230749964714
Message from 0:  	 Batch:2269, loss:0.04336366802453995
Message from 0:  	 Batch:2270, loss:0.035860441625118256
Message from 0:  	 Batch:2271, loss:0.036687638610601425
Message from 0:  	 Batch:2272, loss:0.044040657579898834
Message from 0:  	 Batch:2273, loss:0.041234858334064484
Message from 0:  	 Batch:2274, loss:0.04187644273042679
Message from 0:  	 Batch:2275, loss:0.0406021922826767
Message from 0:  	 Batch:2276, loss:0.0446263924241066
Message from 0:  	 Batch:2277, loss:0.06370086222887039
Message from 0:  	 Batch:2278, loss:0.04073986038565636
Message from 0:  	 Batch:2279, loss:0.045493707060813904
Message from 0:  	 Batch:2280, loss:0.04142624884843826
Message from 0:  	 Batch:2281, loss:0.051879897713661194
Message from 0:  	 Batch:2282, loss:0.03295391798019409
Message from 0:  	 Batch:2283, loss:0.042485930025577545
Message from 0:  	 Batch:2284, loss:0.04353982210159302
Message from 0:  	 Batch:2285, loss:0.053017377853393555
Message from 0:  	 Batch:2286, loss:0.040394339710474014
Message from 0:  	 Batch:2287, loss:0.04198407381772995
Message from 0:  	 Batch:2288, loss:0.06960763037204742
Message from 0:  	 Batch:2289, loss:0.04737653583288193
Message from 0:  	 Batch:2290, loss:0.03943410515785217
Message from 0:  	 Batch:2291, loss:0.03529434651136398
Message from 0:  	 Batch:2292, loss:0.04524242877960205
Message from 0:  	 Batch:2293, loss:0.045758478343486786
Message from 0:  	 Batch:2294, loss:0.04633772373199463
Message from 0:  	 Batch:2295, loss:0.048509109765291214
Message from 0:  	 Batch:2296, loss:0.06222201883792877
Message from 0:  	 Batch:2297, loss:0.047712355852127075
Message from 0:  	 Batch:2298, loss:0.044259727001190186
Message from 0:  	 Batch:2299, loss:0.07173886895179749
Message from 0:  	 Batch:2300, loss:0.05125782638788223
Message from 0:  	 Batch:2301, loss:0.03395196795463562
Message from 0:  	 Batch:2302, loss:0.07545514404773712
Message from 0:  	 Batch:2303, loss:0.04884229227900505
Message from 0:  	 Batch:2304, loss:0.030272144824266434
Message from 0:  	 Batch:2305, loss:0.0382976233959198
Message from 0:  	 Batch:2306, loss:0.04490584135055542
Message from 0:  	 Batch:2307, loss:0.055533312261104584
Message from 0:  	 Batch:2308, loss:0.03570466488599777
Message from 0:  	 Batch:2309, loss:0.050392523407936096
Message from 0:  	 Batch:2310, loss:0.04320676624774933
Message from 0:  	 Batch:2311, loss:0.03133199363946915
Message from 0:  	 Batch:2312, loss:0.0392327643930912
Message from 0:  	 Batch:2313, loss:0.04716482758522034
Message from 0:  	 Batch:2314, loss:0.04766775667667389
Message from 0:  	 Batch:2315, loss:0.04263917729258537
Message from 0:  	 Batch:2316, loss:0.037310417741537094
Message from 0:  	 Batch:2317, loss:0.04044125974178314
Message from 0:  	 Batch:2318, loss:0.04829783737659454
Message from 0:  	 Batch:2319, loss:0.046706072986125946
Message from 0:  	 Batch:2320, loss:0.03514205664396286
Message from 0:  	 Batch:2321, loss:0.039287641644477844
Message from 0:  	 Batch:2322, loss:0.04713573679327965
Message from 0:  	 Batch:2323, loss:0.04554295539855957
Message from 0:  	 Batch:2324, loss:0.041974663734436035
Message from 0:  	 Batch:2325, loss:0.049011457711458206
Message from 0:  	 Batch:2326, loss:0.0490713007748127
Message from 0:  	 Batch:2327, loss:0.040194444358348846
Message from 0:  	 Batch:2328, loss:0.05207638442516327
Message from 0:  	 Batch:2329, loss:0.04485929384827614
Message from 0:  	 Batch:2330, loss:0.0449664369225502
Message from 0:  	 Batch:2331, loss:0.05047962814569473
Message from 0:  	 Batch:2332, loss:0.04642088711261749
Message from 0:  	 Batch:2333, loss:0.04563282057642937
Message from 0:  	 Batch:2334, loss:0.0456538088619709
Message from 0:  	 Batch:2335, loss:0.04564577341079712
Message from 0:  	 Batch:2336, loss:0.04941168799996376
Message from 0:  	 Batch:2337, loss:0.05124136805534363
Message from 0:  	 Batch:2338, loss:0.04658523201942444
Message from 0:  	 Batch:2339, loss:0.050844382494688034
Message from 0:  	 Batch:2340, loss:0.06058313697576523
Message from 0:  	 Batch:2341, loss:0.043365396559238434
Message from 0:  	 Batch:2342, loss:0.050395697355270386
Message from 0:  	 Batch:2343, loss:0.04163718968629837
Message from 0:  	 Batch:2344, loss:0.0455755814909935
Message from 0:  	 Batch:2345, loss:0.053132589906454086
Message from 0:  	 Batch:2346, loss:0.04782970994710922
Message from 0:  	 Batch:2347, loss:0.07248570024967194
Message from 0:  	 Batch:2348, loss:0.04142868518829346
Message from 0:  	 Batch:2349, loss:0.04012627154588699
Message from 0:  	 Batch:2350, loss:0.05080559849739075
Message from 0:  	 Batch:2351, loss:0.04809540882706642
Message from 0:  	 Batch:2352, loss:0.050152648240327835
Message from 0:  	 Batch:2353, loss:0.07368435710668564
Message from 0:  	 Batch:2354, loss:0.03149820864200592
Message from 0:  	 Batch:2355, loss:0.04619283974170685
Message from 0:  	 Batch:2356, loss:0.04576339200139046
Message from 0:  	 Batch:2357, loss:0.05721910297870636
Message from 0:  	 Batch:2358, loss:0.033571094274520874
Message from 0:  	 Batch:2359, loss:0.04554134979844093
Message from 0:  	 Batch:2360, loss:0.04373446851968765
Message from 0:  	 Batch:2361, loss:0.05337410420179367
Message from 0:  	 Batch:2362, loss:0.06953468173742294
Message from 0:  	 Batch:2363, loss:0.048305146396160126
Message from 0:  	 Batch:2364, loss:0.059936653822660446
Message from 0:  	 Batch:2365, loss:0.045891594141721725
Message from 0:  	 Batch:2366, loss:0.046127185225486755
Message from 0:  	 Batch:2367, loss:0.046583354473114014
Message from 0:  	 Batch:2368, loss:0.05161762982606888
Message from 0:  	 Batch:2369, loss:0.05180363729596138
Message from 0:  	 Batch:2370, loss:0.03754609078168869
Message from 0:  	 Batch:2371, loss:0.049520447850227356
Message from 0:  	 Batch:2372, loss:0.0372171476483345
Message from 0:  	 Batch:2373, loss:0.06660272926092148
Message from 0:  	 Batch:2374, loss:0.05777377635240555
Message from 0:  	 Batch:2375, loss:0.03151128441095352
Message from 0:  	 Batch:2376, loss:0.04014534503221512
Message from 0:  	 Batch:2377, loss:0.06619381904602051
Message from 0:  	 Batch:2378, loss:0.05324489623308182
Message from 0:  	 Batch:2379, loss:0.04743543639779091
Message from 0:  	 Batch:2380, loss:0.042264070361852646
Message from 0:  	 Batch:2381, loss:0.04996895417571068
Message from 0:  	 Batch:2382, loss:0.051883891224861145
Message from 0:  	 Batch:2383, loss:0.0459502637386322
Message from 0:  	 Batch:2384, loss:0.0570288822054863
Message from 0:  	 Batch:2385, loss:0.053324051201343536
Message from 0:  	 Batch:2386, loss:0.04962802678346634
Message from 0:  	 Batch:2387, loss:0.053955353796482086
Message from 0:  	 Batch:2388, loss:0.05069805681705475
Message from 0:  	 Batch:2389, loss:0.051424432545900345
Message from 0:  	 Batch:2390, loss:0.04747183620929718
Message from 0:  	 Batch:2391, loss:0.056755319237709045
Message from 0:  	 Batch:2392, loss:0.04763859510421753
Message from 0:  	 Batch:2393, loss:0.04234442114830017
Message from 0:  	 Batch:2394, loss:0.05413328483700752
Message from 0:  	 Batch:2395, loss:0.021132521331310272
Message from 0:  	 Batch:2396, loss:0.047877803444862366
Message from 0:  	 Batch:2397, loss:0.046149253845214844
Message from 0:  	 Batch:2398, loss:0.049664050340652466
Message from 0:  	 Batch:2399, loss:0.056872911751270294
Message from 0:  	 Batch:2400, loss:0.048122555017471313
Message from 0:  	 Batch:2401, loss:0.049374669790267944
Message from 0:  	 Batch:2402, loss:0.04733988270163536
Message from 0:  	 Batch:2403, loss:0.05115436390042305
Message from 0:  	 Batch:2404, loss:0.057349979877471924
Message from 0:  	 Batch:2405, loss:0.03721657395362854
Message from 0:  	 Batch:2406, loss:0.06179187446832657
Message from 0:  	 Batch:2407, loss:0.048771925270557404
Message from 0:  	 Batch:2408, loss:0.05114979296922684
Message from 0:  	 Batch:2409, loss:0.044535938650369644
Message from 0:  	 Batch:2410, loss:0.04710829630494118
Message from 0:  	 Batch:2411, loss:0.04489053785800934
Message from 0:  	 Batch:2412, loss:0.04822259396314621
Message from 0:  	 Batch:2413, loss:0.07252249121665955
Message from 0:  	 Batch:2414, loss:0.04364686459302902
Message from 0:  	 Batch:2415, loss:0.03798846900463104
Message from 0:  	 Batch:2416, loss:0.04955378919839859
Message from 0:  	 Batch:2417, loss:0.04543101042509079
Message from 0:  	 Batch:2418, loss:0.04751669615507126
Message from 0:  	 Batch:2419, loss:0.049299921840429306
Message from 0:  	 Batch:2420, loss:0.040621861815452576
Message from 0:  	 Batch:2421, loss:0.049041979014873505
Message from 0:  	 Batch:2422, loss:0.051578156650066376
Message from 0:  	 Batch:2423, loss:0.04529618099331856
Message from 0:  	 Batch:2424, loss:0.05853156000375748
Message from 0:  	 Batch:2425, loss:0.06331910938024521
Message from 0:  	 Batch:2426, loss:0.05045439675450325
Message from 0:  	 Batch:2427, loss:0.09440898150205612
Message from 0:  	 Batch:2428, loss:0.039578430354595184
Message from 0:  	 Batch:2429, loss:0.06757493317127228
Message from 0:  	 Batch:2430, loss:0.04766668379306793
Message from 0:  	 Batch:2431, loss:0.04042378067970276
Message from 0:  	 Batch:2432, loss:0.04809267073869705
Message from 0:  	 Batch:2433, loss:0.04457031190395355
Message from 0:  	 Batch:2434, loss:0.04365801438689232
Message from 0:  	 Batch:2435, loss:0.055823639035224915
Message from 0:  	 Batch:2436, loss:0.044978875666856766
Message from 0:  	 Batch:2437, loss:0.03918665647506714
Message from 0:  	 Batch:2438, loss:0.048654697835445404
Message from 0:  	 Batch:2439, loss:0.05731002241373062
Message from 0:  	 Batch:2440, loss:0.04561387374997139
Message from 0:  	 Batch:2441, loss:0.04561331123113632
Message from 0:  	 Batch:2442, loss:0.056763701140880585
Message from 0:  	 Batch:2443, loss:0.049402423202991486
Message from 0:  	 Batch:2444, loss:0.05796865373849869
Message from 0:  	 Batch:2445, loss:0.043944235891103745
Message from 0:  	 Batch:2446, loss:0.05985599011182785
Message from 0:  	 Batch:2447, loss:0.04805465042591095
Message from 0:  	 Batch:2448, loss:0.03852614760398865
Message from 0:  	 Batch:2449, loss:0.03580606356263161
Message from 0:  	 Batch:2450, loss:0.04686509072780609
Message from 0:  	 Batch:2451, loss:0.05653761327266693
Message from 0:  	 Batch:2452, loss:0.030744114890694618
Message from 0:  	 Batch:2453, loss:0.04873810335993767
Message from 0:  	 Batch:2454, loss:0.04486357420682907
Message from 0:  	 Batch:2455, loss:0.07839761674404144
Message from 0:  	 Batch:2456, loss:0.046486929059028625
Message from 0:  	 Batch:2457, loss:0.045626476407051086
Message from 0:  	 Batch:2458, loss:0.05153348296880722
Message from 0:  	 Batch:2459, loss:0.04737967997789383
Message from 0:  	 Batch:2460, loss:0.045887771993875504
Message from 0:  	 Batch:2461, loss:0.04139865189790726
Message from 0:  	 Batch:2462, loss:0.04615939408540726
Message from 0:  	 Batch:2463, loss:0.04384660720825195
Message from 0:  	 Batch:2464, loss:0.05170891433954239
Message from 0:  	 Batch:2465, loss:0.06957507133483887
Message from 0:  	 Batch:2466, loss:0.08915413916110992
Message from 0:  	 Batch:2467, loss:0.04164087027311325
Message from 0:  	 Batch:2468, loss:0.04998431354761124
Message from 0:  	 Batch:2469, loss:0.06639517843723297
Message from 0:  	 Batch:2470, loss:0.05553407967090607
Message from 0:  	 Batch:2471, loss:0.039253056049346924
Message from 0:  	 Batch:2472, loss:0.048374004662036896
Message from 0:  	 Batch:2473, loss:0.06008404493331909
Message from 0:  	 Batch:2474, loss:0.039512112736701965
Message from 0:  	 Batch:2475, loss:0.04421849921345711
Message from 0:  	 Batch:2476, loss:0.044676218181848526
Message from 0:  	 Batch:2477, loss:0.059337735176086426
Message from 0:  	 Batch:2478, loss:0.052138954401016235
Message from 0:  	 Batch:2479, loss:0.043931469321250916
Message from 0:  	 Batch:2480, loss:0.04577481374144554
Message from 0:  	 Batch:2481, loss:0.045983538031578064
Message from 0:  	 Batch:2482, loss:0.049210548400878906
Message from 0:  	 Batch:2483, loss:0.04003802314400673
Message from 0:  	 Batch:2484, loss:0.041912488639354706
Message from 0:  	 Batch:2485, loss:0.07339096814393997
Message from 0:  	 Batch:2486, loss:0.03497275337576866
Message from 0:  	 Batch:2487, loss:0.0386478416621685
Message from 0:  	 Batch:2488, loss:0.04882477968931198
Message from 0:  	 Batch:2489, loss:0.03927727788686752
Message from 0:  	 Batch:2490, loss:0.05557643622159958
Message from 0:  	 Batch:2491, loss:0.04753376543521881
Message from 0:  	 Batch:2492, loss:0.031408000737428665
Message from 0:  	 Batch:2493, loss:0.032195985317230225
Message from 0:  	 Batch:2494, loss:0.04965493828058243
Message from 0:  	 Batch:2495, loss:0.04213305562734604
Message from 0:  	 Batch:2496, loss:0.045468367636203766
Message from 0:  	 Batch:2497, loss:0.04943251609802246
Message from 0:  	 Batch:2498, loss:0.047010213136672974
Message from 0:  	 Batch:2499, loss:0.05344945937395096
Message from 0:  	 Batch:2500, loss:0.035008449107408524
Message from 0:  	 Batch:2501, loss:0.04018251597881317
Message from 0:  	 Batch:2502, loss:0.04791641980409622
Message from 0:  	 Batch:2503, loss:0.05552300065755844
Message from 0:  	 Batch:2504, loss:0.04860113561153412
Message from 0:  	 Batch:2505, loss:0.03884930536150932
Message from 0:  	 Batch:2506, loss:0.04468293488025665
Message from 0:  	 Batch:2507, loss:0.05597540736198425
Message from 0:  	 Batch:2508, loss:0.03958090394735336
Message from 0:  	 Batch:2509, loss:0.05278394743800163
Message from 0:  	 Batch:2510, loss:0.03497857227921486
Message from 0:  	 Batch:2511, loss:0.049080219119787216
Message from 0:  	 Batch:2512, loss:0.04870060831308365
Message from 0:  	 Batch:2513, loss:0.05145890265703201
Message from 0:  	 Batch:2514, loss:0.048079196363687515
Message from 0:  	 Batch:2515, loss:0.04652576893568039
Message from 0:  	 Batch:2516, loss:0.06349903345108032
Message from 0:  	 Batch:2517, loss:0.055746935307979584
Message from 0:  	 Batch:2518, loss:0.03479235619306564
Message from 0:  	 Batch:2519, loss:0.0359957292675972
Message from 0:  	 Batch:2520, loss:0.05102396011352539
Message from 0:  	 Batch:2521, loss:0.04529348760843277
Message from 0:  	 Batch:2522, loss:0.05291426181793213
Message from 0:  	 Batch:2523, loss:0.03691381216049194
Message from 0:  	 Batch:2524, loss:0.042965322732925415
Message from 0:  	 Batch:2525, loss:0.043860167264938354
Message from 0:  	 Batch:2526, loss:0.033767785876989365
Message from 0:  	 Batch:2527, loss:0.03257162496447563
Message from 0:  	 Batch:2528, loss:0.07828815281391144
Message from 0:  	 Batch:2529, loss:0.058571957051754
Message from 0:  	 Batch:2530, loss:0.06297802925109863
Message from 0:  	 Batch:2531, loss:0.0528423972427845
Message from 0:  	 Batch:2532, loss:0.04324699938297272
Message from 0:  	 Batch:2533, loss:0.03845398128032684
Message from 0:  	 Batch:2534, loss:0.04642963409423828
Message from 0:  	 Batch:2535, loss:0.07455667853355408
Message from 0:  	 Batch:2536, loss:0.04566944018006325
Message from 0:  	 Batch:2537, loss:0.058366090059280396
Message from 0:  	 Batch:2538, loss:0.04686478525400162
Message from 0:  	 Batch:2539, loss:0.046340424567461014
Message from 0:  	 Batch:2540, loss:0.04897516220808029
Message from 0:  	 Batch:2541, loss:0.05163828283548355
Message from 0:  	 Batch:2542, loss:0.0747278705239296
Message from 0:  	 Batch:2543, loss:0.04283563792705536
Message from 0:  	 Batch:2544, loss:0.04357639327645302
Message from 0:  	 Batch:2545, loss:0.04575304687023163
Message from 0:  	 Batch:2546, loss:0.04772786423563957
Message from 0:  	 Batch:2547, loss:0.045766472816467285
Message from 0:  	 Batch:2548, loss:0.05956483259797096
Message from 0:  	 Batch:2549, loss:0.05101199448108673
Message from 0:  	 Batch:2550, loss:0.04734291136264801
Message from 0:  	 Batch:2551, loss:0.0551128163933754
Message from 0:  	 Batch:2552, loss:0.04586627706885338
Message from 0:  	 Batch:2553, loss:0.06404941529035568
Message from 0:  	 Batch:2554, loss:0.039764344692230225
Message from 0:  	 Batch:2555, loss:0.05200819671154022
Message from 0:  	 Batch:2556, loss:0.04714449122548103
Message from 0:  	 Batch:2557, loss:0.04377968981862068
Message from 0:  	 Batch:2558, loss:0.04219064116477966
Message from 0:  	 Batch:2559, loss:0.06746628135442734
Message from 0:  	 Batch:2560, loss:0.03466629981994629
Message from 0:  	 Batch:2561, loss:0.04457462206482887
Message from 0:  	 Batch:2562, loss:0.049796588718891144
Message from 0:  	 Batch:2563, loss:0.043004363775253296
Message from 0:  	 Batch:2564, loss:0.0565180778503418
Message from 0:  	 Batch:2565, loss:0.0387410968542099
Message from 0:  	 Batch:2566, loss:0.04338926076889038
Message from 0:  	 Batch:2567, loss:0.0752626284956932
Message from 0:  	 Batch:2568, loss:0.06763142347335815
Message from 0:  	 Batch:2569, loss:0.05398028343915939
Message from 0:  	 Batch:2570, loss:0.04751599580049515
Message from 0:  	 Batch:2571, loss:0.032555412501096725
Message from 0:  	 Batch:2572, loss:0.04723570495843887
Message from 0:  	 Batch:2573, loss:0.045373834669589996
Message from 0:  	 Batch:2574, loss:0.04982884228229523
Message from 0:  	 Batch:2575, loss:0.04863502085208893
Message from 0:  	 Batch:2576, loss:0.04507972300052643
Message from 0:  	 Batch:2577, loss:0.048018015921115875
Message from 0:  	 Batch:2578, loss:0.04766847565770149
Message from 0:  	 Batch:2579, loss:0.04344812035560608
Message from 0:  	 Batch:2580, loss:0.05056653916835785
Message from 0:  	 Batch:2581, loss:0.04689944162964821
Message from 0:  	 Batch:2582, loss:0.048333168029785156
Message from 0:  	 Batch:2583, loss:0.07185167074203491
Message from 0:  	 Batch:2584, loss:0.052244603633880615
Message from 0:  	 Batch:2585, loss:0.062522292137146
Message from 0:  	 Batch:2586, loss:0.0569748729467392
Message from 0:  	 Batch:2587, loss:0.03321477398276329
Message from 0:  	 Batch:2588, loss:0.05338137224316597
Message from 0:  	 Batch:2589, loss:0.049060702323913574
Message from 0:  	 Batch:2590, loss:0.04622837156057358
Message from 0:  	 Batch:2591, loss:0.045068033039569855
Message from 0:  	 Batch:2592, loss:0.04213174432516098
Message from 0:  	 Batch:2593, loss:0.04857640713453293
Message from 0:  	 Batch:2594, loss:0.04463206231594086
Message from 0:  	 Batch:2595, loss:0.047425776720047
Message from 0:  	 Batch:2596, loss:0.04074705392122269
Message from 0:  	 Batch:2597, loss:0.029837898910045624
Message from 0:  	 Batch:2598, loss:0.0493464469909668
Message from 0:  	 Batch:2599, loss:0.0406566746532917
Message from 0:  	 Batch:2600, loss:0.0485118068754673
Message from 0:  	 Batch:2601, loss:0.0544796958565712
Message from 0:  	 Batch:2602, loss:0.04689423739910126
Message from 0:  	 Batch:2603, loss:0.0646875649690628
Message from 0:  	 Batch:2604, loss:0.04369279742240906
Message from 0:  	 Batch:2605, loss:0.05271288752555847
Message from 0:  	 Batch:2606, loss:0.0419117733836174
Message from 0:  	 Batch:2607, loss:0.049092669039964676
Message from 0:  	 Batch:2608, loss:0.05998048558831215
Message from 0:  	 Batch:2609, loss:0.04536757618188858
Message from 0:  	 Batch:2610, loss:0.044915735721588135
Message from 0:  	 Batch:2611, loss:0.059735916554927826
Message from 0:  	 Batch:2612, loss:0.04411625862121582
Message from 0:  	 Batch:2613, loss:0.03885958343744278
Message from 0:  	 Batch:2614, loss:0.042872585356235504
Message from 0:  	 Batch:2615, loss:0.045464418828487396
Message from 0:  	 Batch:2616, loss:0.06534115970134735
Message from 0:  	 Batch:2617, loss:0.04440531134605408
Message from 0:  	 Batch:2618, loss:0.04911230131983757
Message from 0:  	 Batch:2619, loss:0.050134021788835526
Message from 0:  	 Batch:2620, loss:0.03776869177818298
Message from 0:  	 Batch:2621, loss:0.09680264443159103
Message from 0:  	 Batch:2622, loss:0.04985472559928894
Message from 0:  	 Batch:2623, loss:0.047367602586746216
Message from 0:  	 Batch:2624, loss:0.041702695190906525
Message from 0:  	 Batch:2625, loss:0.06460530310869217
Message from 0:  	 Batch:2626, loss:0.04912947863340378
Message from 0:  	 Batch:2627, loss:0.050836049020290375
Message from 0:  	 Batch:2628, loss:0.03259681165218353
Message from 0:  	 Batch:2629, loss:0.045376189053058624
Message from 0:  	 Batch:2630, loss:0.0520152784883976
Message from 0:  	 Batch:2631, loss:0.046010635793209076
Message from 0:  	 Batch:2632, loss:0.047076717019081116
Message from 0:  	 Batch:2633, loss:0.06538429856300354
Message from 0:  	 Batch:2634, loss:0.06173429638147354
Message from 0:  	 Batch:2635, loss:0.05761457979679108
Message from 0:  	 Batch:2636, loss:0.04959271103143692
Message from 0:  	 Batch:2637, loss:0.043611351400613785
Message from 0:  	 Batch:2638, loss:0.027785325422883034
Message from 0:  	 Batch:2639, loss:0.044510386884212494
Message from 0:  	 Batch:2640, loss:0.04833024740219116
Message from 0:  	 Batch:2641, loss:0.03302864730358124
Message from 0:  	 Batch:2642, loss:0.04202093556523323
Message from 0:  	 Batch:2643, loss:0.03226844593882561
Message from 0:  	 Batch:2644, loss:0.04118439927697182
Message from 0:  	 Batch:2645, loss:0.0751754641532898
Message from 0:  	 Batch:2646, loss:0.05994764715433121
Message from 0:  	 Batch:2647, loss:0.05863545835018158
Message from 0:  	 Batch:2648, loss:0.05593806132674217
Message from 0:  	 Batch:2649, loss:0.04964543133974075
Message from 0:  	 Batch:2650, loss:0.03995871543884277
Message from 0:  	 Batch:2651, loss:0.05079098045825958
Message from 0:  	 Batch:2652, loss:0.051983803510665894
Message from 0:  	 Batch:2653, loss:0.04340488091111183
Message from 0:  	 Batch:2654, loss:0.042424704879522324
Message from 0:  	 Batch:2655, loss:0.044736556708812714
Message from 0:  	 Batch:2656, loss:0.05547329783439636
Message from 0:  	 Batch:2657, loss:0.038769304752349854
Message from 0:  	 Batch:2658, loss:0.05398394167423248
Message from 0:  	 Batch:2659, loss:0.04640887677669525
Message from 0:  	 Batch:2660, loss:0.05906010419130325
Message from 0:  	 Batch:2661, loss:0.03837309032678604
Message from 0:  	 Batch:2662, loss:0.05108686164021492
Message from 0:  	 Batch:2663, loss:0.05332431569695473
Message from 0:  	 Batch:2664, loss:0.04356309771537781
Message from 0:  	 Batch:2665, loss:0.04486136883497238
Message from 0:  	 Batch:2666, loss:0.04436980560421944
Message from 0:  	 Batch:2667, loss:0.049038030207157135
Message from 0:  	 Batch:2668, loss:0.05593777075409889
Message from 0:  	 Batch:2669, loss:0.06306764483451843
Message from 0:  	 Batch:2670, loss:0.04339853674173355
Message from 0:  	 Batch:2671, loss:0.05114700645208359
Message from 0:  	 Batch:2672, loss:0.04598149657249451
Message from 0:  	 Batch:2673, loss:0.03831329569220543
Message from 0:  	 Batch:2674, loss:0.046339258551597595
Message from 0:  	 Batch:2675, loss:0.04013434424996376
Message from 0:  	 Batch:2676, loss:0.0548882856965065
Message from 0:  	 Batch:2677, loss:0.04852332919836044
Message from 0:  	 Batch:2678, loss:0.03749510645866394
Message from 0:  	 Batch:2679, loss:0.0476861298084259
Message from 0:  	 Batch:2680, loss:0.051257647573947906
Message from 0:  	 Batch:2681, loss:0.047073714435100555
Message from 0:  	 Batch:2682, loss:0.048530835658311844
Message from 0:  	 Batch:2683, loss:0.042858149856328964
Message from 0:  	 Batch:2684, loss:0.04770658165216446
Message from 0:  	 Batch:2685, loss:0.046171896159648895
Message from 0:  	 Batch:2686, loss:0.04694086313247681
Message from 0:  	 Batch:2687, loss:0.05087459087371826
Message from 0:  	 Batch:2688, loss:0.05044006556272507
Message from 0:  	 Batch:2689, loss:0.04372017830610275
Message from 0:  	 Batch:2690, loss:0.043823059648275375
Message from 0:  	 Batch:2691, loss:0.04761173576116562
Message from 0:  	 Batch:2692, loss:0.04838190972805023
Message from 0:  	 Batch:2693, loss:0.06699711084365845
Message from 0:  	 Batch:2694, loss:0.055075958371162415
Message from 0:  	 Batch:2695, loss:0.042087405920028687
Message from 0:  	 Batch:2696, loss:0.04818808659911156
Message from 0:  	 Batch:2697, loss:0.04369395226240158
Message from 0:  	 Batch:2698, loss:0.06598934531211853
Message from 0:  	 Batch:2699, loss:0.0430818572640419
Message from 0:  	 Batch:2700, loss:0.04730435460805893
Message from 0:  	 Batch:2701, loss:0.04103449359536171
Message from 0:  	 Batch:2702, loss:0.04680239409208298
Message from 0:  	 Batch:2703, loss:0.06007465720176697
Message from 0:  	 Batch:2704, loss:0.04277438297867775
Message from 0:  	 Batch:2705, loss:0.041487231850624084
Message from 0:  	 Batch:2706, loss:0.054896675050258636
Message from 0:  	 Batch:2707, loss:0.04438922926783562
Message from 0:  	 Batch:2708, loss:0.06895074248313904
Message from 0:  	 Batch:2709, loss:0.049924686551094055
Message from 0:  	 Batch:2710, loss:0.04164537042379379
Message from 0:  	 Batch:2711, loss:0.05640636757016182
Message from 0:  	 Batch:2712, loss:0.04845280945301056
Message from 0:  	 Batch:2713, loss:0.05714217573404312
Message from 0:  	 Batch:2714, loss:0.04349006712436676
Message from 0:  	 Batch:2715, loss:0.035980451852083206
Message from 0:  	 Batch:2716, loss:0.05045236274600029
Message from 0:  	 Batch:2717, loss:0.042208798229694366
Message from 0:  	 Batch:2718, loss:0.04488901048898697
Message from 0:  	 Batch:2719, loss:0.03367086872458458
Message from 0:  	 Batch:2720, loss:0.06382279098033905
Message from 0:  	 Batch:2721, loss:0.03474275767803192
Message from 0:  	 Batch:2722, loss:0.06831806153059006
Message from 0:  	 Batch:2723, loss:0.034493282437324524
Message from 0:  	 Batch:2724, loss:0.0507512167096138
Message from 0:  	 Batch:2725, loss:0.05119289457798004
Message from 0:  	 Batch:2726, loss:0.043489258736371994
Message from 0:  	 Batch:2727, loss:0.04698621481657028
Message from 0:  	 Batch:2728, loss:0.062295034527778625
Message from 0:  	 Batch:2729, loss:0.045273296535015106
Message from 0:  	 Batch:2730, loss:0.04665111377835274
Message from 0:  	 Batch:2731, loss:0.04925417900085449
Message from 0:  	 Batch:2732, loss:0.04518602415919304
Message from 0:  	 Batch:2733, loss:0.04200711473822594
Message from 0:  	 Batch:2734, loss:0.03837890923023224
Message from 0:  	 Batch:2735, loss:0.0687880665063858
Message from 0:  	 Batch:2736, loss:0.04926076903939247
Message from 0:  	 Batch:2737, loss:0.04268959164619446
Message from 0:  	 Batch:2738, loss:0.04952520877122879
Message from 0:  	 Batch:2739, loss:0.0513281412422657
Message from 0:  	 Batch:2740, loss:0.04065997898578644
Message from 0:  	 Batch:2741, loss:0.049026668071746826
Message from 0:  	 Batch:2742, loss:0.04407868534326553
Message from 0:  	 Batch:2743, loss:0.034436292946338654
Message from 0:  	 Batch:2744, loss:0.037174541503190994
Message from 0:  	 Batch:2745, loss:0.0340743213891983
Message from 0:  	 Batch:2746, loss:0.06876231729984283
Message from 0:  	 Batch:2747, loss:0.04189082980155945
Message from 0:  	 Batch:2748, loss:0.04794321209192276
Message from 0:  	 Batch:2749, loss:0.06007774919271469
Message from 0:  	 Batch:2750, loss:0.05877487361431122
Message from 0:  	 Batch:2751, loss:0.053797606378793716
Message from 0:  	 Batch:2752, loss:0.04552092030644417
Message from 0:  	 Batch:2753, loss:0.03379916772246361
Message from 0:  	 Batch:2754, loss:0.046614453196525574
Message from 0:  	 Batch:2755, loss:0.04552536457777023
Message from 0:  	 Batch:2756, loss:0.05724325403571129
Message from 0:  	 Batch:2757, loss:0.046857528388500214
Message from 0:  	 Batch:2758, loss:0.05206175893545151
Message from 0:  	 Batch:2759, loss:0.049317579716444016
Message from 0:  	 Batch:2760, loss:0.04586860537528992
Message from 0:  	 Batch:2761, loss:0.033479511737823486
Message from 0:  	 Batch:2762, loss:0.04310466721653938
Message from 0:  	 Batch:2763, loss:0.052621178328990936
Message from 0:  	 Batch:2764, loss:0.056304045021533966
Message from 0:  	 Batch:2765, loss:0.0461655929684639
Message from 0:  	 Batch:2766, loss:0.07610298693180084
Message from 0:  	 Batch:2767, loss:0.04943731054663658
Message from 0:  	 Batch:2768, loss:0.04789118841290474
Message from 0:  	 Batch:2769, loss:0.05377382040023804
Message from 0:  	 Batch:2770, loss:0.04491199925541878
Message from 0:  	 Batch:2771, loss:0.07675978541374207
Message from 0:  	 Batch:2772, loss:0.07414640486240387
Message from 0:  	 Batch:2773, loss:0.054804518818855286
Message from 0:  	 Batch:2774, loss:0.038225315511226654
Message from 0:  	 Batch:2775, loss:0.03585111349821091
Message from 0:  	 Batch:2776, loss:0.046735551208257675
Message from 0:  	 Batch:2777, loss:0.04517150670289993
Message from 0:  	 Batch:2778, loss:0.053495533764362335
Message from 0:  	 Batch:2779, loss:0.04398040473461151
Message from 0:  	 Batch:2780, loss:0.062413591891527176
Message from 0:  	 Batch:2781, loss:0.05236196517944336
Message from 0:  	 Batch:2782, loss:0.047527629882097244
Message from 0:  	 Batch:2783, loss:0.04936360940337181
Message from 0:  	 Batch:2784, loss:0.04016086459159851
Message from 0:  	 Batch:2785, loss:0.05953933671116829
Message from 0:  	 Batch:2786, loss:0.0359344482421875
Message from 0:  	 Batch:2787, loss:0.04297378286719322
Message from 0:  	 Batch:2788, loss:0.04854702576994896
Message from 0:  	 Batch:2789, loss:0.051337726414203644
Message from 0:  	 Batch:2790, loss:0.048312701284885406
Message from 0:  	 Batch:2791, loss:0.043472159653902054
Message from 0:  	 Batch:2792, loss:0.055807460099458694
Message from 0:  	 Batch:2793, loss:0.0459110252559185
Message from 0:  	 Batch:2794, loss:0.06318186223506927
Message from 0:  	 Batch:2795, loss:0.06185346469283104
Message from 0:  	 Batch:2796, loss:0.04568900167942047
Message from 0:  	 Batch:2797, loss:0.05466502904891968
Message from 0:  	 Batch:2798, loss:0.052056923508644104
Message from 0:  	 Batch:2799, loss:0.04942110925912857
Message from 0:  	 Batch:2800, loss:0.04267062619328499
Message from 0:  	 Batch:2801, loss:0.04874609410762787
Message from 0:  	 Batch:2802, loss:0.041799239814281464
Message from 0:  	 Batch:2803, loss:0.10539910942316055
Message from 0:  	 Batch:2804, loss:0.06890814751386642
Message from 0:  	 Batch:2805, loss:0.04099811613559723
Message from 0:  	 Batch:2806, loss:0.03470143675804138
Message from 0:  	 Batch:2807, loss:0.05756095051765442
Message from 0:  	 Batch:2808, loss:0.046618491411209106
Message from 0:  	 Batch:2809, loss:0.051071763038635254
Message from 0:  	 Batch:2810, loss:0.04652918130159378
Message from 0:  	 Batch:2811, loss:0.058365363627672195
Message from 0:  	 Batch:2812, loss:0.0345681793987751
Message from 0:  	 Batch:2813, loss:0.046339958906173706
Message from 0:  	 Batch:2814, loss:0.03915984183549881
Message from 0:  	 Batch:2815, loss:0.0406133234500885
Message from 0:  	 Batch:2816, loss:0.04513799026608467
Message from 0:  	 Batch:2817, loss:0.04363539069890976
Message from 0:  	 Batch:2818, loss:0.043502043932676315
Message from 0:  	 Batch:2819, loss:0.061375752091407776
Message from 0:  	 Batch:2820, loss:0.03952181339263916
Message from 0:  	 Batch:2821, loss:0.06644848734140396
Message from 0:  	 Batch:2822, loss:0.10510490089654922
Message from 0:  	 Batch:2823, loss:0.04466826841235161
Message from 0:  	 Batch:2824, loss:0.0472402349114418
Message from 0:  	 Batch:2825, loss:0.04519760608673096
Message from 0:  	 Batch:2826, loss:0.048069067299366
Message from 0:  	 Batch:2827, loss:0.042290765792131424
Message from 0:  	 Batch:2828, loss:0.05397411435842514
Message from 0:  	 Batch:2829, loss:0.048449546098709106
Message from 0:  	 Batch:2830, loss:0.03317585214972496
Message from 0:  	 Batch:2831, loss:0.07360132038593292
Message from 0:  	 Batch:2832, loss:0.07160541415214539
Message from 0:  	 Batch:2833, loss:0.04344335198402405
Message from 0:  	 Batch:2834, loss:0.045760028064250946
Message from 0:  	 Batch:2835, loss:0.07230377197265625
Message from 0:  	 Batch:2836, loss:0.04678058624267578
Message from 0:  	 Batch:2837, loss:0.05114895850419998
Message from 0:  	 Batch:2838, loss:0.04125577211380005
Message from 0:  	 Batch:2839, loss:0.047806307673454285
Message from 0:  	 Batch:2840, loss:0.046272166073322296
Message from 0:  	 Batch:2841, loss:0.05140156298875809
Message from 0:  	 Batch:2842, loss:0.044282667338848114
Message from 0:  	 Batch:2843, loss:0.043395034968853
Message from 0:  	 Batch:2844, loss:0.04546374827623367
Message from 0:  	 Batch:2845, loss:0.039527714252471924
Message from 0:  	 Batch:2846, loss:0.05792468041181564
Message from 0:  	 Batch:2847, loss:0.04291266202926636
Message from 0:  	 Batch:2848, loss:0.05588534474372864
Message from 0:  	 Batch:2849, loss:0.043021053075790405
Message from 0:  	 Batch:2850, loss:0.04847594350576401
Message from 0:  	 Batch:2851, loss:0.053113240748643875
Message from 0:  	 Batch:2852, loss:0.07294046878814697
Message from 0:  	 Batch:2853, loss:0.05165994167327881
Message from 0:  	 Batch:2854, loss:0.04827788472175598
Message from 0:  	 Batch:2855, loss:0.05014852434396744
Message from 0:  	 Batch:2856, loss:0.04913816601037979
Message from 0:  	 Batch:2857, loss:0.033663060516119
Message from 0:  	 Batch:2858, loss:0.034897468984127045
Message from 0:  	 Batch:2859, loss:0.02252703718841076
Message from 0:  	 Batch:2860, loss:0.043800707906484604
Message from 0:  	 Batch:2861, loss:0.04665534570813179
Message from 0:  	 Batch:2862, loss:0.04095346853137016
Message from 0:  	 Batch:2863, loss:0.049241289496421814
Message from 0:  	 Batch:2864, loss:0.046844612807035446
Message from 0:  	 Batch:2865, loss:0.03723279386758804
Message from 0:  	 Batch:2866, loss:0.040451981127262115
Message from 0:  	 Batch:2867, loss:0.04079602658748627
Message from 0:  	 Batch:2868, loss:0.04302629083395004
Message from 0:  	 Batch:2869, loss:0.045940957963466644
Message from 0:  	 Batch:2870, loss:0.04899878799915314
Message from 0:  	 Batch:2871, loss:0.04332255572080612
Message from 0:  	 Batch:2872, loss:0.048000890761613846
Message from 0:  	 Batch:2873, loss:0.05873754620552063
Message from 0:  	 Batch:2874, loss:0.04196954518556595
Message from 0:  	 Batch:2875, loss:0.0327310711145401
Message from 0:  	 Batch:2876, loss:0.04309410974383354
Message from 0:  	 Batch:2877, loss:0.04024825245141983
Message from 0:  	 Batch:2878, loss:0.04480551928281784
Message from 0:  	 Batch:2879, loss:0.050783850252628326
Message from 0:  	 Batch:2880, loss:0.04745200276374817
Message from 0:  	 Batch:2881, loss:0.048652008175849915
Message from 0:  	 Batch:2882, loss:0.042618393898010254
Message from 0:  	 Batch:2883, loss:0.044683873653411865
Message from 0:  	 Batch:2884, loss:0.052074696868658066
Message from 0:  	 Batch:2885, loss:0.04396085441112518
Message from 0:  	 Batch:2886, loss:0.05058401823043823
Message from 0:  	 Batch:2887, loss:0.048387326300144196
Message from 0:  	 Batch:2888, loss:0.04346415773034096
Message from 0:  	 Batch:2889, loss:0.03378472104668617
Message from 0:  	 Batch:2890, loss:0.04732349514961243
Message from 0:  	 Batch:2891, loss:0.04136982560157776
Message from 0:  	 Batch:2892, loss:0.05199485644698143
Message from 0:  	 Batch:2893, loss:0.04693957418203354
Message from 0:  	 Batch:2894, loss:0.04261518642306328
Message from 0:  	 Batch:2895, loss:0.057535432279109955
Message from 0:  	 Batch:2896, loss:0.04682573676109314
Message from 0:  	 Batch:2897, loss:0.04767405241727829
Message from 0:  	 Batch:2898, loss:0.0443992018699646
Message from 0:  	 Batch:2899, loss:0.04195467382669449
Message from 0:  	 Batch:2900, loss:0.051722072064876556
Message from 0:  	 Batch:2901, loss:0.07357253134250641
Message from 0:  	 Batch:2902, loss:0.04479054734110832
Message from 0:  	 Batch:2903, loss:0.046618130058050156
Message from 0:  	 Batch:2904, loss:0.045478615909814835
Message from 0:  	 Batch:2905, loss:0.05158576741814613
Message from 0:  	 Batch:2906, loss:0.03665723279118538
Message from 0:  	 Batch:2907, loss:0.03855826333165169
Message from 0:  	 Batch:2908, loss:0.050640568137168884
Message from 0:  	 Batch:2909, loss:0.04854517802596092
Message from 0:  	 Batch:2910, loss:0.04353874921798706
Message from 0:  	 Batch:2911, loss:0.04124647378921509
Message from 0:  	 Batch:2912, loss:0.04417289420962334
Message from 0:  	 Batch:2913, loss:0.049216464161872864
Message from 0:  	 Batch:2914, loss:0.04633479192852974
Message from 0:  	 Batch:2915, loss:0.04046303778886795
Message from 0:  	 Batch:2916, loss:0.04164271801710129
Message from 0:  	 Batch:2917, loss:0.04823421686887741
Message from 0:  	 Batch:2918, loss:0.04534812271595001
Message from 0:  	 Batch:2919, loss:0.052175022661685944
Message from 0:  	 Batch:2920, loss:0.03469143807888031
Message from 0:  	 Batch:2921, loss:0.05947884917259216
Message from 0:  	 Batch:2922, loss:0.05075303465127945
Message from 0:  	 Batch:2923, loss:0.051313359290361404
Message from 0:  	 Batch:2924, loss:0.07086136937141418
Message from 0:  	 Batch:2925, loss:0.03585086762905121
Message from 0:  	 Batch:2926, loss:0.0401143878698349
Message from 0:  	 Batch:2927, loss:0.04398810863494873
Message from 0:  	 Batch:2928, loss:0.052462972700595856
Message from 0:  	 Batch:2929, loss:0.05387876555323601
Message from 0:  	 Batch:2930, loss:0.055622898042201996
Message from 0:  	 Batch:2931, loss:0.046279340982437134
Message from 0:  	 Batch:2932, loss:0.03710200637578964
Message from 0:  	 Batch:2933, loss:0.04372045397758484
Message from 0:  	 Batch:2934, loss:0.04947603493928909
Message from 0:  	 Batch:2935, loss:0.04824355989694595
Message from 0:  	 Batch:2936, loss:0.05076802894473076
Message from 0:  	 Batch:2937, loss:0.047304973006248474
Message from 0:  	 Batch:2938, loss:0.053827833384275436
Message from 0:  	 Batch:2939, loss:0.060481250286102295
Message from 0:  	 Batch:2940, loss:0.04692605137825012
Message from 0:  	 Batch:2941, loss:0.04582152143120766
Message from 0:  	 Batch:2942, loss:0.049027130007743835
Message from 0:  	 Batch:2943, loss:0.06265845149755478
Message from 0:  	 Batch:2944, loss:0.04224254935979843
Message from 0:  	 Batch:2945, loss:0.04569602012634277
Message from 0:  	 Batch:2946, loss:0.053977370262145996
Message from 0:  	 Batch:2947, loss:0.03476598113775253
Message from 0:  	 Batch:2948, loss:0.06185382604598999
Message from 0:  	 Batch:2949, loss:0.04603353887796402
Message from 0:  	 Batch:2950, loss:0.03462587669491768
Message from 0:  	 Batch:2951, loss:0.07098331302404404
Message from 0:  	 Batch:2952, loss:0.046822741627693176
Message from 0:  	 Batch:2953, loss:0.047945693135261536
Message from 0:  	 Batch:2954, loss:0.05591695010662079
Message from 0:  	 Batch:2955, loss:0.05281722545623779
Message from 0:  	 Batch:2956, loss:0.04729202389717102
Message from 0:  	 Batch:2957, loss:0.03652678802609444
Message from 0:  	 Batch:2958, loss:0.044004350900650024
Message from 0:  	 Batch:2959, loss:0.05800676345825195
Message from 0:  	 Batch:2960, loss:0.03413987532258034
Message from 0:  	 Batch:2961, loss:0.038893185555934906
Message from 0:  	 Batch:2962, loss:0.04109656810760498
Message from 0:  	 Batch:2963, loss:0.042117342352867126
Message from 0:  	 Batch:2964, loss:0.05607683211565018
Message from 0:  	 Batch:2965, loss:0.04276832938194275
Message from 0:  	 Batch:2966, loss:0.06709685921669006
Message from 0:  	 Batch:2967, loss:0.03471130505204201
Message from 0:  	 Batch:2968, loss:0.04685134068131447
Message from 0:  	 Batch:2969, loss:0.07449326664209366
Message from 0:  	 Batch:2970, loss:0.047991469502449036
Message from 0:  	 Batch:2971, loss:0.046772073954343796
Message from 0:  	 Batch:2972, loss:0.05302135646343231
Message from 0:  	 Batch:2973, loss:0.041078921407461166
Message from 0:  	 Batch:2974, loss:0.05878559499979019
Message from 0:  	 Batch:2975, loss:0.04420775920152664
Message from 0:  	 Batch:2976, loss:0.06202137470245361
Message from 0:  	 Batch:2977, loss:0.03502403199672699
Message from 0:  	 Batch:2978, loss:0.0746133029460907
Message from 0:  	 Batch:2979, loss:0.04782930016517639
Message from 0:  	 Batch:2980, loss:0.05895526334643364
Message from 0:  	 Batch:2981, loss:0.033618368208408356
Message from 0:  	 Batch:2982, loss:0.03464925289154053
Message from 0:  	 Batch:2983, loss:0.04430219903588295
Message from 0:  	 Batch:2984, loss:0.047917336225509644
Message from 0:  	 Batch:2985, loss:0.049063656479120255
Message from 0:  	 Batch:2986, loss:0.050022054463624954
Message from 0:  	 Batch:2987, loss:0.04776725172996521
Message from 0:  	 Batch:2988, loss:0.03273320943117142
Message from 0:  	 Batch:2989, loss:0.05301439389586449
Message from 0:  	 Batch:2990, loss:0.045636922121047974
Message from 0:  	 Batch:2991, loss:0.0510285384953022
Message from 0:  	 Batch:2992, loss:0.056652434170246124
Message from 0:  	 Batch:2993, loss:0.05107344314455986
Message from 0:  	 Batch:2994, loss:0.0333869494497776
Message from 0:  	 Batch:2995, loss:0.04829450696706772
Message from 0:  	 Batch:2996, loss:0.07667914032936096
Message from 0:  	 Batch:2997, loss:0.03516487404704094
Message from 0:  	 Batch:2998, loss:0.04859594255685806
Message from 1:  	 Epoch:0, process-local average loss:0.0487000863670061
Message from 0:  	 Batch:2999, loss:0.04841872304677963
Message from 0:  	 Epoch:0, process-local average loss:0.04867394147006174
[33m[1m[W 09/10/21 20:05:05.400 22358] [type_check.cpp:visit@100] [$94630] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:05.400 22358] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 93, in collide
    dist = self.sdf(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 59, in sdf
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 20:05:05.400 22358] [type_check.cpp:visit@100] [$94632] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:05.400 22358] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 93, in collide
    dist = self.sdf(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 59, in sdf
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 20:05:05.400 22358] [type_check.cpp:visit@100] [$94634] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:05.400 22358] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 93, in collide
    dist = self.sdf(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 59, in sdf
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 20:05:05.400 22358] [type_check.cpp:visit@100] [$95046] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:05.400 22358] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 79, in normal
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 20:05:05.400 22358] [type_check.cpp:visit@100] [$95048] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:05.400 22358] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 79, in normal
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 20:05:05.400 22358] [type_check.cpp:visit@100] [$95050] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:05.400 22358] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 79, in normal
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 20:05:05.400 22358] [type_check.cpp:visit@100] [$95110] Local store may lose precision (target = i32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:05.400 22358] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 80, in normal
    return qrot(self.rotation[f], self._normal(f, grid_pos))
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primitives.py", line 178, in _normal
    f = ti.cast(d[0] > d[1], self.dtype)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 135, in wrapped
    return imp_foo(a, b)
[m
[33m[1m[W 09/10/21 20:05:05.570 22359] [type_check.cpp:visit@100] [$94630] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:05.570 22359] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 93, in collide
    dist = self.sdf(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 59, in sdf
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 20:05:05.570 22359] [type_check.cpp:visit@100] [$94632] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:05.570 22359] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 93, in collide
    dist = self.sdf(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 59, in sdf
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 20:05:05.570 22359] [type_check.cpp:visit@100] [$94634] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:05.570 22359] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 93, in collide
    dist = self.sdf(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 59, in sdf
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 20:05:05.570 22359] [type_check.cpp:visit@100] [$95046] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:05.570 22359] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 79, in normal
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 20:05:05.570 22359] [type_check.cpp:visit@100] [$95048] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:05.570 22359] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 79, in normal
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 20:05:05.570 22359] [type_check.cpp:visit@100] [$95050] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:05.570 22359] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 79, in normal
    grid_pos = inv_trans(grid_pos, self.position[f], self.rotation[f])
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 130, in wrapped
    return a.element_wise_writeback_binary(imp_foo, b)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/matrix.py", line 179, in element_wise_writeback_binary
    ret.entries[i] = foo(self.entries[i], other.entries[i])
[m
[33m[1m[W 09/10/21 20:05:05.570 22359] [type_check.cpp:visit@100] [$95110] Local store may lose precision (target = i32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:05.570 22359] [type_check.cpp:visit@101] 
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/mpm_simulator.py", line 198, in grid_op
    v_out = self.primitives[i].collide(f, I * self.dx, v_out, self.dt)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 96, in collide
    D = self.normal(f, grid_pos)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primive_base.py", line 80, in normal
    return qrot(self.rotation[f], self._normal(f, grid_pos))
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 50, in decorated
    return fun.__call__(*args)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/kernel_impl.py", line 122, in __call__
    ret = self.compiled(*args)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/engine/primitive/primitives.py", line 178, in _normal
    f = ti.cast(d[0] > d[1], self.dtype)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/util.py", line 163, in wrapped
    return func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/impl.py", line 202, in maybe_transform_ti_func_call_to_stmt
    return ti_func(*args, **kwargs)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/common_ops.py", line 227, in assign
    return ti.assign(self, other)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/taichi/lang/ops.py", line 135, in wrapped
    return imp_foo(a, b)
[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@100] [$97418] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@100] [$97446] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@100] [$97482] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@100] [$99016] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@100] [$99063] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@100] [$99184] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@100] [$99230] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@100] [$99974] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.047 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.048 22358] [type_check.cpp:visit@100] [$104459] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.048 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.048 22358] [type_check.cpp:visit@100] [$104477] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.048 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.048 22358] [type_check.cpp:visit@100] [$104495] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.048 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.232 22359] [type_check.cpp:visit@100] [$97418] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@100] [$97446] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@100] [$97482] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@100] [$99016] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@100] [$99063] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@100] [$99184] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@100] [$99230] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@100] [$99974] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@100] [$104459] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@100] [$104477] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@100] [$104495] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:06.233 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:29.862 22358] [type_check.cpp:visit@100] [$153054] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:29.862 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:29.889 22359] [type_check.cpp:visit@100] [$153054] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:29.889 22359] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:30.038 22358] [type_check.cpp:visit@100] [$153398] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:30.038 22358] [type_check.cpp:visit@101] 
[m
[33m[1m[W 09/10/21 20:05:30.065 22359] [type_check.cpp:visit@100] [$153398] Local store may lose precision (target = f32, value = f64) at[m
[33m[1m[W 09/10/21 20:05:30.065 22359] [type_check.cpp:visit@101] 
[m
Message from 0:  	 Batch:0, loss:0.04280399531126022
Message from 0:  	 Batch:1, loss:0.04822637885808945
Message from 0:  	 Batch:2, loss:0.05495627969503403
Message from 0:  	 Batch:3, loss:0.04755677655339241
Message from 0:  	 Batch:4, loss:0.042971864342689514
Message from 0:  	 Batch:5, loss:0.04207908734679222
Message from 0:  	 Batch:6, loss:0.03761615976691246
Message from 0:  	 Batch:7, loss:0.0375184491276741
Message from 0:  	 Batch:8, loss:0.07038973271846771
Message from 0:  	 Batch:9, loss:0.046342119574546814
Message from 0:  	 Batch:10, loss:0.04059866443276405
Message from 0:  	 Batch:11, loss:0.039647169411182404
Message from 0:  	 Batch:12, loss:0.04821784794330597
Message from 0:  	 Batch:13, loss:0.04678613692522049
Message from 0:  	 Batch:14, loss:0.049374960362911224
Message from 0:  	 Batch:15, loss:0.04544753581285477
Message from 0:  	 Batch:16, loss:0.03669920936226845
Message from 0:  	 Batch:17, loss:0.04064033925533295
Message from 0:  	 Batch:18, loss:0.06865815818309784
Message from 0:  	 Batch:19, loss:0.04147389903664589
Message from 0:  	 Batch:20, loss:0.04004869610071182
Message from 0:  	 Batch:21, loss:0.04524756968021393
Message from 0:  	 Batch:22, loss:0.03495259955525398
Message from 0:  	 Batch:23, loss:0.03677314892411232
Message from 0:  	 Batch:24, loss:0.05317562818527222
Message from 0:  	 Batch:25, loss:0.06273657083511353
Message from 0:  	 Batch:26, loss:0.04268572852015495
Message from 0:  	 Batch:27, loss:0.0434834286570549
Message from 0:  	 Batch:28, loss:0.05514165014028549
Message from 0:  	 Batch:29, loss:0.0493728369474411
Message from 0:  	 Batch:30, loss:0.058598969131708145
Message from 0:  	 Batch:31, loss:0.03760524094104767
Message from 0:  	 Batch:32, loss:0.04269429296255112
Message from 0:  	 Batch:33, loss:0.03915157914161682
Message from 0:  	 Batch:34, loss:0.03536372259259224
Message from 0:  	 Batch:35, loss:0.0672643855214119
Message from 0:  	 Batch:36, loss:0.05624125152826309
Message from 0:  	 Batch:37, loss:0.0326518639922142
Message from 0:  	 Batch:38, loss:0.0449126660823822
Message from 0:  	 Batch:39, loss:0.058327727019786835
Message from 0:  	 Batch:40, loss:0.043695710599422455
Message from 0:  	 Batch:41, loss:0.06820827722549438
Message from 0:  	 Batch:42, loss:0.04275406897068024
Message from 0:  	 Batch:43, loss:0.04178400710225105
Message from 0:  	 Batch:44, loss:0.04161425679922104
Message from 0:  	 Batch:45, loss:0.06438859552145004
Message from 0:  	 Batch:46, loss:0.03592295199632645
Message from 0:  	 Batch:47, loss:0.053214699029922485
Message from 0:  	 Batch:48, loss:0.04435741901397705
Message from 0:  	 Batch:49, loss:0.04907272011041641
Message from 0:  	 Batch:50, loss:0.039210859686136246
Message from 0:  	 Batch:51, loss:0.04052824527025223
Message from 0:  	 Batch:52, loss:0.07937416434288025
Message from 0:  	 Batch:53, loss:0.06923502683639526
Message from 0:  	 Batch:54, loss:0.04067924618721008
Message from 0:  	 Batch:55, loss:0.05755649507045746
Message from 0:  	 Batch:56, loss:0.03795768320560455
Message from 0:  	 Batch:57, loss:0.03919464349746704
Message from 0:  	 Batch:58, loss:0.041535280644893646
Message from 0:  	 Batch:59, loss:0.06066293269395828
Message from 0:  	 Batch:60, loss:0.03429160267114639
Message from 0:  	 Batch:61, loss:0.050002798438072205
Message from 0:  	 Batch:62, loss:0.04665378853678703
Message from 0:  	 Batch:63, loss:0.04095657914876938
Message from 0:  	 Batch:64, loss:0.036976613104343414
Message from 0:  	 Batch:65, loss:0.05119871348142624
Message from 0:  	 Batch:66, loss:0.039878278970718384
Message from 0:  	 Batch:67, loss:0.03334689140319824
Message from 0:  	 Batch:68, loss:0.09586083143949509
Message from 0:  	 Batch:69, loss:0.03521881252527237
Message from 0:  	 Batch:70, loss:0.05042356997728348
Message from 0:  	 Batch:71, loss:0.04296974837779999
Message from 0:  	 Batch:72, loss:0.06648462265729904
Message from 0:  	 Batch:73, loss:0.03934841975569725
Message from 0:  	 Batch:74, loss:0.04890376329421997
Message from 0:  	 Batch:75, loss:0.04482552781701088
Message from 0:  	 Batch:76, loss:0.04062908887863159
Message from 0:  	 Batch:77, loss:0.040185391902923584
Message from 0:  	 Batch:78, loss:0.043593116104602814
Message from 0:  	 Batch:79, loss:0.03367248922586441
Message from 0:  	 Batch:80, loss:0.042357705533504486
Message from 0:  	 Batch:81, loss:0.10490724444389343
Message from 0:  	 Batch:82, loss:0.0826222151517868
Message from 0:  	 Batch:83, loss:0.05030020326375961
Message from 0:  	 Batch:84, loss:0.03768234699964523
Message from 0:  	 Batch:85, loss:0.043002739548683167
Message from 0:  	 Batch:86, loss:0.03984534367918968
Message from 0:  	 Batch:87, loss:0.06519459187984467
Message from 0:  	 Batch:88, loss:0.06390061974525452
Message from 0:  	 Batch:89, loss:0.04055318981409073
Message from 0:  	 Batch:90, loss:0.03907357528805733
Message from 0:  	 Batch:91, loss:0.03980877250432968
Message from 0:  	 Batch:92, loss:0.04244808852672577
Message from 0:  	 Batch:93, loss:0.03777216002345085
Message from 0:  	 Batch:94, loss:0.05576682463288307
Message from 0:  	 Batch:95, loss:0.04047460854053497
Message from 0:  	 Batch:96, loss:0.04038463532924652
Message from 0:  	 Batch:97, loss:0.08276566118001938
Message from 0:  	 Batch:98, loss:0.0476863756775856
Message from 0:  	 Batch:99, loss:0.038002535700798035
Message from 0:  	 Batch:100, loss:0.05515582114458084
Message from 0:  	 Batch:101, loss:0.039898231625556946
Message from 0:  	 Batch:102, loss:0.048343475908041
Message from 0:  	 Batch:103, loss:0.036215685307979584
Message from 0:  	 Batch:104, loss:0.03528091683983803
Message from 0:  	 Batch:105, loss:0.043853066861629486
Message from 0:  	 Batch:106, loss:0.04667367786169052
Message from 0:  	 Batch:107, loss:0.04011765867471695
Message from 0:  	 Batch:108, loss:0.04355045408010483
Message from 0:  	 Batch:109, loss:0.05739594250917435
Message from 0:  	 Batch:110, loss:0.034812018275260925
Message from 0:  	 Batch:111, loss:0.045788902789354324
Message from 0:  	 Batch:112, loss:0.058597367256879807
Message from 0:  	 Batch:113, loss:0.03782438486814499
Message from 0:  	 Batch:114, loss:0.04131130129098892
Message from 0:  	 Batch:115, loss:0.041807450354099274
Message from 0:  	 Batch:116, loss:0.043087512254714966
Message from 0:  	 Batch:117, loss:0.03947079926729202
Message from 0:  	 Batch:118, loss:0.04440928250551224
Message from 0:  	 Batch:119, loss:0.043169982731342316
Message from 0:  	 Batch:120, loss:0.04633039981126785
Message from 0:  	 Batch:121, loss:0.03580007329583168
Message from 0:  	 Batch:122, loss:0.05896943062543869
Message from 0:  	 Batch:123, loss:0.04031439125537872
Message from 0:  	 Batch:124, loss:0.05896061658859253
Message from 0:  	 Batch:125, loss:0.03970026224851608
Message from 0:  	 Batch:126, loss:0.04065515846014023
Message from 0:  	 Batch:127, loss:0.04779736325144768
Message from 0:  	 Batch:128, loss:0.06302320957183838
Message from 0:  	 Batch:129, loss:0.05341078341007233
Message from 0:  	 Batch:130, loss:0.03621082380414009
Message from 0:  	 Batch:131, loss:0.03850136697292328
Message from 0:  	 Batch:132, loss:0.04007945954799652
Message from 0:  	 Batch:133, loss:0.03769071400165558
Message from 0:  	 Batch:134, loss:0.07272211462259293
Message from 0:  	 Batch:135, loss:0.03517145663499832
Message from 0:  	 Batch:136, loss:0.05884945020079613
Message from 0:  	 Batch:137, loss:0.035350773483514786
Message from 0:  	 Batch:138, loss:0.04274336248636246
Message from 0:  	 Batch:139, loss:0.03722302243113518
Message from 0:  	 Batch:140, loss:0.08970186859369278
Message from 0:  	 Batch:141, loss:0.0550030916929245
Message from 0:  	 Batch:142, loss:0.04174546152353287
Message from 0:  	 Batch:143, loss:0.04351729154586792
Message from 0:  	 Batch:144, loss:0.042482201009988785
Message from 0:  	 Batch:145, loss:0.04206065833568573
Message from 0:  	 Batch:146, loss:0.03763722628355026
Message from 0:  	 Batch:147, loss:0.04126384109258652
Message from 0:  	 Batch:148, loss:0.03788542374968529
Message from 0:  	 Batch:149, loss:0.03767489641904831
Message from 0:  	 Batch:150, loss:0.039043426513671875
Message from 0:  	 Batch:151, loss:0.034337472170591354
Message from 0:  	 Batch:152, loss:0.04083334654569626
Message from 0:  	 Batch:153, loss:0.03846612572669983
Message from 0:  	 Batch:154, loss:0.03681877255439758
Message from 0:  	 Batch:155, loss:0.05606193095445633
Message from 0:  	 Batch:156, loss:0.051461994647979736
Message from 0:  	 Batch:157, loss:0.04253970831632614
Message from 0:  	 Batch:158, loss:0.04571885988116264
Message from 0:  	 Batch:159, loss:0.040209606289863586
Message from 0:  	 Batch:160, loss:0.03919658064842224
Message from 0:  	 Batch:161, loss:0.051130931824445724
Message from 0:  	 Batch:162, loss:0.03398662060499191
Message from 0:  	 Batch:163, loss:0.04157610237598419
Message from 0:  	 Batch:164, loss:0.041566602885723114
Message from 0:  	 Batch:165, loss:0.04953446239233017
Message from 0:  	 Batch:166, loss:0.047523900866508484
Message from 0:  	 Batch:167, loss:0.039206698536872864
Message from 0:  	 Batch:168, loss:0.07908520102500916
Message from 0:  	 Batch:169, loss:0.03852163255214691
Message from 0:  	 Batch:170, loss:0.038567811250686646
Message from 0:  	 Batch:171, loss:0.033607352524995804
Message from 0:  	 Batch:172, loss:0.05989709496498108
Message from 0:  	 Batch:173, loss:0.03162306547164917
Message from 0:  	 Batch:174, loss:0.05406258627772331
Message from 0:  	 Batch:175, loss:0.039508603513240814
Message from 0:  	 Batch:176, loss:0.03576523810625076
Message from 0:  	 Batch:177, loss:0.0372137725353241
Message from 0:  	 Batch:178, loss:0.035645850002765656
Message from 0:  	 Batch:179, loss:0.0483076237142086
Message from 0:  	 Batch:180, loss:0.03793063387274742
Message from 0:  	 Batch:181, loss:0.03743329644203186
Message from 0:  	 Batch:182, loss:0.03500966727733612
Message from 0:  	 Batch:183, loss:0.04332468658685684
Message from 0:  	 Batch:184, loss:0.04128822684288025
Message from 0:  	 Batch:185, loss:0.03539393097162247
Message from 0:  	 Batch:186, loss:0.05167841166257858
Message from 0:  	 Batch:187, loss:0.03737474977970123
Message from 0:  	 Batch:188, loss:0.04049210995435715
Message from 0:  	 Batch:189, loss:0.036256276071071625
Message from 0:  	 Batch:190, loss:0.060101427137851715
Message from 0:  	 Batch:191, loss:0.055875033140182495
Message from 0:  	 Batch:192, loss:0.050449129194021225
Message from 0:  	 Batch:193, loss:0.06137501448392868
Message from 0:  	 Batch:194, loss:0.035595543682575226
Message from 0:  	 Batch:195, loss:0.04934404045343399
Message from 0:  	 Batch:196, loss:0.030922144651412964
Message from 0:  	 Batch:197, loss:0.04532022774219513
Message from 0:  	 Batch:198, loss:0.03433123230934143
Message from 0:  	 Batch:199, loss:0.04528533294796944
Message from 0:  	 Batch:200, loss:0.055319644510746
Message from 0:  	 Batch:201, loss:0.061991140246391296
Message from 0:  	 Batch:202, loss:0.039109665900468826
Message from 0:  	 Batch:203, loss:0.04142849147319794
Message from 0:  	 Batch:204, loss:0.03207976743578911
Message from 0:  	 Batch:205, loss:0.03977937996387482
Message from 0:  	 Batch:206, loss:0.03819552809000015
Message from 0:  	 Batch:207, loss:0.040325962007045746
Message from 0:  	 Batch:208, loss:0.04146527498960495
Message from 0:  	 Batch:209, loss:0.05260882526636124
Message from 0:  	 Batch:210, loss:0.07440990954637527
Message from 0:  	 Batch:211, loss:0.03713368624448776
Message from 0:  	 Batch:212, loss:0.06803840398788452
Message from 0:  	 Batch:213, loss:0.07297354936599731
Message from 0:  	 Batch:214, loss:0.04265560209751129
Message from 0:  	 Batch:215, loss:0.03332013636827469
Message from 0:  	 Batch:216, loss:0.040882084518671036
Message from 0:  	 Batch:217, loss:0.060339752584695816
Message from 0:  	 Batch:218, loss:0.039820656180381775
Message from 0:  	 Batch:219, loss:0.05210728570818901
Message from 0:  	 Batch:220, loss:0.04169798269867897
Message from 0:  	 Batch:221, loss:0.05621575564146042
Message from 0:  	 Batch:222, loss:0.029182109981775284
Message from 0:  	 Batch:223, loss:0.03653280809521675
Message from 0:  	 Batch:224, loss:0.037951916456222534
Message from 0:  	 Batch:225, loss:0.04503924772143364
Message from 0:  	 Batch:226, loss:0.0383940190076828
Message from 0:  	 Batch:227, loss:0.04459143429994583
Message from 0:  	 Batch:228, loss:0.038684286177158356
Message from 0:  	 Batch:229, loss:0.03577272593975067
Message from 0:  	 Batch:230, loss:0.03488434478640556
Message from 0:  	 Batch:231, loss:0.05797403305768967
Message from 0:  	 Batch:232, loss:0.03328123316168785
Message from 0:  	 Batch:233, loss:0.07391732186079025
Message from 0:  	 Batch:234, loss:0.03487437590956688
Message from 0:  	 Batch:235, loss:0.040103279054164886
Message from 0:  	 Batch:236, loss:0.06102433800697327
Message from 0:  	 Batch:237, loss:0.05911823362112045
Message from 0:  	 Batch:238, loss:0.05029314383864403
Message from 0:  	 Batch:239, loss:0.04347441345453262
Message from 0:  	 Batch:240, loss:0.03667654097080231
Message from 0:  	 Batch:241, loss:0.03267719969153404
Message from 0:  	 Batch:242, loss:0.040664736181497574
Message from 0:  	 Batch:243, loss:0.051219627261161804
Message from 0:  	 Batch:244, loss:0.05945885181427002
Message from 0:  	 Batch:245, loss:0.05236392840743065
Message from 0:  	 Batch:246, loss:0.04268863797187805
Message from 0:  	 Batch:247, loss:0.039157696068286896
Message from 0:  	 Batch:248, loss:0.04399290680885315
Message from 0:  	 Batch:249, loss:0.0415613055229187
Message from 0:  	 Batch:250, loss:0.037626929581165314
Message from 0:  	 Batch:251, loss:0.04693412035703659
Message from 0:  	 Batch:252, loss:0.05757594853639603
Message from 0:  	 Batch:253, loss:0.04903072491288185
Message from 0:  	 Batch:254, loss:0.04842294007539749
Message from 0:  	 Batch:255, loss:0.046793513000011444
Message from 0:  	 Batch:256, loss:0.03912081569433212
Message from 0:  	 Batch:257, loss:0.037780024111270905
Message from 0:  	 Batch:258, loss:0.03995705768465996
Message from 0:  	 Batch:259, loss:0.040093012154102325
Message from 0:  	 Batch:260, loss:0.03950674459338188
Message from 0:  	 Batch:261, loss:0.051155660301446915
Message from 0:  	 Batch:262, loss:0.04376795142889023
Message from 0:  	 Batch:263, loss:0.04449164867401123
Message from 0:  	 Batch:264, loss:0.04021576792001724
Message from 0:  	 Batch:265, loss:0.044701479375362396
Message from 0:  	 Batch:266, loss:0.035833291709423065
Message from 0:  	 Batch:267, loss:0.03472158685326576
Message from 0:  	 Batch:268, loss:0.04056743532419205
Message from 0:  	 Batch:269, loss:0.043028272688388824
Message from 0:  	 Batch:270, loss:0.038423169404268265
Message from 0:  	 Batch:271, loss:0.04645293578505516
Message from 0:  	 Batch:272, loss:0.03741593658924103
Message from 0:  	 Batch:273, loss:0.054994769394397736
Message from 0:  	 Batch:274, loss:0.0772457867860794
Message from 0:  	 Batch:275, loss:0.03266718238592148
Message from 0:  	 Batch:276, loss:0.04273384064435959
Message from 0:  	 Batch:277, loss:0.044437408447265625
Message from 0:  	 Batch:278, loss:0.03887089341878891
Message from 0:  	 Batch:279, loss:0.04306212067604065
Message from 0:  	 Batch:280, loss:0.058530546724796295
Message from 0:  	 Batch:281, loss:0.050047747790813446
Message from 0:  	 Batch:282, loss:0.03746180608868599
Message from 0:  	 Batch:283, loss:0.0688798576593399
Message from 0:  	 Batch:284, loss:0.06422680616378784
Message from 0:  	 Batch:285, loss:0.056610822677612305
Message from 0:  	 Batch:286, loss:0.05125516653060913
Message from 0:  	 Batch:287, loss:0.0916840136051178
Message from 0:  	 Batch:288, loss:0.04172896966338158
Message from 0:  	 Batch:289, loss:0.03955505043268204
Message from 0:  	 Batch:290, loss:0.037599701434373856
Message from 0:  	 Batch:291, loss:0.041912373155355453
Message from 0:  	 Batch:292, loss:0.0536690428853035
Message from 0:  	 Batch:293, loss:0.035726044327020645
Message from 0:  	 Batch:294, loss:0.03893037885427475
Message from 0:  	 Batch:295, loss:0.04065902531147003
Message from 0:  	 Batch:296, loss:0.0553736686706543
Message from 0:  	 Batch:297, loss:0.03850860893726349
Message from 0:  	 Batch:298, loss:0.04947291314601898
Message from 0:  	 Batch:299, loss:0.030016586184501648
Message from 0:  	 Batch:300, loss:0.043532706797122955
Message from 0:  	 Batch:301, loss:0.06003560125827789
Message from 0:  	 Batch:302, loss:0.042232267558574677
Message from 0:  	 Batch:303, loss:0.04090350493788719
Message from 0:  	 Batch:304, loss:0.04370745271444321
Message from 0:  	 Batch:305, loss:0.03523442894220352
Message from 0:  	 Batch:306, loss:0.04563559591770172
Message from 0:  	 Batch:307, loss:0.06905996054410934
Message from 0:  	 Batch:308, loss:0.034857623279094696
Message from 0:  	 Batch:309, loss:0.035439763218164444
Message from 0:  	 Batch:310, loss:0.058861084282398224
Message from 0:  	 Batch:311, loss:0.036003075540065765
Message from 0:  	 Batch:312, loss:0.062423329800367355
Message from 0:  	 Batch:313, loss:0.03220394253730774
Message from 0:  	 Batch:314, loss:0.038530390709638596
Message from 0:  	 Batch:315, loss:0.043294619768857956
Message from 0:  	 Batch:316, loss:0.05387219041585922
Message from 0:  	 Batch:317, loss:0.0489460751414299
Message from 0:  	 Batch:318, loss:0.033389702439308167
Message from 0:  	 Batch:319, loss:0.05467402935028076
Message from 0:  	 Batch:320, loss:0.03399080038070679
Message from 0:  	 Batch:321, loss:0.03784916177392006
Message from 0:  	 Batch:322, loss:0.03435837849974632
Message from 0:  	 Batch:323, loss:0.03431934118270874
Message from 0:  	 Batch:324, loss:0.04134604334831238
Message from 0:  	 Batch:325, loss:0.04029436409473419
Message from 0:  	 Batch:326, loss:0.03577939793467522
Message from 0:  	 Batch:327, loss:0.04076270014047623
Message from 0:  	 Batch:328, loss:0.03337857127189636
Message from 0:  	 Batch:329, loss:0.03203241527080536
Message from 0:  	 Batch:330, loss:0.03978855907917023
Message from 0:  	 Batch:331, loss:0.03437080979347229
Message from 0:  	 Batch:332, loss:0.03937351703643799
Message from 0:  	 Batch:333, loss:0.05578489601612091
Message from 0:  	 Batch:334, loss:0.03885535150766373
Message from 0:  	 Batch:335, loss:0.03564537316560745
Message from 0:  	 Batch:336, loss:0.09043090790510178
Message from 0:  	 Batch:337, loss:0.042027346789836884
Message from 0:  	 Batch:338, loss:0.04823955520987511
Message from 0:  	 Batch:339, loss:0.03781857341527939
Message from 0:  	 Batch:340, loss:0.04595355689525604
Message from 0:  	 Batch:341, loss:0.045813802629709244
Message from 0:  	 Batch:342, loss:0.03791980445384979
Message from 0:  	 Batch:343, loss:0.0530690997838974
Message from 0:  	 Batch:344, loss:0.03959006816148758
Message from 0:  	 Batch:345, loss:0.036207415163517
Message from 0:  	 Batch:346, loss:0.035956211388111115
Message from 0:  	 Batch:347, loss:0.03936789184808731
Message from 0:  	 Batch:348, loss:0.04010939598083496
Message from 0:  	 Batch:349, loss:0.0359148308634758
Message from 0:  	 Batch:350, loss:0.0338197685778141
Message from 0:  	 Batch:351, loss:0.040123000741004944
Message from 0:  	 Batch:352, loss:0.0401679202914238
Message from 0:  	 Batch:353, loss:0.04198740795254707
Message from 0:  	 Batch:354, loss:0.037044771015644073
Message from 0:  	 Batch:355, loss:0.03893560916185379
Message from 0:  	 Batch:356, loss:0.034255146980285645
Message from 0:  	 Batch:357, loss:0.055222656577825546
Message from 0:  	 Batch:358, loss:0.03296680748462677
Message from 0:  	 Batch:359, loss:0.059844229370355606
Message from 0:  	 Batch:360, loss:0.034893810749053955
Message from 0:  	 Batch:361, loss:0.058487419039011
Message from 0:  	 Batch:362, loss:0.03990501910448074
Message from 0:  	 Batch:363, loss:0.041920557618141174
Message from 0:  	 Batch:364, loss:0.04039302095770836
Message from 0:  	 Batch:365, loss:0.04149564355611801
Message from 0:  	 Batch:366, loss:0.0444202721118927
Message from 0:  	 Batch:367, loss:0.04583640396595001
Message from 0:  	 Batch:368, loss:0.04928340017795563
Message from 0:  	 Batch:369, loss:0.04742038995027542
Message from 0:  	 Batch:370, loss:0.04578842967748642
Message from 0:  	 Batch:371, loss:0.05091014876961708
Message from 0:  	 Batch:372, loss:0.03958931192755699
Message from 0:  	 Batch:373, loss:0.03819507732987404
Message from 0:  	 Batch:374, loss:0.04790829122066498
Message from 0:  	 Batch:375, loss:0.04144870489835739
Message from 0:  	 Batch:376, loss:0.03773054853081703
Message from 0:  	 Batch:377, loss:0.05856439098715782
Message from 0:  	 Batch:378, loss:0.05431847274303436
Message from 0:  	 Batch:379, loss:0.041223373264074326
Message from 0:  	 Batch:380, loss:0.051218368113040924
Message from 0:  	 Batch:381, loss:0.07858682423830032
Message from 0:  	 Batch:382, loss:0.03549160435795784
Message from 0:  	 Batch:383, loss:0.03871198743581772
Message from 0:  	 Batch:384, loss:0.042124535888433456
Message from 0:  	 Batch:385, loss:0.035139359533786774
Message from 0:  	 Batch:386, loss:0.0409548357129097
Message from 0:  	 Batch:387, loss:0.04543545842170715
Message from 0:  	 Batch:388, loss:0.04383796453475952
Message from 0:  	 Batch:389, loss:0.040959008038043976
Message from 0:  	 Batch:390, loss:0.03782852739095688
Message from 0:  	 Batch:391, loss:0.036379940807819366
Message from 0:  	 Batch:392, loss:0.04640892148017883
Message from 0:  	 Batch:393, loss:0.036481309682130814
Message from 0:  	 Batch:394, loss:0.03377941995859146
Message from 0:  	 Batch:395, loss:0.029451580718159676
Message from 0:  	 Batch:396, loss:0.06857012957334518
Message from 0:  	 Batch:397, loss:0.039738528430461884
Message from 0:  	 Batch:398, loss:0.030199110507965088
Message from 0:  	 Batch:399, loss:0.046893469989299774
Message from 0:  	 Batch:400, loss:0.03356163576245308
Message from 0:  	 Batch:401, loss:0.04410684108734131
Message from 0:  	 Batch:402, loss:0.04055919498205185
Message from 0:  	 Batch:403, loss:0.055368222296237946
Message from 0:  	 Batch:404, loss:0.03659076988697052
Message from 0:  	 Batch:405, loss:0.03515823185443878
Message from 0:  	 Batch:406, loss:0.037456341087818146
Message from 0:  	 Batch:407, loss:0.06627162545919418
Message from 0:  	 Batch:408, loss:0.07162328064441681
Message from 0:  	 Batch:409, loss:0.029786238446831703
Message from 0:  	 Batch:410, loss:0.0416288785636425
Message from 0:  	 Batch:411, loss:0.04332968592643738
Message from 0:  	 Batch:412, loss:0.03897302597761154
Message from 0:  	 Batch:413, loss:0.0535745769739151
Message from 0:  	 Batch:414, loss:0.03910495340824127
Message from 0:  	 Batch:415, loss:0.038497671484947205
Message from 0:  	 Batch:416, loss:0.05867321789264679
Message from 0:  	 Batch:417, loss:0.02923743799328804
Message from 0:  	 Batch:418, loss:0.034283485263586044
Message from 0:  	 Batch:419, loss:0.03482428938150406
Message from 0:  	 Batch:420, loss:0.03904277831315994
Message from 0:  	 Batch:421, loss:0.03491406887769699
Message from 0:  	 Batch:422, loss:0.03427201509475708
Message from 0:  	 Batch:423, loss:0.03883437067270279
Message from 0:  	 Batch:424, loss:0.04817262291908264
Message from 0:  	 Batch:425, loss:0.04569108784198761
Message from 0:  	 Batch:426, loss:0.03838806971907616
Message from 0:  	 Batch:427, loss:0.03436673432588577
Message from 0:  	 Batch:428, loss:0.047784969210624695
Message from 0:  	 Batch:429, loss:0.06486310064792633
Message from 0:  	 Batch:430, loss:0.032215990126132965
Message from 0:  	 Batch:431, loss:0.03842250257730484
Message from 0:  	 Batch:432, loss:0.05692335218191147
Message from 0:  	 Batch:433, loss:0.04859696328639984
Message from 0:  	 Batch:434, loss:0.039335571229457855
Message from 0:  	 Batch:435, loss:0.033497028052806854
Message from 0:  	 Batch:436, loss:0.05408322811126709
Message from 0:  	 Batch:437, loss:0.034448299556970596
Message from 0:  	 Batch:438, loss:0.04857843369245529
Message from 0:  	 Batch:439, loss:0.03627840429544449
Message from 0:  	 Batch:440, loss:0.03755203261971474
Message from 0:  	 Batch:441, loss:0.06148940697312355
Message from 0:  	 Batch:442, loss:0.05903211981058121
Message from 0:  	 Batch:443, loss:0.045522015541791916
Message from 0:  	 Batch:444, loss:0.043393947184085846
Message from 0:  	 Batch:445, loss:0.03723863512277603
Message from 0:  	 Batch:446, loss:0.03550310432910919
Message from 0:  	 Batch:447, loss:0.07346417009830475
Message from 0:  	 Batch:448, loss:0.05247574299573898
Message from 0:  	 Batch:449, loss:0.04401719570159912
Message from 0:  	 Batch:450, loss:0.03999513387680054
Message from 0:  	 Batch:451, loss:0.03121139481663704
Message from 0:  	 Batch:452, loss:0.0395747572183609
Message from 0:  	 Batch:453, loss:0.043102823197841644
Message from 0:  	 Batch:454, loss:0.05162752419710159
Message from 0:  	 Batch:455, loss:0.0329149067401886
Message from 0:  	 Batch:456, loss:0.03852662071585655
Message from 0:  	 Batch:457, loss:0.06203918531537056
Message from 0:  	 Batch:458, loss:0.037698306143283844
Message from 0:  	 Batch:459, loss:0.043303925544023514
Message from 0:  	 Batch:460, loss:0.035770196467638016
Message from 0:  	 Batch:461, loss:0.040530331432819366
Message from 0:  	 Batch:462, loss:0.06721483916044235
Message from 0:  	 Batch:463, loss:0.051835089921951294
Message from 0:  	 Batch:464, loss:0.03459922969341278
Message from 0:  	 Batch:465, loss:0.03373441845178604
Message from 0:  	 Batch:466, loss:0.03349899873137474
Message from 0:  	 Batch:467, loss:0.053392067551612854
Message from 0:  	 Batch:468, loss:0.03352949768304825
Message from 0:  	 Batch:469, loss:0.038509562611579895
Message from 0:  	 Batch:470, loss:0.07195964455604553
Message from 0:  	 Batch:471, loss:0.058046095073223114
Message from 0:  	 Batch:472, loss:0.049077123403549194
Message from 0:  	 Batch:473, loss:0.029140466824173927
Message from 0:  	 Batch:474, loss:0.03975890576839447
Message from 0:  	 Batch:475, loss:0.04556863382458687
Message from 0:  	 Batch:476, loss:0.035231586545705795
Message from 0:  	 Batch:477, loss:0.040113385766744614
Message from 0:  	 Batch:478, loss:0.03550465404987335
Message from 0:  	 Batch:479, loss:0.03779110312461853
Message from 0:  	 Batch:480, loss:0.04376555234193802
Message from 0:  	 Batch:481, loss:0.037199921905994415
Message from 0:  	 Batch:482, loss:0.05390509217977524
Message from 0:  	 Batch:483, loss:0.03612540289759636
Message from 0:  	 Batch:484, loss:0.08841042965650558
Message from 0:  	 Batch:485, loss:0.0406697578728199
Message from 0:  	 Batch:486, loss:0.046208858489990234
Message from 0:  	 Batch:487, loss:0.037720128893852234
Message from 0:  	 Batch:488, loss:0.03129872679710388
Message from 0:  	 Batch:489, loss:0.039139389991760254
Message from 0:  	 Batch:490, loss:0.041562169790267944
Message from 0:  	 Batch:491, loss:0.06259209662675858
Message from 0:  	 Batch:492, loss:0.04146731272339821
Message from 0:  	 Batch:493, loss:0.03718992695212364
Message from 0:  	 Batch:494, loss:0.04763754829764366
Message from 0:  	 Batch:495, loss:0.04186996445059776
Message from 0:  	 Batch:496, loss:0.055571381002664566
Message from 0:  	 Batch:497, loss:0.05418333783745766
Message from 0:  	 Batch:498, loss:0.04266524314880371
Message from 1:  	 Epoch:1, process-local average loss:0.04612003330788086
Message from 0:  	 Batch:499, loss:0.04159754887223244
Message from 0:  	 Epoch:1, process-local average loss:0.044125515907696325
Message from 0:  	 Batch:0, loss:0.047352612018585205
Message from 0:  	 Batch:1, loss:0.03785579651594162
Message from 0:  	 Batch:2, loss:0.0472293458878994
Message from 0:  	 Batch:3, loss:0.04433946684002876
Message from 0:  	 Batch:4, loss:0.03862713277339935
Message from 0:  	 Batch:5, loss:0.035035375505685806
Message from 0:  	 Batch:6, loss:0.03118055686354637
Message from 0:  	 Batch:7, loss:0.0335908941924572
Message from 0:  	 Batch:8, loss:0.07078103721141815
Message from 0:  	 Batch:9, loss:0.04440944641828537
Message from 0:  	 Batch:10, loss:0.040411561727523804
Message from 0:  	 Batch:11, loss:0.031977638602256775
Message from 0:  	 Batch:12, loss:0.040179651230573654
Message from 0:  	 Batch:13, loss:0.04675297439098358
Message from 0:  	 Batch:14, loss:0.04129382222890854
Message from 0:  	 Batch:15, loss:0.04289549961686134
Message from 0:  	 Batch:16, loss:0.03291894495487213
Message from 0:  	 Batch:17, loss:0.037525467574596405
Message from 0:  	 Batch:18, loss:0.06798091530799866
Message from 0:  	 Batch:19, loss:0.04043389856815338
Message from 0:  	 Batch:20, loss:0.04124601557850838
Message from 0:  	 Batch:21, loss:0.04485693573951721
Message from 0:  	 Batch:22, loss:0.03232221305370331
Message from 0:  	 Batch:23, loss:0.03245009109377861
Message from 0:  	 Batch:24, loss:0.04942930489778519
Message from 0:  	 Batch:25, loss:0.058017272502183914
Message from 0:  	 Batch:26, loss:0.04139482229948044
Message from 0:  	 Batch:27, loss:0.0415639728307724
Message from 0:  	 Batch:28, loss:0.04724704846739769
Message from 0:  	 Batch:29, loss:0.04237329959869385
Message from 0:  	 Batch:30, loss:0.05402154475450516
Message from 0:  	 Batch:31, loss:0.03454114496707916
Message from 0:  	 Batch:32, loss:0.041561514139175415
Message from 0:  	 Batch:33, loss:0.03681833669543266
Message from 0:  	 Batch:34, loss:0.03347485512495041
Message from 0:  	 Batch:35, loss:0.06279225647449493
Message from 0:  	 Batch:36, loss:0.05071927234530449
Message from 0:  	 Batch:37, loss:0.03068852424621582
Message from 0:  	 Batch:38, loss:0.04270703345537186
Message from 0:  	 Batch:39, loss:0.05497664213180542
Message from 0:  	 Batch:40, loss:0.04325898364186287
Message from 0:  	 Batch:41, loss:0.06408475339412689
Message from 0:  	 Batch:42, loss:0.04219680279493332
Message from 0:  	 Batch:43, loss:0.039505790919065475
Message from 0:  	 Batch:44, loss:0.039171621203422546
Message from 0:  	 Batch:45, loss:0.05766499787569046
Message from 0:  	 Batch:46, loss:0.03353177011013031
Message from 0:  	 Batch:47, loss:0.05073204264044762
Message from 0:  	 Batch:48, loss:0.03990329056978226
Message from 0:  	 Batch:49, loss:0.04576388746500015
Message from 0:  	 Batch:50, loss:0.03980085253715515
Message from 0:  	 Batch:51, loss:0.038787148892879486
Message from 0:  	 Batch:52, loss:0.07172789424657822
Message from 0:  	 Batch:53, loss:0.06652724742889404
Message from 0:  	 Batch:54, loss:0.03772450238466263
Message from 0:  	 Batch:55, loss:0.05040251836180687
Message from 0:  	 Batch:56, loss:0.035326555371284485
Message from 0:  	 Batch:57, loss:0.03781525790691376
Message from 0:  	 Batch:58, loss:0.04052294045686722
Message from 0:  	 Batch:59, loss:0.054959241300821304
Message from 0:  	 Batch:60, loss:0.03228458762168884
Message from 0:  	 Batch:61, loss:0.047807447612285614
Message from 0:  	 Batch:62, loss:0.04350549727678299
Message from 0:  	 Batch:63, loss:0.037264421582221985
Message from 0:  	 Batch:64, loss:0.03588060289621353
Message from 0:  	 Batch:65, loss:0.04688961058855057
Message from 0:  	 Batch:66, loss:0.03776252269744873
Message from 0:  	 Batch:67, loss:0.029524557292461395
Message from 0:  	 Batch:68, loss:0.09179465472698212
Message from 0:  	 Batch:69, loss:0.03231808543205261
Message from 0:  	 Batch:70, loss:0.04597774147987366
Message from 0:  	 Batch:71, loss:0.041848957538604736
Message from 0:  	 Batch:72, loss:0.06276573985815048
Message from 0:  	 Batch:73, loss:0.037504397332668304
Message from 0:  	 Batch:74, loss:0.04607032611966133
Message from 0:  	 Batch:75, loss:0.04388917610049248
Message from 0:  	 Batch:76, loss:0.0397983156144619
Message from 0:  	 Batch:77, loss:0.0380437970161438
Message from 0:  	 Batch:78, loss:0.04276983067393303
Message from 0:  	 Batch:79, loss:0.03196266293525696
Message from 0:  	 Batch:80, loss:0.04191645607352257
Message from 0:  	 Batch:81, loss:0.10234537720680237
Message from 0:  	 Batch:82, loss:0.08142177760601044
Message from 0:  	 Batch:83, loss:0.04815799742937088
Message from 0:  	 Batch:84, loss:0.036208078265190125
Message from 0:  	 Batch:85, loss:0.04017337039113045
Message from 0:  	 Batch:86, loss:0.03775482997298241
Message from 0:  	 Batch:87, loss:0.06340055167675018
Message from 0:  	 Batch:88, loss:0.061941977590322495
Message from 0:  	 Batch:89, loss:0.03821398690342903
Message from 0:  	 Batch:90, loss:0.03760921210050583
Message from 0:  	 Batch:91, loss:0.038029856979846954
Message from 0:  	 Batch:92, loss:0.042176879942417145
Message from 0:  	 Batch:93, loss:0.03667975962162018
Message from 0:  	 Batch:94, loss:0.052235234528779984
Message from 0:  	 Batch:95, loss:0.041130807250738144
Message from 0:  	 Batch:96, loss:0.03843870013952255
Message from 0:  	 Batch:97, loss:0.08131124079227448
Message from 0:  	 Batch:98, loss:0.04350162670016289
Message from 0:  	 Batch:99, loss:0.0373234897851944
Message from 0:  	 Batch:100, loss:0.05130466818809509
Message from 0:  	 Batch:101, loss:0.03816906735301018
Message from 0:  	 Batch:102, loss:0.04687105491757393
Message from 0:  	 Batch:103, loss:0.035124845802783966
Message from 0:  	 Batch:104, loss:0.034067198634147644
Message from 0:  	 Batch:105, loss:0.04357297718524933
Message from 0:  	 Batch:106, loss:0.046454984694719315
Message from 0:  	 Batch:107, loss:0.03950052708387375
Message from 0:  	 Batch:108, loss:0.04301643371582031
Message from 0:  	 Batch:109, loss:0.054947949945926666
Message from 0:  	 Batch:110, loss:0.03428835794329643
Message from 0:  	 Batch:111, loss:0.04224949702620506
Message from 0:  	 Batch:112, loss:0.05448630079627037
Message from 0:  	 Batch:113, loss:0.03814420849084854
Message from 0:  	 Batch:114, loss:0.03990289941430092
Message from 0:  	 Batch:115, loss:0.042460694909095764
Message from 0:  	 Batch:116, loss:0.0428117960691452
Message from 0:  	 Batch:117, loss:0.03888174518942833
Message from 0:  	 Batch:118, loss:0.043875761330127716
Message from 0:  	 Batch:119, loss:0.042464278638362885
Message from 0:  	 Batch:120, loss:0.04569655656814575
Message from 0:  	 Batch:121, loss:0.035159558057785034
Message from 0:  	 Batch:122, loss:0.055564552545547485
Message from 0:  	 Batch:123, loss:0.0402478352189064
Message from 0:  	 Batch:124, loss:0.05675092339515686
Message from 0:  	 Batch:125, loss:0.03804736211895943
Message from 0:  	 Batch:126, loss:0.040413614362478256
Message from 0:  	 Batch:127, loss:0.04549539089202881
Message from 0:  	 Batch:128, loss:0.059810031205415726
Message from 0:  	 Batch:129, loss:0.05185301601886749
Message from 0:  	 Batch:130, loss:0.03496024012565613
Message from 0:  	 Batch:131, loss:0.03727095574140549
Message from 0:  	 Batch:132, loss:0.0397668331861496
Message from 0:  	 Batch:133, loss:0.03691334277391434
Message from 0:  	 Batch:134, loss:0.06785693764686584
Message from 0:  	 Batch:135, loss:0.03410102799534798
Message from 0:  	 Batch:136, loss:0.05541063845157623
Message from 0:  	 Batch:137, loss:0.03448754921555519
Message from 0:  	 Batch:138, loss:0.04222988337278366
Message from 0:  	 Batch:139, loss:0.03593830019235611
Message from 0:  	 Batch:140, loss:0.08955776691436768
Message from 0:  	 Batch:141, loss:0.051802463829517365
Message from 0:  	 Batch:142, loss:0.041817374527454376
Message from 0:  	 Batch:143, loss:0.04332974553108215
Message from 0:  	 Batch:144, loss:0.03782105818390846
Message from 0:  	 Batch:145, loss:0.041041359305381775
Message from 0:  	 Batch:146, loss:0.03794965520501137
Message from 0:  	 Batch:147, loss:0.04052890092134476
Message from 0:  	 Batch:148, loss:0.038344018161296844
Message from 0:  	 Batch:149, loss:0.03763532638549805
Message from 0:  	 Batch:150, loss:0.03884512186050415
Message from 0:  	 Batch:151, loss:0.03183562308549881
Message from 0:  	 Batch:152, loss:0.04055371880531311
Message from 0:  	 Batch:153, loss:0.03563188016414642
Message from 0:  	 Batch:154, loss:0.03562535345554352
Message from 0:  	 Batch:155, loss:0.05515815317630768
Message from 0:  	 Batch:156, loss:0.050301894545555115
Message from 0:  	 Batch:157, loss:0.042936645448207855
Message from 0:  	 Batch:158, loss:0.04352415353059769
Message from 0:  	 Batch:159, loss:0.04023437947034836
Message from 0:  	 Batch:160, loss:0.03833554685115814
Message from 0:  	 Batch:161, loss:0.04946506768465042
Message from 0:  	 Batch:162, loss:0.03302222862839699
Message from 0:  	 Batch:163, loss:0.041409119963645935
Message from 0:  	 Batch:164, loss:0.04154819995164871
Message from 0:  	 Batch:165, loss:0.046747587621212006
Message from 0:  	 Batch:166, loss:0.045336298644542694
Message from 0:  	 Batch:167, loss:0.03912065178155899
Message from 0:  	 Batch:168, loss:0.07525888085365295
Message from 0:  	 Batch:169, loss:0.038336873054504395
Message from 0:  	 Batch:170, loss:0.036925047636032104
Message from 0:  	 Batch:171, loss:0.030892569571733475
Message from 0:  	 Batch:172, loss:0.057442229241132736
Message from 0:  	 Batch:173, loss:0.029157016426324844
Message from 0:  	 Batch:174, loss:0.05296586453914642
Message from 0:  	 Batch:175, loss:0.03932911530137062
Message from 0:  	 Batch:176, loss:0.035160087049007416
Message from 0:  	 Batch:177, loss:0.035866133868694305
Message from 0:  	 Batch:178, loss:0.035191819071769714
Message from 0:  	 Batch:179, loss:0.04552733153104782
Message from 0:  	 Batch:180, loss:0.03627265617251396
Message from 0:  	 Batch:181, loss:0.036003850400447845
Message from 0:  	 Batch:182, loss:0.034566730260849
Message from 0:  	 Batch:183, loss:0.042209286242723465
Message from 0:  	 Batch:184, loss:0.04130581021308899
Message from 0:  	 Batch:185, loss:0.03424069285392761
Message from 0:  	 Batch:186, loss:0.05108851566910744
Message from 0:  	 Batch:187, loss:0.03722728043794632
Message from 0:  	 Batch:188, loss:0.04008524864912033
Message from 0:  	 Batch:189, loss:0.03482919931411743
Message from 0:  	 Batch:190, loss:0.056088972836732864
Message from 0:  	 Batch:191, loss:0.05551700294017792
Message from 0:  	 Batch:192, loss:0.04599277302622795
Message from 0:  	 Batch:193, loss:0.05770016461610794
Message from 0:  	 Batch:194, loss:0.035301145166158676
Message from 0:  	 Batch:195, loss:0.04864663630723953
Message from 0:  	 Batch:196, loss:0.030674222856760025
Message from 0:  	 Batch:197, loss:0.043911077082157135
Message from 0:  	 Batch:198, loss:0.0334276519715786
Message from 0:  	 Batch:199, loss:0.04454801231622696
Message from 0:  	 Batch:200, loss:0.05200383439660072
Message from 0:  	 Batch:201, loss:0.059339895844459534
Message from 0:  	 Batch:202, loss:0.0389544740319252
Message from 0:  	 Batch:203, loss:0.04109995812177658
Message from 0:  	 Batch:204, loss:0.03105546534061432
Message from 0:  	 Batch:205, loss:0.03969667851924896
Message from 0:  	 Batch:206, loss:0.0383865162730217
Message from 0:  	 Batch:207, loss:0.04026935249567032
Message from 0:  	 Batch:208, loss:0.04155482351779938
Message from 0:  	 Batch:209, loss:0.049638159573078156
Message from 0:  	 Batch:210, loss:0.07111845165491104
Message from 0:  	 Batch:211, loss:0.03690280020236969
Message from 0:  	 Batch:212, loss:0.0654437392950058
Message from 0:  	 Batch:213, loss:0.07277505099773407
Message from 0:  	 Batch:214, loss:0.041438184678554535
Message from 0:  	 Batch:215, loss:0.03206302225589752
Message from 0:  	 Batch:216, loss:0.0399494469165802
Message from 0:  	 Batch:217, loss:0.05872604250907898
Message from 0:  	 Batch:218, loss:0.038553401827812195
Message from 0:  	 Batch:219, loss:0.049884408712387085
Message from 0:  	 Batch:220, loss:0.040533244609832764
Message from 0:  	 Batch:221, loss:0.05457763373851776
Message from 0:  	 Batch:222, loss:0.02916608564555645
Message from 0:  	 Batch:223, loss:0.03641332685947418
Message from 0:  	 Batch:224, loss:0.03780750930309296
Message from 0:  	 Batch:225, loss:0.0438152514398098
Message from 0:  	 Batch:226, loss:0.03759363666176796
Message from 0:  	 Batch:227, loss:0.04408986121416092
Message from 0:  	 Batch:228, loss:0.03895432502031326
Message from 0:  	 Batch:229, loss:0.03525259718298912
Message from 0:  	 Batch:230, loss:0.0338563546538353
Message from 0:  	 Batch:231, loss:0.056032828986644745
Message from 0:  	 Batch:232, loss:0.03266071528196335
Message from 0:  	 Batch:233, loss:0.07265928387641907
Message from 0:  	 Batch:234, loss:0.03461480885744095
Message from 0:  	 Batch:235, loss:0.03978678956627846
Message from 0:  	 Batch:236, loss:0.05820666253566742
Message from 0:  	 Batch:237, loss:0.05678248405456543
Message from 0:  	 Batch:238, loss:0.04809916391968727
Message from 0:  	 Batch:239, loss:0.0432589054107666
Message from 0:  	 Batch:240, loss:0.036021821200847626
Message from 0:  	 Batch:241, loss:0.03185437247157097
Message from 0:  	 Batch:242, loss:0.04063616693019867
Message from 0:  	 Batch:243, loss:0.04871629178524017
Message from 0:  	 Batch:244, loss:0.05824992060661316
Message from 0:  	 Batch:245, loss:0.050752099603414536
Message from 0:  	 Batch:246, loss:0.04267985373735428
Message from 0:  	 Batch:247, loss:0.038792721927165985
Message from 0:  	 Batch:248, loss:0.04402562975883484
Message from 0:  	 Batch:249, loss:0.04097779095172882
Message from 0:  	 Batch:250, loss:0.036049190908670425
Message from 0:  	 Batch:251, loss:0.04474542289972305
Message from 0:  	 Batch:252, loss:0.055351197719573975
Message from 0:  	 Batch:253, loss:0.04814872145652771
Message from 0:  	 Batch:254, loss:0.04750653728842735
Message from 0:  	 Batch:255, loss:0.045502424240112305
Message from 0:  	 Batch:256, loss:0.03850114345550537
Message from 0:  	 Batch:257, loss:0.03769645467400551
Message from 0:  	 Batch:258, loss:0.03822429105639458
Message from 0:  	 Batch:259, loss:0.040091440081596375
Message from 0:  	 Batch:260, loss:0.03897182643413544
Message from 0:  	 Batch:261, loss:0.049756888300180435
Message from 0:  	 Batch:262, loss:0.042998604476451874
Message from 0:  	 Batch:263, loss:0.04389004409313202
Message from 0:  	 Batch:264, loss:0.03941468149423599
Message from 0:  	 Batch:265, loss:0.04278735816478729
Message from 0:  	 Batch:266, loss:0.03488542512059212
Message from 0:  	 Batch:267, loss:0.03396987542510033
Message from 0:  	 Batch:268, loss:0.040181875228881836
Message from 0:  	 Batch:269, loss:0.043117642402648926
Message from 0:  	 Batch:270, loss:0.037996213883161545
Message from 0:  	 Batch:271, loss:0.04524679481983185
Message from 0:  	 Batch:272, loss:0.03673764318227768
Message from 0:  	 Batch:273, loss:0.054088905453681946
Message from 0:  	 Batch:274, loss:0.07752534002065659
Message from 0:  	 Batch:275, loss:0.03269660845398903
Message from 0:  	 Batch:276, loss:0.042269594967365265
Message from 0:  	 Batch:277, loss:0.043173354119062424
Message from 0:  	 Batch:278, loss:0.038842398673295975
Message from 0:  	 Batch:279, loss:0.04262641817331314
Message from 0:  	 Batch:280, loss:0.055729590356349945
Message from 0:  	 Batch:281, loss:0.04872070625424385
Message from 0:  	 Batch:282, loss:0.03644731268286705
Message from 0:  	 Batch:283, loss:0.06579247117042542
Message from 0:  	 Batch:284, loss:0.062058936804533005
Message from 0:  	 Batch:285, loss:0.05560971051454544
Message from 0:  	 Batch:286, loss:0.05050722137093544
Message from 0:  	 Batch:287, loss:0.091027170419693
Message from 0:  	 Batch:288, loss:0.041634105145931244
Message from 0:  	 Batch:289, loss:0.039444200694561005
Message from 0:  	 Batch:290, loss:0.03724806010723114
Message from 0:  	 Batch:291, loss:0.041545696556568146
Message from 0:  	 Batch:292, loss:0.05258473753929138
Message from 0:  	 Batch:293, loss:0.03533739596605301
Message from 0:  	 Batch:294, loss:0.03855025768280029
Message from 0:  	 Batch:295, loss:0.04122023284435272
Message from 0:  	 Batch:296, loss:0.05434457212686539
Message from 0:  	 Batch:297, loss:0.03813667222857475
Message from 0:  	 Batch:298, loss:0.04858388751745224
Message from 0:  	 Batch:299, loss:0.029299311339855194
Message from 0:  	 Batch:300, loss:0.04285891354084015
Message from 0:  	 Batch:301, loss:0.05779141187667847
Message from 0:  	 Batch:302, loss:0.041929714381694794
Message from 0:  	 Batch:303, loss:0.0408797562122345
Message from 0:  	 Batch:304, loss:0.04305761307477951
Message from 0:  	 Batch:305, loss:0.033777184784412384
Message from 0:  	 Batch:306, loss:0.04463464766740799
Message from 0:  	 Batch:307, loss:0.06874021887779236
Message from 0:  	 Batch:308, loss:0.034565530717372894
Message from 0:  	 Batch:309, loss:0.034550026059150696
Message from 0:  	 Batch:310, loss:0.0552147813141346
Message from 0:  	 Batch:311, loss:0.03593650460243225
Message from 0:  	 Batch:312, loss:0.06336851418018341
Message from 0:  	 Batch:313, loss:0.03142470866441727
Message from 0:  	 Batch:314, loss:0.03865282982587814
Message from 0:  	 Batch:315, loss:0.04182760789990425
Message from 0:  	 Batch:316, loss:0.05320793390274048
Message from 0:  	 Batch:317, loss:0.04690122604370117
Message from 0:  	 Batch:318, loss:0.03299306705594063
Message from 0:  	 Batch:319, loss:0.05299420654773712
Message from 0:  	 Batch:320, loss:0.033415909856557846
Message from 0:  	 Batch:321, loss:0.037155091762542725
Message from 0:  	 Batch:322, loss:0.03398504853248596
Message from 0:  	 Batch:323, loss:0.03390824422240257
Message from 0:  	 Batch:324, loss:0.041469864547252655
Message from 0:  	 Batch:325, loss:0.040435437113046646
Message from 0:  	 Batch:326, loss:0.03442419692873955
Message from 0:  	 Batch:327, loss:0.039212681353092194
Message from 0:  	 Batch:328, loss:0.03297699615359306
Message from 0:  	 Batch:329, loss:0.03194783255457878
Message from 0:  	 Batch:330, loss:0.03975605219602585
Message from 0:  	 Batch:331, loss:0.03390655294060707
Message from 0:  	 Batch:332, loss:0.03924814611673355
Message from 0:  	 Batch:333, loss:0.0543951652944088
Message from 0:  	 Batch:334, loss:0.03897286206483841
Message from 0:  	 Batch:335, loss:0.035336412489414215
Message from 0:  	 Batch:336, loss:0.08935824036598206
Message from 0:  	 Batch:337, loss:0.04147222638130188
Message from 0:  	 Batch:338, loss:0.04628869146108627
Message from 0:  	 Batch:339, loss:0.037604108452796936
Message from 0:  	 Batch:340, loss:0.04480455815792084
Message from 0:  	 Batch:341, loss:0.04531287029385567
Message from 0:  	 Batch:342, loss:0.0375785306096077
Message from 0:  	 Batch:343, loss:0.05277474224567413
Message from 0:  	 Batch:344, loss:0.03950247913599014
Message from 0:  	 Batch:345, loss:0.03585793077945709
Message from 0:  	 Batch:346, loss:0.03576739877462387
Message from 0:  	 Batch:347, loss:0.03955768048763275
Message from 0:  	 Batch:348, loss:0.04008891433477402
Message from 0:  	 Batch:349, loss:0.03412047401070595
Message from 0:  	 Batch:350, loss:0.03269638866186142
Message from 0:  	 Batch:351, loss:0.04038028419017792
Message from 0:  	 Batch:352, loss:0.03973422944545746
Message from 0:  	 Batch:353, loss:0.04162721335887909
Message from 0:  	 Batch:354, loss:0.03623947501182556
Message from 0:  	 Batch:355, loss:0.03839176520705223
Message from 0:  	 Batch:356, loss:0.03403935581445694
Message from 0:  	 Batch:357, loss:0.05451182276010513
Message from 0:  	 Batch:358, loss:0.03250629082322121
Message from 0:  	 Batch:359, loss:0.058126263320446014
Message from 0:  	 Batch:360, loss:0.03431231901049614
Message from 0:  	 Batch:361, loss:0.057174235582351685
Message from 0:  	 Batch:362, loss:0.039217013865709305
Message from 0:  	 Batch:363, loss:0.04186694696545601
Message from 0:  	 Batch:364, loss:0.040217749774456024
Message from 0:  	 Batch:365, loss:0.04136056452989578
Message from 0:  	 Batch:366, loss:0.044405221939086914
Message from 0:  	 Batch:367, loss:0.04558683931827545
Message from 0:  	 Batch:368, loss:0.04839089512825012
Message from 0:  	 Batch:369, loss:0.045665860176086426
Message from 0:  	 Batch:370, loss:0.04493538662791252
Message from 0:  	 Batch:371, loss:0.048840899020433426
Message from 0:  	 Batch:372, loss:0.03930622339248657
Message from 0:  	 Batch:373, loss:0.038294918835163116
Message from 0:  	 Batch:374, loss:0.045754022896289825
Message from 0:  	 Batch:375, loss:0.04142868518829346
Message from 0:  	 Batch:376, loss:0.0375080369412899
Message from 0:  	 Batch:377, loss:0.05732901394367218
Message from 0:  	 Batch:378, loss:0.05317922681570053
Message from 0:  	 Batch:379, loss:0.0408640131354332
Message from 0:  	 Batch:380, loss:0.04813053458929062
Message from 0:  	 Batch:381, loss:0.0772090032696724
Message from 0:  	 Batch:382, loss:0.035040467977523804
Message from 0:  	 Batch:383, loss:0.0384470596909523
Message from 0:  	 Batch:384, loss:0.04183848202228546
Message from 0:  	 Batch:385, loss:0.03491590917110443
Message from 0:  	 Batch:386, loss:0.04136613383889198
Message from 0:  	 Batch:387, loss:0.04404716193675995
Message from 0:  	 Batch:388, loss:0.042299751192331314
Message from 0:  	 Batch:389, loss:0.04115443676710129
Message from 0:  	 Batch:390, loss:0.037507131695747375
Message from 0:  	 Batch:391, loss:0.03608766198158264
Message from 0:  	 Batch:392, loss:0.045998163521289825
Message from 0:  	 Batch:393, loss:0.0363205187022686
Message from 0:  	 Batch:394, loss:0.032922010868787766
Message from 0:  	 Batch:395, loss:0.028042759746313095
Message from 0:  	 Batch:396, loss:0.06474971771240234
Message from 0:  	 Batch:397, loss:0.03939688950777054
Message from 0:  	 Batch:398, loss:0.030035894364118576
Message from 0:  	 Batch:399, loss:0.04567338526248932
Message from 0:  	 Batch:400, loss:0.03291785344481468
Message from 0:  	 Batch:401, loss:0.04340960830450058
Message from 0:  	 Batch:402, loss:0.04028838872909546
Message from 0:  	 Batch:403, loss:0.054649222642183304
Message from 0:  	 Batch:404, loss:0.03626304119825363
Message from 0:  	 Batch:405, loss:0.0347718819975853
Message from 0:  	 Batch:406, loss:0.036881085485219955
Message from 0:  	 Batch:407, loss:0.06537234783172607
Message from 0:  	 Batch:408, loss:0.07103640586137772
Message from 0:  	 Batch:409, loss:0.028627092018723488
Message from 0:  	 Batch:410, loss:0.04228955879807472
Message from 0:  	 Batch:411, loss:0.04307761788368225
Message from 0:  	 Batch:412, loss:0.03890214115381241
Message from 0:  	 Batch:413, loss:0.05266798287630081
Message from 0:  	 Batch:414, loss:0.03858327865600586
Message from 0:  	 Batch:415, loss:0.03691933676600456
Message from 0:  	 Batch:416, loss:0.056146569550037384
Message from 0:  	 Batch:417, loss:0.02939920872449875
Message from 0:  	 Batch:418, loss:0.0335877388715744
Message from 0:  	 Batch:419, loss:0.034397419542074203
Message from 0:  	 Batch:420, loss:0.038688868284225464
Message from 0:  	 Batch:421, loss:0.033813633024692535
Message from 0:  	 Batch:422, loss:0.0335276760160923
Message from 0:  	 Batch:423, loss:0.038893453776836395
Message from 0:  	 Batch:424, loss:0.04755778610706329
Message from 0:  	 Batch:425, loss:0.044468194246292114
Message from 0:  	 Batch:426, loss:0.038088358938694
Message from 0:  	 Batch:427, loss:0.034116264432668686
Message from 0:  	 Batch:428, loss:0.045806773006916046
Message from 0:  	 Batch:429, loss:0.0634160190820694
Message from 0:  	 Batch:430, loss:0.030360009521245956
Message from 0:  	 Batch:431, loss:0.03786693513393402
Message from 0:  	 Batch:432, loss:0.05619065836071968
Message from 0:  	 Batch:433, loss:0.04794350638985634
Message from 0:  	 Batch:434, loss:0.03913239762187004
Message from 0:  	 Batch:435, loss:0.03303440660238266
Message from 0:  	 Batch:436, loss:0.052380215376615524
Message from 0:  	 Batch:437, loss:0.034626517444849014
Message from 0:  	 Batch:438, loss:0.04781538248062134
Message from 0:  	 Batch:439, loss:0.03646592050790787
Message from 0:  	 Batch:440, loss:0.03738279640674591
Message from 0:  	 Batch:441, loss:0.059799641370773315
Message from 0:  	 Batch:442, loss:0.056936830282211304
Message from 0:  	 Batch:443, loss:0.04489116370677948
Message from 0:  	 Batch:444, loss:0.04311206564307213
Message from 0:  	 Batch:445, loss:0.036897044628858566
Message from 0:  	 Batch:446, loss:0.034268856048583984
Message from 0:  	 Batch:447, loss:0.07149029523134232
Message from 0:  	 Batch:448, loss:0.05154988914728165
Message from 0:  	 Batch:449, loss:0.04384999722242355
Message from 0:  	 Batch:450, loss:0.04001608118414879
Message from 0:  	 Batch:451, loss:0.030384907498955727
Message from 0:  	 Batch:452, loss:0.03945259377360344
Message from 0:  	 Batch:453, loss:0.04284349083900452
Message from 0:  	 Batch:454, loss:0.04930566996335983
Message from 0:  	 Batch:455, loss:0.03181895613670349
Message from 0:  	 Batch:456, loss:0.038272760808467865
Message from 0:  	 Batch:457, loss:0.05967225134372711
Message from 0:  	 Batch:458, loss:0.03805675730109215
Message from 0:  	 Batch:459, loss:0.04309643805027008
Message from 0:  	 Batch:460, loss:0.03556378185749054
Message from 0:  	 Batch:461, loss:0.039938658475875854
Message from 0:  	 Batch:462, loss:0.0663662999868393
Message from 0:  	 Batch:463, loss:0.050650984048843384
Message from 0:  	 Batch:464, loss:0.03427736088633537
Message from 0:  	 Batch:465, loss:0.03315405920147896
Message from 0:  	 Batch:466, loss:0.03342729061841965
Message from 0:  	 Batch:467, loss:0.05137822777032852
Message from 0:  	 Batch:468, loss:0.033237211406230927
Message from 0:  	 Batch:469, loss:0.03821233659982681
Message from 0:  	 Batch:470, loss:0.06902434676885605
Message from 0:  	 Batch:471, loss:0.05516805499792099
Message from 0:  	 Batch:472, loss:0.04607691988348961
Message from 0:  	 Batch:473, loss:0.029159478843212128
Message from 0:  	 Batch:474, loss:0.039496082812547684
Message from 0:  	 Batch:475, loss:0.04344020038843155
Message from 0:  	 Batch:476, loss:0.03544201701879501
Message from 0:  	 Batch:477, loss:0.03986019641160965
Message from 0:  	 Batch:478, loss:0.035283029079437256
Message from 0:  	 Batch:479, loss:0.03754904866218567
Message from 0:  	 Batch:480, loss:0.04328693822026253
Message from 0:  	 Batch:481, loss:0.036826908588409424
Message from 0:  	 Batch:482, loss:0.05228541046380997
Message from 0:  	 Batch:483, loss:0.03542175889015198
Message from 0:  	 Batch:484, loss:0.0876174122095108
Message from 0:  	 Batch:485, loss:0.04024181514978409
Message from 0:  	 Batch:486, loss:0.044997431337833405
Message from 0:  	 Batch:487, loss:0.03749816119670868
Message from 0:  	 Batch:488, loss:0.030860962346196175
Message from 0:  	 Batch:489, loss:0.03897994011640549
Message from 0:  	 Batch:490, loss:0.04131878912448883
Message from 0:  	 Batch:491, loss:0.061810340732336044
Message from 0:  	 Batch:492, loss:0.041264645755290985
Message from 0:  	 Batch:493, loss:0.03610476106405258
Message from 0:  	 Batch:494, loss:0.04656539857387543
Message from 0:  	 Batch:495, loss:0.04200077801942825
Message from 0:  	 Batch:496, loss:0.054344311356544495
Message from 0:  	 Batch:497, loss:0.053148530423641205
Message from 0:  	 Batch:498, loss:0.04262930899858475
Message from 1:  	 Epoch:2, process-local average loss:0.04472095053749422
Message from 0:  	 Batch:499, loss:0.0416240356862545
Message from 0:  	 Epoch:2, process-local average loss:0.042916258449000974
Message from 0:  	 Batch:0, loss:0.04743242636322975
Message from 0:  	 Batch:1, loss:0.037492066621780396
Message from 0:  	 Batch:2, loss:0.04635956138372421
Message from 0:  	 Batch:3, loss:0.044188499450683594
Message from 0:  	 Batch:4, loss:0.038596101105213165
Message from 0:  	 Batch:5, loss:0.03490457683801651
Message from 0:  	 Batch:6, loss:0.03050862066447735
Message from 0:  	 Batch:7, loss:0.0332607664167881
Message from 0:  	 Batch:8, loss:0.07116934657096863
Message from 0:  	 Batch:9, loss:0.04403702914714813
Message from 0:  	 Batch:10, loss:0.040538541972637177
Message from 0:  	 Batch:11, loss:0.03134460374712944
Message from 0:  	 Batch:12, loss:0.03921179473400116
Message from 0:  	 Batch:13, loss:0.04638732597231865
Message from 0:  	 Batch:14, loss:0.040226854383945465
Message from 0:  	 Batch:15, loss:0.0423247329890728
Message from 0:  	 Batch:16, loss:0.03265924006700516
Message from 0:  	 Batch:17, loss:0.037495993077754974
Message from 0:  	 Batch:18, loss:0.06783732026815414
Message from 0:  	 Batch:19, loss:0.04017464071512222
Message from 0:  	 Batch:20, loss:0.04131191223859787
Message from 0:  	 Batch:21, loss:0.044613152742385864
Message from 0:  	 Batch:22, loss:0.032590776681900024
Message from 0:  	 Batch:23, loss:0.03257521241903305
Message from 0:  	 Batch:24, loss:0.04852302744984627
Message from 0:  	 Batch:25, loss:0.05693265423178673
Message from 0:  	 Batch:26, loss:0.041143663227558136
Message from 0:  	 Batch:27, loss:0.041614145040512085
Message from 0:  	 Batch:28, loss:0.04482443258166313
Message from 0:  	 Batch:29, loss:0.040798790752887726
Message from 0:  	 Batch:30, loss:0.05268189311027527
Message from 0:  	 Batch:31, loss:0.034479767084121704
Message from 0:  	 Batch:32, loss:0.04152879863977432
Message from 0:  	 Batch:33, loss:0.036446016281843185
Message from 0:  	 Batch:34, loss:0.03356080502271652
Message from 0:  	 Batch:35, loss:0.06136884540319443
Message from 0:  	 Batch:36, loss:0.048798538744449615
Message from 0:  	 Batch:37, loss:0.030726097524166107
Message from 0:  	 Batch:38, loss:0.042151205241680145
Message from 0:  	 Batch:39, loss:0.05379997938871384
Message from 0:  	 Batch:40, loss:0.04329921305179596
Message from 0:  	 Batch:41, loss:0.06215829402208328
Message from 0:  	 Batch:42, loss:0.042350903153419495
Message from 0:  	 Batch:43, loss:0.03984544798731804
Message from 0:  	 Batch:44, loss:0.038651738315820694
Message from 0:  	 Batch:45, loss:0.05496722832322121
Message from 0:  	 Batch:46, loss:0.03345227241516113
Message from 0:  	 Batch:47, loss:0.04992014169692993
Message from 0:  	 Batch:48, loss:0.03858340531587601
Message from 0:  	 Batch:49, loss:0.044765546917915344
Message from 0:  	 Batch:50, loss:0.04012676328420639
Message from 0:  	 Batch:51, loss:0.03790765628218651
Message from 0:  	 Batch:52, loss:0.06886515021324158
Message from 0:  	 Batch:53, loss:0.06595390290021896
Message from 0:  	 Batch:54, loss:0.03751318156719208
Message from 0:  	 Batch:55, loss:0.04858177527785301
Message from 0:  	 Batch:56, loss:0.03516898304224014
Message from 0:  	 Batch:57, loss:0.03768753632903099
Message from 0:  	 Batch:58, loss:0.04062706232070923
Message from 0:  	 Batch:59, loss:0.05349984019994736
Message from 0:  	 Batch:60, loss:0.03207388520240784
Message from 0:  	 Batch:61, loss:0.04673384875059128
Message from 0:  	 Batch:62, loss:0.04282717406749725
Message from 0:  	 Batch:63, loss:0.03685775771737099
Message from 0:  	 Batch:64, loss:0.03552427887916565
Message from 0:  	 Batch:65, loss:0.045256227254867554
Message from 0:  	 Batch:66, loss:0.037544965744018555
Message from 0:  	 Batch:67, loss:0.0297361072152853
Message from 0:  	 Batch:68, loss:0.0907653272151947
Message from 0:  	 Batch:69, loss:0.031334035098552704
Message from 0:  	 Batch:70, loss:0.04405323415994644
Message from 0:  	 Batch:71, loss:0.041588954627513885
Message from 0:  	 Batch:72, loss:0.06102911755442619
Message from 0:  	 Batch:73, loss:0.03719251602888107
Message from 0:  	 Batch:74, loss:0.04477859288454056
Message from 0:  	 Batch:75, loss:0.0438164621591568
Message from 0:  	 Batch:76, loss:0.039709094911813736
Message from 0:  	 Batch:77, loss:0.03815135359764099
Message from 0:  	 Batch:78, loss:0.04258178174495697
Message from 0:  	 Batch:79, loss:0.03147426247596741
Message from 0:  	 Batch:80, loss:0.041820280253887177
Message from 0:  	 Batch:81, loss:0.10202094167470932
Message from 0:  	 Batch:82, loss:0.0804981142282486
Message from 0:  	 Batch:83, loss:0.0471213273704052
Message from 0:  	 Batch:84, loss:0.03563332185149193
Message from 0:  	 Batch:85, loss:0.03903549537062645
Message from 0:  	 Batch:86, loss:0.03703124448657036
Message from 0:  	 Batch:87, loss:0.06234440207481384
Message from 0:  	 Batch:88, loss:0.060986898839473724
Message from 0:  	 Batch:89, loss:0.038609348237514496
Message from 0:  	 Batch:90, loss:0.03736291825771332
Message from 0:  	 Batch:91, loss:0.037342097610235214
Message from 0:  	 Batch:92, loss:0.04198348522186279
Message from 0:  	 Batch:93, loss:0.03642190247774124
Message from 0:  	 Batch:94, loss:0.051252804696559906
Message from 0:  	 Batch:95, loss:0.041059330105781555
Message from 0:  	 Batch:96, loss:0.037949129939079285
Message from 0:  	 Batch:97, loss:0.08091326802968979
Message from 0:  	 Batch:98, loss:0.04239290952682495
Message from 0:  	 Batch:99, loss:0.03729172796010971
Message from 0:  	 Batch:100, loss:0.050205666571855545
Message from 0:  	 Batch:101, loss:0.038149863481521606
Message from 0:  	 Batch:102, loss:0.046246688812971115
Message from 0:  	 Batch:103, loss:0.035207539796829224
Message from 0:  	 Batch:104, loss:0.03378764167428017
Message from 0:  	 Batch:105, loss:0.0435645654797554
Message from 0:  	 Batch:106, loss:0.04633849859237671
Message from 0:  	 Batch:107, loss:0.03945033624768257
Message from 0:  	 Batch:108, loss:0.04276901111006737
Message from 0:  	 Batch:109, loss:0.05393694341182709
Message from 0:  	 Batch:110, loss:0.034261174499988556
Message from 0:  	 Batch:111, loss:0.04095669463276863
Message from 0:  	 Batch:112, loss:0.05274996906518936
Message from 0:  	 Batch:113, loss:0.038297489285469055
Message from 0:  	 Batch:114, loss:0.03934473171830177
Message from 0:  	 Batch:115, loss:0.04280422627925873
Message from 0:  	 Batch:116, loss:0.04276340454816818
Message from 0:  	 Batch:117, loss:0.03896719217300415
Message from 0:  	 Batch:118, loss:0.04380204528570175
Message from 0:  	 Batch:119, loss:0.042104028165340424
Message from 0:  	 Batch:120, loss:0.04564235359430313
Message from 0:  	 Batch:121, loss:0.03439278155565262
Message from 0:  	 Batch:122, loss:0.05399768799543381
Message from 0:  	 Batch:123, loss:0.04031842201948166
Message from 0:  	 Batch:124, loss:0.05578573793172836
Message from 0:  	 Batch:125, loss:0.03812423348426819
Message from 0:  	 Batch:126, loss:0.040402062237262726
Message from 0:  	 Batch:127, loss:0.04461219161748886
Message from 0:  	 Batch:128, loss:0.05872933939099312
Message from 0:  	 Batch:129, loss:0.05043482035398483
Message from 0:  	 Batch:130, loss:0.035028718411922455
Message from 0:  	 Batch:131, loss:0.03704969584941864
Message from 0:  	 Batch:132, loss:0.039634399116039276
Message from 0:  	 Batch:133, loss:0.036921776831150055
Message from 0:  	 Batch:134, loss:0.06555140018463135
Message from 0:  	 Batch:135, loss:0.03415024280548096
Message from 0:  	 Batch:136, loss:0.053924642503261566
Message from 0:  	 Batch:137, loss:0.03449691832065582
Message from 0:  	 Batch:138, loss:0.042185112833976746
Message from 0:  	 Batch:139, loss:0.03629860281944275
Message from 0:  	 Batch:140, loss:0.08936072885990143
Message from 0:  	 Batch:141, loss:0.05012276768684387
Message from 0:  	 Batch:142, loss:0.04125026986002922
Message from 0:  	 Batch:143, loss:0.04336521029472351
Message from 0:  	 Batch:144, loss:0.03678850829601288
Message from 0:  	 Batch:145, loss:0.04082440212368965
Message from 0:  	 Batch:146, loss:0.038617704063653946
Message from 0:  	 Batch:147, loss:0.04027383029460907
Message from 0:  	 Batch:148, loss:0.03706079721450806
Message from 0:  	 Batch:149, loss:0.037184566259384155
Message from 0:  	 Batch:150, loss:0.03890978544950485
Message from 0:  	 Batch:151, loss:0.031698718667030334
Message from 0:  	 Batch:152, loss:0.04024725407361984
Message from 0:  	 Batch:153, loss:0.03503785282373428
Message from 0:  	 Batch:154, loss:0.035493914037942886
Message from 0:  	 Batch:155, loss:0.055115580558776855
Message from 0:  	 Batch:156, loss:0.04973370581865311
Message from 0:  	 Batch:157, loss:0.04291721433401108
Message from 0:  	 Batch:158, loss:0.042623721063137054
Message from 0:  	 Batch:159, loss:0.040007878094911575
Message from 0:  	 Batch:160, loss:0.03809887915849686
Message from 0:  	 Batch:161, loss:0.04842882603406906
Message from 0:  	 Batch:162, loss:0.03248322755098343
Message from 0:  	 Batch:163, loss:0.04140076786279678
Message from 0:  	 Batch:164, loss:0.041569292545318604
Message from 0:  	 Batch:165, loss:0.04582752659916878
Message from 0:  	 Batch:166, loss:0.044601500034332275
Message from 0:  	 Batch:167, loss:0.039102017879486084
Message from 0:  	 Batch:168, loss:0.07352571189403534
Message from 0:  	 Batch:169, loss:0.038350313901901245
Message from 0:  	 Batch:170, loss:0.03588299825787544
Message from 0:  	 Batch:171, loss:0.02958141267299652
Message from 0:  	 Batch:172, loss:0.05596943199634552
Message from 0:  	 Batch:173, loss:0.028959251940250397
Message from 0:  	 Batch:174, loss:0.05194312706589699
Message from 0:  	 Batch:175, loss:0.039222437888383865
Message from 0:  	 Batch:176, loss:0.034857653081417084
Message from 0:  	 Batch:177, loss:0.03542149439454079
Message from 0:  	 Batch:178, loss:0.03529597073793411
Message from 0:  	 Batch:179, loss:0.044374920427799225
Message from 0:  	 Batch:180, loss:0.03606998547911644
Message from 0:  	 Batch:181, loss:0.03586428612470627
Message from 0:  	 Batch:182, loss:0.03452730551362038
Message from 0:  	 Batch:183, loss:0.04158051311969757
Message from 0:  	 Batch:184, loss:0.0412636399269104
Message from 0:  	 Batch:185, loss:0.03383784741163254
Message from 0:  	 Batch:186, loss:0.05038373917341232
Message from 0:  	 Batch:187, loss:0.037081971764564514
Message from 0:  	 Batch:188, loss:0.039751000702381134
Message from 0:  	 Batch:189, loss:0.03447817265987396
Message from 0:  	 Batch:190, loss:0.054407402873039246
Message from 0:  	 Batch:191, loss:0.055767789483070374
Message from 0:  	 Batch:192, loss:0.04391578212380409
Message from 0:  	 Batch:193, loss:0.05591876059770584
Message from 0:  	 Batch:194, loss:0.035245660692453384
Message from 0:  	 Batch:195, loss:0.048087894916534424
Message from 0:  	 Batch:196, loss:0.0313396155834198
Message from 0:  	 Batch:197, loss:0.04358753189444542
Message from 0:  	 Batch:198, loss:0.033169202506542206
Message from 0:  	 Batch:199, loss:0.04447565972805023
Message from 0:  	 Batch:200, loss:0.05036770552396774
Message from 0:  	 Batch:201, loss:0.05827774852514267
Message from 0:  	 Batch:202, loss:0.03894413262605667
Message from 0:  	 Batch:203, loss:0.04062046855688095
Message from 0:  	 Batch:204, loss:0.03045525588095188
Message from 0:  	 Batch:205, loss:0.039505816996097565
Message from 0:  	 Batch:206, loss:0.03849758952856064
Message from 0:  	 Batch:207, loss:0.040366146713495255
Message from 0:  	 Batch:208, loss:0.04139441251754761
Message from 0:  	 Batch:209, loss:0.04840731620788574
Message from 0:  	 Batch:210, loss:0.07052655518054962
Message from 0:  	 Batch:211, loss:0.037357818335294724
Message from 0:  	 Batch:212, loss:0.06426827609539032
Message from 0:  	 Batch:213, loss:0.07232528924942017
Message from 0:  	 Batch:214, loss:0.04129457473754883
Message from 0:  	 Batch:215, loss:0.03134792298078537
Message from 0:  	 Batch:216, loss:0.039514362812042236
Message from 0:  	 Batch:217, loss:0.058047372847795486
Message from 0:  	 Batch:218, loss:0.03816917538642883
Message from 0:  	 Batch:219, loss:0.049092847853899
Message from 0:  	 Batch:220, loss:0.03997517377138138
Message from 0:  	 Batch:221, loss:0.05366627871990204
Message from 0:  	 Batch:222, loss:0.028041772544384003
Message from 0:  	 Batch:223, loss:0.03619860112667084
Message from 0:  	 Batch:224, loss:0.03757498413324356
Message from 0:  	 Batch:225, loss:0.043284907937049866
Message from 0:  	 Batch:226, loss:0.03727709501981735
Message from 0:  	 Batch:227, loss:0.04379645362496376
Message from 0:  	 Batch:228, loss:0.038982003927230835
Message from 0:  	 Batch:229, loss:0.0351695753633976
Message from 0:  	 Batch:230, loss:0.03273885324597359
Message from 0:  	 Batch:231, loss:0.054804883897304535
Message from 0:  	 Batch:232, loss:0.033091191202402115
Message from 0:  	 Batch:233, loss:0.07167994230985641
Message from 0:  	 Batch:234, loss:0.03431834653019905
Message from 0:  	 Batch:235, loss:0.03971949219703674
Message from 0:  	 Batch:236, loss:0.057144321501255035
Message from 0:  	 Batch:237, loss:0.0556463897228241
Message from 0:  	 Batch:238, loss:0.046944111585617065
Message from 0:  	 Batch:239, loss:0.04289134964346886
Message from 0:  	 Batch:240, loss:0.035901669412851334
Message from 0:  	 Batch:241, loss:0.0315825529396534
Message from 0:  	 Batch:242, loss:0.04064392298460007
Message from 0:  	 Batch:243, loss:0.047732964158058167
Message from 0:  	 Batch:244, loss:0.05773770064115524
Message from 0:  	 Batch:245, loss:0.04996864125132561
Message from 0:  	 Batch:246, loss:0.04265465587377548
Message from 0:  	 Batch:247, loss:0.03849193453788757
Message from 0:  	 Batch:248, loss:0.04419100657105446
Message from 0:  	 Batch:249, loss:0.040778957307338715
Message from 0:  	 Batch:250, loss:0.03562943637371063
Message from 0:  	 Batch:251, loss:0.04396601393818855
Message from 0:  	 Batch:252, loss:0.054249897599220276
Message from 0:  	 Batch:253, loss:0.047497376799583435
Message from 0:  	 Batch:254, loss:0.047110386192798615
Message from 0:  	 Batch:255, loss:0.044861119240522385
Message from 0:  	 Batch:256, loss:0.03842442110180855
Message from 0:  	 Batch:257, loss:0.03758455067873001
Message from 0:  	 Batch:258, loss:0.03763499855995178
Message from 0:  	 Batch:259, loss:0.04010654240846634
Message from 0:  	 Batch:260, loss:0.038659751415252686
Message from 0:  	 Batch:261, loss:0.04905565083026886
Message from 0:  	 Batch:262, loss:0.04251323640346527
Message from 0:  	 Batch:263, loss:0.04351530224084854
Message from 0:  	 Batch:264, loss:0.03905470296740532
Message from 0:  	 Batch:265, loss:0.04191555082798004
Message from 0:  	 Batch:266, loss:0.03464115038514137
Message from 0:  	 Batch:267, loss:0.03352872282266617
Message from 0:  	 Batch:268, loss:0.03985369950532913
Message from 0:  	 Batch:269, loss:0.042948201298713684
Message from 0:  	 Batch:270, loss:0.037882715463638306
Message from 0:  	 Batch:271, loss:0.04464087635278702
Message from 0:  	 Batch:272, loss:0.036525893956422806
Message from 0:  	 Batch:273, loss:0.053791821002960205
Message from 0:  	 Batch:274, loss:0.077314093708992
Message from 0:  	 Batch:275, loss:0.03266112506389618
Message from 0:  	 Batch:276, loss:0.04209834337234497
Message from 0:  	 Batch:277, loss:0.04268576577305794
Message from 0:  	 Batch:278, loss:0.03865600749850273
Message from 0:  	 Batch:279, loss:0.042535506188869476
Message from 0:  	 Batch:280, loss:0.054238460958004
Message from 0:  	 Batch:281, loss:0.0476931631565094
Message from 0:  	 Batch:282, loss:0.03583373501896858
Message from 0:  	 Batch:283, loss:0.06442650407552719
Message from 0:  	 Batch:284, loss:0.06127719581127167
Message from 0:  	 Batch:285, loss:0.05536844581365585
Message from 0:  	 Batch:286, loss:0.05011569708585739
Message from 0:  	 Batch:287, loss:0.09092473983764648
Message from 0:  	 Batch:288, loss:0.04172302782535553
Message from 0:  	 Batch:289, loss:0.0396469384431839
Message from 0:  	 Batch:290, loss:0.03715839236974716
Message from 0:  	 Batch:291, loss:0.041265204548835754
Message from 0:  	 Batch:292, loss:0.052530065178871155
Message from 0:  	 Batch:293, loss:0.03528280556201935
Message from 0:  	 Batch:294, loss:0.03810928761959076
Message from 0:  	 Batch:295, loss:0.041228342801332474
Message from 0:  	 Batch:296, loss:0.054184891283512115
Message from 0:  	 Batch:297, loss:0.037849344313144684
Message from 0:  	 Batch:298, loss:0.04764861240983009
Message from 0:  	 Batch:299, loss:0.029688529670238495
Message from 0:  	 Batch:300, loss:0.042524345219135284
Message from 0:  	 Batch:301, loss:0.05657026171684265
Message from 0:  	 Batch:302, loss:0.04175332188606262
Message from 0:  	 Batch:303, loss:0.04078575596213341
Message from 0:  	 Batch:304, loss:0.043008338660001755
Message from 0:  	 Batch:305, loss:0.033601343631744385
Message from 0:  	 Batch:306, loss:0.04417406767606735
Message from 0:  	 Batch:307, loss:0.06779681146144867
Message from 0:  	 Batch:308, loss:0.03424622118473053
Message from 0:  	 Batch:309, loss:0.03466757386922836
Message from 0:  	 Batch:310, loss:0.05355530232191086
Message from 0:  	 Batch:311, loss:0.03610242158174515
Message from 0:  	 Batch:312, loss:0.06339684128761292
Message from 0:  	 Batch:313, loss:0.0313124880194664
Message from 0:  	 Batch:314, loss:0.038689784705638885
Message from 0:  	 Batch:315, loss:0.04148830845952034
Message from 0:  	 Batch:316, loss:0.05276172608137131
Message from 0:  	 Batch:317, loss:0.046286702156066895
Message from 0:  	 Batch:318, loss:0.033080365508794785
Message from 0:  	 Batch:319, loss:0.052320219576358795
Message from 0:  	 Batch:320, loss:0.03296756371855736
Message from 0:  	 Batch:321, loss:0.036960288882255554
Message from 0:  	 Batch:322, loss:0.03367169201374054
Message from 0:  	 Batch:323, loss:0.03358083963394165
Message from 0:  	 Batch:324, loss:0.04159180074930191
Message from 0:  	 Batch:325, loss:0.040439173579216
Message from 0:  	 Batch:326, loss:0.03380488231778145
Message from 0:  	 Batch:327, loss:0.03828787803649902
Message from 0:  	 Batch:328, loss:0.03277036175131798
Message from 0:  	 Batch:329, loss:0.03181357681751251
Message from 0:  	 Batch:330, loss:0.03948149085044861
Message from 0:  	 Batch:331, loss:0.03431626409292221
Message from 0:  	 Batch:332, loss:0.03920910507440567
Message from 0:  	 Batch:333, loss:0.05385980010032654
Message from 0:  	 Batch:334, loss:0.03889354690909386
Message from 0:  	 Batch:335, loss:0.035119593143463135
Message from 0:  	 Batch:336, loss:0.08862975239753723
Message from 0:  	 Batch:337, loss:0.041534245014190674
Message from 0:  	 Batch:338, loss:0.045291587710380554
Message from 0:  	 Batch:339, loss:0.03739789500832558
Message from 0:  	 Batch:340, loss:0.044535573571920395
Message from 0:  	 Batch:341, loss:0.04501190781593323
Message from 0:  	 Batch:342, loss:0.037546802312135696
Message from 0:  	 Batch:343, loss:0.05261889100074768
Message from 0:  	 Batch:344, loss:0.039460957050323486
Message from 0:  	 Batch:345, loss:0.03576871007680893
Message from 0:  	 Batch:346, loss:0.03512140363454819
Message from 0:  	 Batch:347, loss:0.039454735815525055
Message from 0:  	 Batch:348, loss:0.039898864924907684
Message from 0:  	 Batch:349, loss:0.03364931792020798
Message from 0:  	 Batch:350, loss:0.03271971642971039
Message from 0:  	 Batch:351, loss:0.04059553146362305
Message from 0:  	 Batch:352, loss:0.03970526158809662
Message from 0:  	 Batch:353, loss:0.041535884141922
Message from 0:  	 Batch:354, loss:0.035984840244054794
Message from 0:  	 Batch:355, loss:0.0377536341547966
Message from 0:  	 Batch:356, loss:0.03402014449238777
Message from 0:  	 Batch:357, loss:0.054009318351745605
Message from 0:  	 Batch:358, loss:0.03251130133867264
Message from 0:  	 Batch:359, loss:0.05710183084011078
Message from 0:  	 Batch:360, loss:0.03397897630929947
Message from 0:  	 Batch:361, loss:0.0562162846326828
Message from 0:  	 Batch:362, loss:0.03897562250494957
Message from 0:  	 Batch:363, loss:0.041805729269981384
Message from 0:  	 Batch:364, loss:0.039706505835056305
Message from 0:  	 Batch:365, loss:0.04122759774327278
Message from 0:  	 Batch:366, loss:0.04427192360162735
Message from 0:  	 Batch:367, loss:0.04561406746506691
Message from 0:  	 Batch:368, loss:0.0479225218296051
Message from 0:  	 Batch:369, loss:0.04440680891275406
Message from 0:  	 Batch:370, loss:0.04478829726576805
Message from 0:  	 Batch:371, loss:0.04766687750816345
Message from 0:  	 Batch:372, loss:0.03937423974275589
Message from 0:  	 Batch:373, loss:0.03829151391983032
Message from 0:  	 Batch:374, loss:0.04480969160795212
Message from 0:  	 Batch:375, loss:0.04158632084727287
Message from 0:  	 Batch:376, loss:0.0374772772192955
Message from 0:  	 Batch:377, loss:0.056481823325157166
Message from 0:  	 Batch:378, loss:0.052552174776792526
Message from 0:  	 Batch:379, loss:0.04063647240400314
Message from 0:  	 Batch:380, loss:0.04704243317246437
Message from 0:  	 Batch:381, loss:0.07708302140235901
Message from 0:  	 Batch:382, loss:0.03520631417632103
Message from 0:  	 Batch:383, loss:0.038488518446683884
Message from 0:  	 Batch:384, loss:0.04190072417259216
Message from 0:  	 Batch:385, loss:0.03473396971821785
Message from 0:  	 Batch:386, loss:0.04165210202336311
Message from 0:  	 Batch:387, loss:0.04312684386968613
Message from 0:  	 Batch:388, loss:0.040613263845443726
Message from 0:  	 Batch:389, loss:0.041407112032175064
Message from 0:  	 Batch:390, loss:0.03746739774942398
Message from 0:  	 Batch:391, loss:0.03583921492099762
Message from 0:  	 Batch:392, loss:0.045788973569869995
Message from 0:  	 Batch:393, loss:0.03613545000553131
Message from 0:  	 Batch:394, loss:0.0331818088889122
Message from 0:  	 Batch:395, loss:0.02779821678996086
Message from 0:  	 Batch:396, loss:0.06238574534654617
Message from 0:  	 Batch:397, loss:0.0395936593413353
Message from 0:  	 Batch:398, loss:0.02953849732875824
Message from 0:  	 Batch:399, loss:0.0450344979763031
Message from 0:  	 Batch:400, loss:0.03275488317012787
Message from 0:  	 Batch:401, loss:0.04298835247755051
Message from 0:  	 Batch:402, loss:0.03974492847919464
Message from 0:  	 Batch:403, loss:0.0538148432970047
Message from 0:  	 Batch:404, loss:0.0361391082406044
Message from 0:  	 Batch:405, loss:0.03485406935214996
Message from 0:  	 Batch:406, loss:0.036512576043605804
Message from 0:  	 Batch:407, loss:0.06448601931333542
Message from 0:  	 Batch:408, loss:0.07092326879501343
Message from 0:  	 Batch:409, loss:0.02809656411409378
Message from 0:  	 Batch:410, loss:0.04241001605987549
Message from 0:  	 Batch:411, loss:0.04288952425122261
Message from 0:  	 Batch:412, loss:0.03891193866729736
Message from 0:  	 Batch:413, loss:0.05187545716762543
Message from 0:  	 Batch:414, loss:0.038493260741233826
Message from 0:  	 Batch:415, loss:0.035459499806165695
Message from 0:  	 Batch:416, loss:0.05443394184112549
Message from 0:  	 Batch:417, loss:0.029971688985824585
Message from 0:  	 Batch:418, loss:0.03326261788606644
Message from 0:  	 Batch:419, loss:0.034158747643232346
Message from 0:  	 Batch:420, loss:0.038687847554683685
Message from 0:  	 Batch:421, loss:0.032990165054798126
Message from 0:  	 Batch:422, loss:0.03284222260117531
Message from 0:  	 Batch:423, loss:0.03884028643369675
Message from 0:  	 Batch:424, loss:0.04680262506008148
Message from 0:  	 Batch:425, loss:0.04381684213876724
Message from 0:  	 Batch:426, loss:0.038065921515226364
Message from 0:  	 Batch:427, loss:0.03407524526119232
Message from 0:  	 Batch:428, loss:0.04486098140478134
Message from 0:  	 Batch:429, loss:0.062429651618003845
Message from 0:  	 Batch:430, loss:0.03028140962123871
Message from 0:  	 Batch:431, loss:0.03790803253650665
Message from 0:  	 Batch:432, loss:0.0551922544836998
Message from 0:  	 Batch:433, loss:0.04754644259810448
Message from 0:  	 Batch:434, loss:0.03910290449857712
Message from 0:  	 Batch:435, loss:0.032901838421821594
Message from 0:  	 Batch:436, loss:0.05129730701446533
Message from 0:  	 Batch:437, loss:0.034064680337905884
Message from 0:  	 Batch:438, loss:0.04731576889753342
Message from 0:  	 Batch:439, loss:0.036291807889938354
Message from 0:  	 Batch:440, loss:0.037242963910102844
Message from 0:  	 Batch:441, loss:0.05873668193817139
Message from 0:  	 Batch:442, loss:0.05570114776492119
Message from 0:  	 Batch:443, loss:0.04440364986658096
Message from 0:  	 Batch:444, loss:0.04297982156276703
Message from 0:  	 Batch:445, loss:0.03680471330881119
Message from 0:  	 Batch:446, loss:0.03395818918943405
Message from 0:  	 Batch:447, loss:0.07061568647623062
Message from 0:  	 Batch:448, loss:0.05060349777340889
Message from 0:  	 Batch:449, loss:0.04404665529727936
Message from 0:  	 Batch:450, loss:0.039914119988679886
Message from 0:  	 Batch:451, loss:0.030164934694767
Message from 0:  	 Batch:452, loss:0.039310477674007416
Message from 0:  	 Batch:453, loss:0.04294516146183014
Message from 0:  	 Batch:454, loss:0.04842773824930191
Message from 0:  	 Batch:455, loss:0.0318756103515625
Message from 0:  	 Batch:456, loss:0.03813759610056877
Message from 0:  	 Batch:457, loss:0.05817929655313492
Message from 0:  	 Batch:458, loss:0.03787197172641754
Message from 0:  	 Batch:459, loss:0.04311469942331314
Message from 0:  	 Batch:460, loss:0.03475039079785347
Message from 0:  	 Batch:461, loss:0.039795078337192535
Message from 0:  	 Batch:462, loss:0.06602878868579865
Message from 0:  	 Batch:463, loss:0.05022986978292465
Message from 0:  	 Batch:464, loss:0.0346052423119545
Message from 0:  	 Batch:465, loss:0.033186327666044235
Message from 0:  	 Batch:466, loss:0.03308985382318497
Message from 0:  	 Batch:467, loss:0.04951152950525284
Message from 0:  	 Batch:468, loss:0.03309869021177292
Message from 0:  	 Batch:469, loss:0.03817734867334366
Message from 0:  	 Batch:470, loss:0.06691623479127884
Message from 0:  	 Batch:471, loss:0.053168728947639465
Message from 0:  	 Batch:472, loss:0.04549284279346466
Message from 0:  	 Batch:473, loss:0.02951681800186634
Message from 0:  	 Batch:474, loss:0.039369143545627594
Message from 0:  	 Batch:475, loss:0.04223554953932762
Message from 0:  	 Batch:476, loss:0.03605888783931732
Message from 0:  	 Batch:477, loss:0.03991246223449707
Message from 0:  	 Batch:478, loss:0.03484850376844406
Message from 0:  	 Batch:479, loss:0.03721855208277702
Message from 0:  	 Batch:480, loss:0.043087199330329895
Message from 0:  	 Batch:481, loss:0.03700318932533264
Message from 0:  	 Batch:482, loss:0.051474668085575104
Message from 0:  	 Batch:483, loss:0.035471342504024506
Message from 0:  	 Batch:484, loss:0.08764638751745224
Message from 0:  	 Batch:485, loss:0.040086835622787476
Message from 0:  	 Batch:486, loss:0.044090285897254944
Message from 0:  	 Batch:487, loss:0.03720015287399292
Message from 0:  	 Batch:488, loss:0.030874162912368774
Message from 0:  	 Batch:489, loss:0.03878848999738693
Message from 0:  	 Batch:490, loss:0.0414775051176548
Message from 0:  	 Batch:491, loss:0.06091543659567833
Message from 0:  	 Batch:492, loss:0.04117726534605026
Message from 0:  	 Batch:493, loss:0.035965535789728165
Message from 0:  	 Batch:494, loss:0.0460408478975296
Message from 0:  	 Batch:495, loss:0.042048223316669464
Message from 0:  	 Batch:496, loss:0.05359259620308876
Message from 0:  	 Batch:497, loss:0.05211213231086731
Message from 0:  	 Batch:498, loss:0.04267387464642525
Message from 1:  	 Epoch:3, process-local average loss:0.04423179950724753
Message from 0:  	 Batch:499, loss:0.04165475815534592
Message from 0:  	 Epoch:3, process-local average loss:0.04252477982906356
Message from 0:  	 Batch:0, loss:0.04732225835323334
Message from 0:  	 Batch:1, loss:0.03708643466234207
Message from 0:  	 Batch:2, loss:0.045948222279548645
Message from 0:  	 Batch:3, loss:0.04404481500387192
Message from 0:  	 Batch:4, loss:0.03860175237059593
Message from 0:  	 Batch:5, loss:0.03484521433711052
Message from 0:  	 Batch:6, loss:0.030548807233572006
Message from 0:  	 Batch:7, loss:0.03301866352558136
Message from 0:  	 Batch:8, loss:0.0713140219449997
Message from 0:  	 Batch:9, loss:0.04396044462919235
Message from 0:  	 Batch:10, loss:0.04061494767665863
Message from 0:  	 Batch:11, loss:0.030711039900779724
Message from 0:  	 Batch:12, loss:0.038607217371463776
Message from 0:  	 Batch:13, loss:0.046664971858263016
Message from 0:  	 Batch:14, loss:0.03954920917749405
Message from 0:  	 Batch:15, loss:0.042065881192684174
Message from 0:  	 Batch:16, loss:0.03148854151368141
Message from 0:  	 Batch:17, loss:0.03735165297985077
Message from 0:  	 Batch:18, loss:0.06760790944099426
Message from 0:  	 Batch:19, loss:0.03825019299983978
Message from 0:  	 Batch:20, loss:0.04128555953502655
Message from 0:  	 Batch:21, loss:0.044571541249752045
Message from 0:  	 Batch:22, loss:0.032680898904800415
Message from 0:  	 Batch:23, loss:0.03177567571401596
Message from 0:  	 Batch:24, loss:0.04774951934814453
Message from 0:  	 Batch:25, loss:0.05619461461901665
Message from 0:  	 Batch:26, loss:0.04133183881640434
Message from 0:  	 Batch:27, loss:0.04176703840494156
Message from 0:  	 Batch:28, loss:0.04419224336743355
Message from 0:  	 Batch:29, loss:0.04009905830025673
Message from 0:  	 Batch:30, loss:0.05150195583701134
Message from 0:  	 Batch:31, loss:0.03423589468002319
Message from 0:  	 Batch:32, loss:0.04169600456953049
Message from 0:  	 Batch:33, loss:0.03641142696142197
Message from 0:  	 Batch:34, loss:0.03290079906582832
Message from 0:  	 Batch:35, loss:0.060504697263240814
Message from 0:  	 Batch:36, loss:0.04721752181649208
Message from 0:  	 Batch:37, loss:0.03063439577817917
Message from 0:  	 Batch:38, loss:0.04184010997414589
Message from 0:  	 Batch:39, loss:0.05330333113670349
Message from 0:  	 Batch:40, loss:0.04279608279466629
Message from 0:  	 Batch:41, loss:0.061865925788879395
Message from 0:  	 Batch:42, loss:0.04215455800294876
Message from 0:  	 Batch:43, loss:0.03948592767119408
Message from 0:  	 Batch:44, loss:0.03850951045751572
Message from 0:  	 Batch:45, loss:0.05330856889486313
Message from 0:  	 Batch:46, loss:0.034103550016880035
Message from 0:  	 Batch:47, loss:0.049691226333379745
Message from 0:  	 Batch:48, loss:0.038224902004003525
Message from 0:  	 Batch:49, loss:0.043924883008003235
Message from 0:  	 Batch:50, loss:0.040272973477840424
Message from 0:  	 Batch:51, loss:0.03794735297560692
Message from 0:  	 Batch:52, loss:0.06729546934366226
Message from 0:  	 Batch:53, loss:0.065775565803051
Message from 0:  	 Batch:54, loss:0.03759711980819702
Message from 0:  	 Batch:55, loss:0.04785802587866783
Message from 0:  	 Batch:56, loss:0.03523075953125954
Message from 0:  	 Batch:57, loss:0.037420012056827545
Message from 0:  	 Batch:58, loss:0.04081222787499428
Message from 0:  	 Batch:59, loss:0.052943021059036255
Message from 0:  	 Batch:60, loss:0.03038618341088295
Message from 0:  	 Batch:61, loss:0.045958876609802246
Message from 0:  	 Batch:62, loss:0.042403366416692734
Message from 0:  	 Batch:63, loss:0.03706683963537216
Message from 0:  	 Batch:64, loss:0.03551498427987099
Message from 0:  	 Batch:65, loss:0.044268205761909485
Message from 0:  	 Batch:66, loss:0.03744141757488251
Message from 0:  	 Batch:67, loss:0.03016100451350212
Message from 0:  	 Batch:68, loss:0.089565709233284
Message from 0:  	 Batch:69, loss:0.0310867540538311
Message from 0:  	 Batch:70, loss:0.043023042380809784
Message from 0:  	 Batch:71, loss:0.04159638658165932
Message from 0:  	 Batch:72, loss:0.06008445471525192
Message from 0:  	 Batch:73, loss:0.036697808653116226
Message from 0:  	 Batch:74, loss:0.044420138001441956
Message from 0:  	 Batch:75, loss:0.04363084211945534
Message from 0:  	 Batch:76, loss:0.03963254764676094
Message from 0:  	 Batch:77, loss:0.0377214252948761
Message from 0:  	 Batch:78, loss:0.042397305369377136
Message from 0:  	 Batch:79, loss:0.03144686296582222
Message from 0:  	 Batch:80, loss:0.041651368141174316
Message from 0:  	 Batch:81, loss:0.10085366666316986
Message from 0:  	 Batch:82, loss:0.07989569008350372
Message from 0:  	 Batch:83, loss:0.0465904176235199
Message from 0:  	 Batch:84, loss:0.035046279430389404
Message from 0:  	 Batch:85, loss:0.03848857805132866
Message from 0:  	 Batch:86, loss:0.03661002963781357
Message from 0:  	 Batch:87, loss:0.06173034384846687
Message from 0:  	 Batch:88, loss:0.0604565367102623
Message from 0:  	 Batch:89, loss:0.038859132677316666
Message from 0:  	 Batch:90, loss:0.03739365190267563
Message from 0:  	 Batch:91, loss:0.03730397671461105
Message from 0:  	 Batch:92, loss:0.04177488386631012
Message from 0:  	 Batch:93, loss:0.03602986037731171
Message from 0:  	 Batch:94, loss:0.05036269128322601
Message from 0:  	 Batch:95, loss:0.041332680732011795
Message from 0:  	 Batch:96, loss:0.037762969732284546
Message from 0:  	 Batch:97, loss:0.07992532104253769
Message from 0:  	 Batch:98, loss:0.042122095823287964
Message from 0:  	 Batch:99, loss:0.037344783544540405
Message from 0:  	 Batch:100, loss:0.04934646189212799
Message from 0:  	 Batch:101, loss:0.037743475288152695
Message from 0:  	 Batch:102, loss:0.046034883707761765
Message from 0:  	 Batch:103, loss:0.03477700054645538
Message from 0:  	 Batch:104, loss:0.03386932238936424
Message from 0:  	 Batch:105, loss:0.04331148788332939
Message from 0:  	 Batch:106, loss:0.04633408784866333
Message from 0:  	 Batch:107, loss:0.039604417979717255
Message from 0:  	 Batch:108, loss:0.042682163417339325
Message from 0:  	 Batch:109, loss:0.0535023957490921
Message from 0:  	 Batch:110, loss:0.03430670499801636
Message from 0:  	 Batch:111, loss:0.04017695412039757
Message from 0:  	 Batch:112, loss:0.051489487290382385
Message from 0:  	 Batch:113, loss:0.038280293345451355
Message from 0:  	 Batch:114, loss:0.03966651111841202
Message from 0:  	 Batch:115, loss:0.042631328105926514
Message from 0:  	 Batch:116, loss:0.04290936887264252
Message from 0:  	 Batch:117, loss:0.038666702806949615
Message from 0:  	 Batch:118, loss:0.04365924745798111
Message from 0:  	 Batch:119, loss:0.04202543571591377
Message from 0:  	 Batch:120, loss:0.04542739689350128
Message from 0:  	 Batch:121, loss:0.03408900648355484
Message from 0:  	 Batch:122, loss:0.0530383437871933
Message from 0:  	 Batch:123, loss:0.04022476077079773
Message from 0:  	 Batch:124, loss:0.05512690544128418
Message from 0:  	 Batch:125, loss:0.03776011988520622
Message from 0:  	 Batch:126, loss:0.04036751389503479
Message from 0:  	 Batch:127, loss:0.04437755420804024
Message from 0:  	 Batch:128, loss:0.058097679167985916
Message from 0:  	 Batch:129, loss:0.05000389367341995
Message from 0:  	 Batch:130, loss:0.03485332801938057
Message from 0:  	 Batch:131, loss:0.03680252283811569
Message from 0:  	 Batch:132, loss:0.03956135734915733
Message from 0:  	 Batch:133, loss:0.036579519510269165
Message from 0:  	 Batch:134, loss:0.06410840898752213
Message from 0:  	 Batch:135, loss:0.03378479555249214
Message from 0:  	 Batch:136, loss:0.05271809548139572
Message from 0:  	 Batch:137, loss:0.03480175882577896
Message from 0:  	 Batch:138, loss:0.042097534984350204
Message from 0:  	 Batch:139, loss:0.036323949694633484
Message from 0:  	 Batch:140, loss:0.08767929673194885
Message from 0:  	 Batch:141, loss:0.049105897545814514
Message from 0:  	 Batch:142, loss:0.04149724543094635
Message from 0:  	 Batch:143, loss:0.04354575276374817
Message from 0:  	 Batch:144, loss:0.03631167113780975
Message from 0:  	 Batch:145, loss:0.040719784796237946
Message from 0:  	 Batch:146, loss:0.03852009028196335
Message from 0:  	 Batch:147, loss:0.04024425894021988
Message from 0:  	 Batch:148, loss:0.03711911290884018
Message from 0:  	 Batch:149, loss:0.03721853345632553
Message from 0:  	 Batch:150, loss:0.03896686062216759
Message from 0:  	 Batch:151, loss:0.03142053633928299
Message from 0:  	 Batch:152, loss:0.04010606184601784
Message from 0:  	 Batch:153, loss:0.03469616919755936
Message from 0:  	 Batch:154, loss:0.035451799631118774
Message from 0:  	 Batch:155, loss:0.05479208379983902
Message from 0:  	 Batch:156, loss:0.04905691742897034
Message from 0:  	 Batch:157, loss:0.04293452948331833
Message from 0:  	 Batch:158, loss:0.042232856154441833
Message from 0:  	 Batch:159, loss:0.0402555987238884
Message from 0:  	 Batch:160, loss:0.03828466683626175
Message from 0:  	 Batch:161, loss:0.04764600098133087
Message from 0:  	 Batch:162, loss:0.03199032321572304
Message from 0:  	 Batch:163, loss:0.04158899933099747
Message from 0:  	 Batch:164, loss:0.0420319065451622
Message from 0:  	 Batch:165, loss:0.04474351182579994
Message from 0:  	 Batch:166, loss:0.04413791000843048
Message from 0:  	 Batch:167, loss:0.039106082171201706
Message from 0:  	 Batch:168, loss:0.07219173014163971
Message from 0:  	 Batch:169, loss:0.038370609283447266
Message from 0:  	 Batch:170, loss:0.03529809042811394
Message from 0:  	 Batch:171, loss:0.028533149510622025
Message from 0:  	 Batch:172, loss:0.054855115711688995
Message from 0:  	 Batch:173, loss:0.028226088732481003
Message from 0:  	 Batch:174, loss:0.051187410950660706
Message from 0:  	 Batch:175, loss:0.039467550814151764
Message from 0:  	 Batch:176, loss:0.034590743482112885
Message from 0:  	 Batch:177, loss:0.035287559032440186
Message from 0:  	 Batch:178, loss:0.0352134071290493
Message from 0:  	 Batch:179, loss:0.043680302798748016
Message from 0:  	 Batch:180, loss:0.03586877882480621
Message from 0:  	 Batch:181, loss:0.03544297069311142
Message from 0:  	 Batch:182, loss:0.03444374352693558
Message from 0:  	 Batch:183, loss:0.04079469293355942
Message from 0:  	 Batch:184, loss:0.04133845120668411
Message from 0:  	 Batch:185, loss:0.033849336206912994
Message from 0:  	 Batch:186, loss:0.0499253086745739
Message from 0:  	 Batch:187, loss:0.03703448176383972
Message from 0:  	 Batch:188, loss:0.03978315368294716
Message from 0:  	 Batch:189, loss:0.03437265753746033
Message from 0:  	 Batch:190, loss:0.05354350060224533
Message from 0:  	 Batch:191, loss:0.05571993067860603
Message from 0:  	 Batch:192, loss:0.042644426226615906
Message from 0:  	 Batch:193, loss:0.05495726317167282
Message from 0:  	 Batch:194, loss:0.0353267565369606
Message from 0:  	 Batch:195, loss:0.04793979972600937
Message from 0:  	 Batch:196, loss:0.030896101146936417
Message from 0:  	 Batch:197, loss:0.04323142021894455
Message from 0:  	 Batch:198, loss:0.03321012109518051
Message from 0:  	 Batch:199, loss:0.044195763766765594
Message from 0:  	 Batch:200, loss:0.049382273107767105
Message from 0:  	 Batch:201, loss:0.05721508711576462
Message from 0:  	 Batch:202, loss:0.038910023868083954
Message from 0:  	 Batch:203, loss:0.040563639253377914
Message from 0:  	 Batch:204, loss:0.029964253306388855
Message from 0:  	 Batch:205, loss:0.03943662345409393
Message from 0:  	 Batch:206, loss:0.03857395797967911
Message from 0:  	 Batch:207, loss:0.04022740572690964
Message from 0:  	 Batch:208, loss:0.041420988738536835
Message from 0:  	 Batch:209, loss:0.047623101621866226
Message from 0:  	 Batch:210, loss:0.07009992003440857
Message from 0:  	 Batch:211, loss:0.036789488047361374
Message from 0:  	 Batch:212, loss:0.06375924497842789
Message from 0:  	 Batch:213, loss:0.07227347046136856
Message from 0:  	 Batch:214, loss:0.04110002890229225
Message from 0:  	 Batch:215, loss:0.030829891562461853
Message from 0:  	 Batch:216, loss:0.039420805871486664
Message from 0:  	 Batch:217, loss:0.05732332170009613
Message from 0:  	 Batch:218, loss:0.03786901384592056
Message from 0:  	 Batch:219, loss:0.048318348824977875
Message from 0:  	 Batch:220, loss:0.04009365662932396
Message from 0:  	 Batch:221, loss:0.05313332378864288
Message from 0:  	 Batch:222, loss:0.02737250179052353
Message from 0:  	 Batch:223, loss:0.0359366238117218
Message from 0:  	 Batch:224, loss:0.037436433136463165
Message from 0:  	 Batch:225, loss:0.04315686225891113
Message from 0:  	 Batch:226, loss:0.03686513006687164
Message from 0:  	 Batch:227, loss:0.04365250840783119
Message from 0:  	 Batch:228, loss:0.03927641361951828
Message from 0:  	 Batch:229, loss:0.034722134470939636
Message from 0:  	 Batch:230, loss:0.03238518163561821
Message from 0:  	 Batch:231, loss:0.054309695959091187
Message from 0:  	 Batch:232, loss:0.032705921679735184
Message from 0:  	 Batch:233, loss:0.07098662853240967
Message from 0:  	 Batch:234, loss:0.0345819927752018
Message from 0:  	 Batch:235, loss:0.03964066877961159
Message from 0:  	 Batch:236, loss:0.056247077882289886
Message from 0:  	 Batch:237, loss:0.05533839762210846
Message from 0:  	 Batch:238, loss:0.046492524445056915
Message from 0:  	 Batch:239, loss:0.04274522513151169
Message from 0:  	 Batch:240, loss:0.03564827889204025
Message from 0:  	 Batch:241, loss:0.03179876133799553
Message from 0:  	 Batch:242, loss:0.04057532548904419
Message from 0:  	 Batch:243, loss:0.04708804562687874
Message from 0:  	 Batch:244, loss:0.05723346024751663
Message from 0:  	 Batch:245, loss:0.049359604716300964
Message from 0:  	 Batch:246, loss:0.042652152478694916
Message from 0:  	 Batch:247, loss:0.038436949253082275
Message from 0:  	 Batch:248, loss:0.044258080422878265
Message from 0:  	 Batch:249, loss:0.040666546672582626
Message from 0:  	 Batch:250, loss:0.0350654162466526
Message from 0:  	 Batch:251, loss:0.04362615942955017
Message from 0:  	 Batch:252, loss:0.05342552065849304
Message from 0:  	 Batch:253, loss:0.04735875874757767
Message from 0:  	 Batch:254, loss:0.046630859375
Message from 0:  	 Batch:255, loss:0.04415886104106903
Message from 0:  	 Batch:256, loss:0.03825332969427109
Message from 0:  	 Batch:257, loss:0.037610456347465515
Message from 0:  	 Batch:258, loss:0.0370890311896801
Message from 0:  	 Batch:259, loss:0.04025137424468994
Message from 0:  	 Batch:260, loss:0.038586925715208054
Message from 0:  	 Batch:261, loss:0.048848412930965424
Message from 0:  	 Batch:262, loss:0.042107634246349335
Message from 0:  	 Batch:263, loss:0.04358646273612976
Message from 0:  	 Batch:264, loss:0.038704607635736465
Message from 0:  	 Batch:265, loss:0.04120545834302902
Message from 0:  	 Batch:266, loss:0.0348348394036293
Message from 0:  	 Batch:267, loss:0.03331264108419418
Message from 0:  	 Batch:268, loss:0.03971317410469055
Message from 0:  	 Batch:269, loss:0.04300995171070099
Message from 0:  	 Batch:270, loss:0.037863194942474365
Message from 0:  	 Batch:271, loss:0.04440420866012573
Message from 0:  	 Batch:272, loss:0.03641431778669357
Message from 0:  	 Batch:273, loss:0.053242120891809464
Message from 0:  	 Batch:274, loss:0.07757259905338287
Message from 0:  	 Batch:275, loss:0.03241219371557236
Message from 0:  	 Batch:276, loss:0.04207216948270798
Message from 0:  	 Batch:277, loss:0.042275939136743546
Message from 0:  	 Batch:278, loss:0.03873730078339577
Message from 0:  	 Batch:279, loss:0.04242240637540817
Message from 0:  	 Batch:280, loss:0.05331623554229736
Message from 0:  	 Batch:281, loss:0.04743989557027817
Message from 0:  	 Batch:282, loss:0.035597726702690125
Message from 0:  	 Batch:283, loss:0.06335841119289398
Message from 0:  	 Batch:284, loss:0.0608157142996788
Message from 0:  	 Batch:285, loss:0.05501988157629967
Message from 0:  	 Batch:286, loss:0.04977802932262421
Message from 0:  	 Batch:287, loss:0.08969157934188843
Message from 0:  	 Batch:288, loss:0.041329529136419296
Message from 0:  	 Batch:289, loss:0.03863687068223953
Message from 0:  	 Batch:290, loss:0.03714756295084953
Message from 0:  	 Batch:291, loss:0.04098986089229584
Message from 0:  	 Batch:292, loss:0.052038274705410004
Message from 0:  	 Batch:293, loss:0.03536578267812729
Message from 0:  	 Batch:294, loss:0.03811350464820862
Message from 0:  	 Batch:295, loss:0.041401397436857224
Message from 0:  	 Batch:296, loss:0.05354725569486618
Message from 0:  	 Batch:297, loss:0.03785484656691551
Message from 0:  	 Batch:298, loss:0.04701848328113556
Message from 0:  	 Batch:299, loss:0.029956866055727005
Message from 0:  	 Batch:300, loss:0.042584601789712906
Message from 0:  	 Batch:301, loss:0.05561007186770439
Message from 0:  	 Batch:302, loss:0.04169794172048569
Message from 0:  	 Batch:303, loss:0.04081988334655762
Message from 0:  	 Batch:304, loss:0.04291033744812012
Message from 0:  	 Batch:305, loss:0.033140718936920166
Message from 0:  	 Batch:306, loss:0.04364065080881119
Message from 0:  	 Batch:307, loss:0.06770487874746323
Message from 0:  	 Batch:308, loss:0.034536249935626984
Message from 0:  	 Batch:309, loss:0.03458873927593231
Message from 0:  	 Batch:310, loss:0.052266675978899
Message from 0:  	 Batch:311, loss:0.03588340803980827
Message from 0:  	 Batch:312, loss:0.06314949691295624
Message from 0:  	 Batch:313, loss:0.031217185780405998
Message from 0:  	 Batch:314, loss:0.038731615990400314
Message from 0:  	 Batch:315, loss:0.040679071098566055
Message from 0:  	 Batch:316, loss:0.052625082433223724
Message from 0:  	 Batch:317, loss:0.04571540281176567
Message from 0:  	 Batch:318, loss:0.03262634575366974
Message from 0:  	 Batch:319, loss:0.0515773743391037
Message from 0:  	 Batch:320, loss:0.03298727795481682
Message from 0:  	 Batch:321, loss:0.03679672256112099
Message from 0:  	 Batch:322, loss:0.03371288627386093
Message from 0:  	 Batch:323, loss:0.03358934819698334
Message from 0:  	 Batch:324, loss:0.041692301630973816
Message from 0:  	 Batch:325, loss:0.04047194868326187
Message from 0:  	 Batch:326, loss:0.034149445593357086
Message from 0:  	 Batch:327, loss:0.03798087686300278
Message from 0:  	 Batch:328, loss:0.03288079798221588
Message from 0:  	 Batch:329, loss:0.03155031427741051
Message from 0:  	 Batch:330, loss:0.03920608386397362
Message from 0:  	 Batch:331, loss:0.0346476249396801
Message from 0:  	 Batch:332, loss:0.03923889249563217
Message from 0:  	 Batch:333, loss:0.0531960092484951
Message from 0:  	 Batch:334, loss:0.0388324037194252
Message from 0:  	 Batch:335, loss:0.03515850007534027
Message from 0:  	 Batch:336, loss:0.0889844000339508
Message from 0:  	 Batch:337, loss:0.041543349623680115
Message from 0:  	 Batch:338, loss:0.04384770616889
Message from 0:  	 Batch:339, loss:0.03742092847824097
Message from 0:  	 Batch:340, loss:0.04436200112104416
Message from 0:  	 Batch:341, loss:0.044925373047590256
Message from 0:  	 Batch:342, loss:0.03725972771644592
Message from 0:  	 Batch:343, loss:0.052595023065805435
Message from 0:  	 Batch:344, loss:0.039393529295921326
Message from 0:  	 Batch:345, loss:0.035236015915870667
Message from 0:  	 Batch:346, loss:0.034705013036727905
Message from 0:  	 Batch:347, loss:0.039496228098869324
Message from 0:  	 Batch:348, loss:0.03997378051280975
Message from 0:  	 Batch:349, loss:0.03335883840918541
Message from 0:  	 Batch:350, loss:0.032698001712560654
Message from 0:  	 Batch:351, loss:0.04072645306587219
Message from 0:  	 Batch:352, loss:0.039700038731098175
Message from 0:  	 Batch:353, loss:0.04141397774219513
Message from 0:  	 Batch:354, loss:0.03564855456352234
Message from 0:  	 Batch:355, loss:0.03753526136279106
Message from 0:  	 Batch:356, loss:0.03400940075516701
Message from 0:  	 Batch:357, loss:0.05334678292274475
Message from 0:  	 Batch:358, loss:0.03343488276004791
Message from 0:  	 Batch:359, loss:0.05642346292734146
Message from 0:  	 Batch:360, loss:0.03384959697723389
Message from 0:  	 Batch:361, loss:0.05604809150099754
Message from 0:  	 Batch:362, loss:0.03909037262201309
Message from 0:  	 Batch:363, loss:0.041792191565036774
Message from 0:  	 Batch:364, loss:0.039215341210365295
Message from 0:  	 Batch:365, loss:0.041276127099990845
Message from 0:  	 Batch:366, loss:0.04423178732395172
Message from 0:  	 Batch:367, loss:0.04538755863904953
Message from 0:  	 Batch:368, loss:0.04757138341665268
Message from 0:  	 Batch:369, loss:0.04329826310276985
Message from 0:  	 Batch:370, loss:0.044579584151506424
Message from 0:  	 Batch:371, loss:0.046905383467674255
Message from 0:  	 Batch:372, loss:0.03929784148931503
Message from 0:  	 Batch:373, loss:0.038373515009880066
Message from 0:  	 Batch:374, loss:0.04425511881709099
Message from 0:  	 Batch:375, loss:0.04160696268081665
Message from 0:  	 Batch:376, loss:0.03748936206102371
Message from 0:  	 Batch:377, loss:0.05555809289216995
Message from 0:  	 Batch:378, loss:0.052177794277668
Message from 0:  	 Batch:379, loss:0.04038965329527855
Message from 0:  	 Batch:380, loss:0.04615850746631622
Message from 0:  	 Batch:381, loss:0.0765223279595375
Message from 0:  	 Batch:382, loss:0.03485005348920822
Message from 0:  	 Batch:383, loss:0.03837290406227112
Message from 0:  	 Batch:384, loss:0.04188411310315132
Message from 0:  	 Batch:385, loss:0.03433350846171379
Message from 0:  	 Batch:386, loss:0.041647326201200485
Message from 0:  	 Batch:387, loss:0.0425398126244545
Message from 0:  	 Batch:388, loss:0.03991428017616272
Message from 0:  	 Batch:389, loss:0.04136653244495392
Message from 0:  	 Batch:390, loss:0.03766530752182007
Message from 0:  	 Batch:391, loss:0.03586440905928612
Message from 0:  	 Batch:392, loss:0.04557717218995094
Message from 0:  	 Batch:393, loss:0.03612343221902847
Message from 0:  	 Batch:394, loss:0.03409424424171448
Message from 0:  	 Batch:395, loss:0.028860311955213547
Message from 0:  	 Batch:396, loss:0.06088196486234665
Message from 0:  	 Batch:397, loss:0.03954651579260826
Message from 0:  	 Batch:398, loss:0.029588352888822556
Message from 0:  	 Batch:399, loss:0.04446186125278473
Message from 0:  	 Batch:400, loss:0.032658353447914124
Message from 0:  	 Batch:401, loss:0.042796261608600616
Message from 0:  	 Batch:402, loss:0.03969778120517731
Message from 0:  	 Batch:403, loss:0.053758397698402405
Message from 0:  	 Batch:404, loss:0.035428233444690704
Message from 0:  	 Batch:405, loss:0.034247592091560364
Message from 0:  	 Batch:406, loss:0.03598498925566673
Message from 0:  	 Batch:407, loss:0.06395605951547623
Message from 0:  	 Batch:408, loss:0.07134909182786942
Message from 0:  	 Batch:409, loss:0.026674635708332062
Message from 0:  	 Batch:410, loss:0.042320653796195984
Message from 0:  	 Batch:411, loss:0.04311203211545944
Message from 0:  	 Batch:412, loss:0.039030492305755615
Message from 0:  	 Batch:413, loss:0.05144880339503288
Message from 0:  	 Batch:414, loss:0.03804805874824524
Message from 0:  	 Batch:415, loss:0.03490953892469406
Message from 0:  	 Batch:416, loss:0.05370943248271942
Message from 0:  	 Batch:417, loss:0.029479380697011948
Message from 0:  	 Batch:418, loss:0.03315291553735733
Message from 0:  	 Batch:419, loss:0.03413574770092964
Message from 0:  	 Batch:420, loss:0.038613393902778625
Message from 0:  	 Batch:421, loss:0.03272123262286186
Message from 0:  	 Batch:422, loss:0.03265323489904404
Message from 0:  	 Batch:423, loss:0.038645725697278976
Message from 0:  	 Batch:424, loss:0.04669363796710968
Message from 0:  	 Batch:425, loss:0.04340669512748718
Message from 0:  	 Batch:426, loss:0.03801053389906883
Message from 0:  	 Batch:427, loss:0.0339222252368927
Message from 0:  	 Batch:428, loss:0.04425029084086418
Message from 0:  	 Batch:429, loss:0.061685964465141296
Message from 0:  	 Batch:430, loss:0.030440617352724075
Message from 0:  	 Batch:431, loss:0.03796060383319855
Message from 0:  	 Batch:432, loss:0.05480758473277092
Message from 0:  	 Batch:433, loss:0.047527290880680084
Message from 0:  	 Batch:434, loss:0.039199840277433395
Message from 0:  	 Batch:435, loss:0.03265516459941864
Message from 0:  	 Batch:436, loss:0.05058134347200394
Message from 0:  	 Batch:437, loss:0.03347502648830414
Message from 0:  	 Batch:438, loss:0.046993084251880646
Message from 0:  	 Batch:439, loss:0.036050114780664444
Message from 0:  	 Batch:440, loss:0.037339773029088974
Message from 0:  	 Batch:441, loss:0.058086544275283813
Message from 0:  	 Batch:442, loss:0.05465240776538849
Message from 0:  	 Batch:443, loss:0.04416491091251373
Message from 0:  	 Batch:444, loss:0.042859748005867004
Message from 0:  	 Batch:445, loss:0.03689561411738396
Message from 0:  	 Batch:446, loss:0.03377441689372063
Message from 0:  	 Batch:447, loss:0.07053042203187943
Message from 0:  	 Batch:448, loss:0.0501832515001297
Message from 0:  	 Batch:449, loss:0.044011153280735016
Message from 0:  	 Batch:450, loss:0.03990586847066879
Message from 0:  	 Batch:451, loss:0.02995361015200615
Message from 0:  	 Batch:452, loss:0.03920583799481392
Message from 0:  	 Batch:453, loss:0.04282320290803909
Message from 0:  	 Batch:454, loss:0.04774680733680725
Message from 0:  	 Batch:455, loss:0.03098626434803009
Message from 0:  	 Batch:456, loss:0.03829929232597351
Message from 0:  	 Batch:457, loss:0.057328514754772186
Message from 0:  	 Batch:458, loss:0.03741852566599846
Message from 0:  	 Batch:459, loss:0.043147727847099304
Message from 0:  	 Batch:460, loss:0.034857768565416336
Message from 0:  	 Batch:461, loss:0.03976967930793762
Message from 0:  	 Batch:462, loss:0.06609061360359192
Message from 0:  	 Batch:463, loss:0.04982135072350502
Message from 0:  	 Batch:464, loss:0.03379085287451744
Message from 0:  	 Batch:465, loss:0.03302377462387085
Message from 0:  	 Batch:466, loss:0.03299000859260559
Message from 0:  	 Batch:467, loss:0.04872819036245346
Message from 0:  	 Batch:468, loss:0.03302960842847824
Message from 0:  	 Batch:469, loss:0.0380343422293663
Message from 0:  	 Batch:470, loss:0.06604620814323425
Message from 0:  	 Batch:471, loss:0.05222124606370926
Message from 0:  	 Batch:472, loss:0.04402580112218857
Message from 0:  	 Batch:473, loss:0.028908777981996536
Message from 0:  	 Batch:474, loss:0.03933722525835037
Message from 0:  	 Batch:475, loss:0.04170198738574982
Message from 0:  	 Batch:476, loss:0.03618517518043518
Message from 0:  	 Batch:477, loss:0.0397152379155159
Message from 0:  	 Batch:478, loss:0.03478952869772911
Message from 0:  	 Batch:479, loss:0.03702551871538162
Message from 0:  	 Batch:480, loss:0.04295315593481064
Message from 0:  	 Batch:481, loss:0.0370318777859211
Message from 0:  	 Batch:482, loss:0.05090542882680893
Message from 0:  	 Batch:483, loss:0.0358070507645607
Message from 0:  	 Batch:484, loss:0.0869949609041214
Message from 0:  	 Batch:485, loss:0.03992953151464462
Message from 0:  	 Batch:486, loss:0.043511562049388885
Message from 0:  	 Batch:487, loss:0.03719325363636017
Message from 0:  	 Batch:488, loss:0.030373942106962204
Message from 0:  	 Batch:489, loss:0.03869409114122391
Message from 0:  	 Batch:490, loss:0.04144866392016411
Message from 0:  	 Batch:491, loss:0.060745999217033386
Message from 0:  	 Batch:492, loss:0.04115302115678787
Message from 0:  	 Batch:493, loss:0.03514718636870384
Message from 0:  	 Batch:494, loss:0.04573093354701996
Message from 0:  	 Batch:495, loss:0.042080312967300415
Message from 0:  	 Batch:496, loss:0.05317822843790054
Message from 0:  	 Batch:497, loss:0.05160755291581154
Message from 0:  	 Batch:498, loss:0.04285658895969391
Message from 1:  	 Epoch:4, process-local average loss:0.043886413956859216
Message from 0:  	 Batch:499, loss:0.04170067608356476
Message from 0:  	 Epoch:4, process-local average loss:0.042277493638119557
Message from 0:  	 Batch:0, loss:0.05005251616239548
Message from 0:  	 Batch:1, loss:0.04126386344432831
Message from 0:  	 Batch:2, loss:0.047301486134529114
Message from 0:  	 Batch:3, loss:0.0465240404009819
Message from 0:  	 Batch:4, loss:0.04036109149456024
Message from 0:  	 Batch:5, loss:0.039413485676050186
Message from 0:  	 Batch:6, loss:0.03468573838472366
Message from 0:  	 Batch:7, loss:0.03745604306459427
Message from 0:  	 Batch:8, loss:0.07413699477910995
Message from 0:  	 Batch:9, loss:0.04616578668355942
Message from 0:  	 Batch:10, loss:0.04253587871789932
Message from 0:  	 Batch:11, loss:0.034564368426799774
Message from 0:  	 Batch:12, loss:0.04352305829524994
Message from 0:  	 Batch:13, loss:0.05021156370639801
Message from 0:  	 Batch:14, loss:0.04293522983789444
Message from 0:  	 Batch:15, loss:0.04605219513177872
Message from 0:  	 Batch:16, loss:0.036963365972042084
Message from 0:  	 Batch:17, loss:0.04122605174779892
Message from 0:  	 Batch:18, loss:0.06872092187404633
Message from 0:  	 Batch:19, loss:0.04009981080889702
Message from 0:  	 Batch:20, loss:0.0439598485827446
Message from 0:  	 Batch:21, loss:0.047498904168605804
Message from 0:  	 Batch:22, loss:0.03583858907222748
Message from 0:  	 Batch:23, loss:0.03526376187801361
Message from 0:  	 Batch:24, loss:0.048187293112277985
Message from 0:  	 Batch:25, loss:0.06069405749440193
Message from 0:  	 Batch:26, loss:0.043309979140758514
Message from 0:  	 Batch:27, loss:0.04232378304004669
Message from 0:  	 Batch:28, loss:0.045489441603422165
Message from 0:  	 Batch:29, loss:0.043119870126247406
Message from 0:  	 Batch:30, loss:0.05570849031209946
Message from 0:  	 Batch:31, loss:0.03895276039838791
Message from 0:  	 Batch:32, loss:0.04247496649622917
Message from 0:  	 Batch:33, loss:0.04111158102750778
Message from 0:  	 Batch:34, loss:0.0376356802880764
Message from 0:  	 Batch:35, loss:0.06337589770555496
Message from 0:  	 Batch:36, loss:0.0488322451710701
Message from 0:  	 Batch:37, loss:0.03314904123544693
Message from 0:  	 Batch:38, loss:0.04479862004518509
Message from 0:  	 Batch:39, loss:0.05751965939998627
Message from 0:  	 Batch:40, loss:0.04478316754102707
Message from 0:  	 Batch:41, loss:0.06481637060642242
Message from 0:  	 Batch:42, loss:0.043636858463287354
Message from 0:  	 Batch:43, loss:0.04172789305448532
Message from 0:  	 Batch:44, loss:0.04187414050102234
Message from 0:  	 Batch:45, loss:0.05542171373963356
Message from 0:  	 Batch:46, loss:0.0393868163228035
Message from 0:  	 Batch:47, loss:0.05183758959174156
Message from 0:  	 Batch:48, loss:0.04272086173295975
Message from 0:  	 Batch:49, loss:0.046242937445640564
Message from 0:  	 Batch:50, loss:0.04018864408135414
Message from 0:  	 Batch:51, loss:0.04040968418121338
Message from 0:  	 Batch:52, loss:0.07151640951633453
Message from 0:  	 Batch:53, loss:0.06921137869358063
Message from 0:  	 Batch:54, loss:0.03987854719161987
Message from 0:  	 Batch:55, loss:0.05127348750829697
Message from 0:  	 Batch:56, loss:0.039969898760318756
Message from 0:  	 Batch:57, loss:0.041876427829265594
Message from 0:  	 Batch:58, loss:0.04179759323596954
Message from 0:  	 Batch:59, loss:0.054737161844968796
Message from 0:  	 Batch:60, loss:0.03831642121076584
Message from 0:  	 Batch:61, loss:0.049271464347839355
Message from 0:  	 Batch:62, loss:0.04371969774365425
Message from 0:  	 Batch:63, loss:0.0395677424967289
Message from 0:  	 Batch:64, loss:0.038481637835502625
Message from 0:  	 Batch:65, loss:0.04784630611538887
Message from 0:  	 Batch:66, loss:0.04064235836267471
Message from 0:  	 Batch:67, loss:0.03188763186335564
Message from 0:  	 Batch:68, loss:0.09046216309070587
Message from 0:  	 Batch:69, loss:0.03529071807861328
Message from 0:  	 Batch:70, loss:0.04559262841939926
Message from 0:  	 Batch:71, loss:0.04362975060939789
Message from 0:  	 Batch:72, loss:0.06509021669626236
Message from 0:  	 Batch:73, loss:0.042719826102256775
Message from 0:  	 Batch:74, loss:0.05137749761343002
Message from 0:  	 Batch:75, loss:0.04348268359899521
Message from 0:  	 Batch:76, loss:0.04278024286031723
Message from 0:  	 Batch:77, loss:0.040569670498371124
Message from 0:  	 Batch:78, loss:0.04380542039871216
Message from 0:  	 Batch:79, loss:0.033810701221227646
Message from 0:  	 Batch:80, loss:0.04431581497192383
Message from 0:  	 Batch:81, loss:0.10449250787496567
Message from 0:  	 Batch:82, loss:0.08299306035041809
Message from 0:  	 Batch:83, loss:0.04915151745080948
Message from 0:  	 Batch:84, loss:0.0381910540163517
Message from 0:  	 Batch:85, loss:0.04396028071641922
Message from 0:  	 Batch:86, loss:0.042760513722896576
Message from 0:  	 Batch:87, loss:0.06432028859853745
Message from 0:  	 Batch:88, loss:0.06176534295082092
Message from 0:  	 Batch:89, loss:0.04206894338130951
Message from 0:  	 Batch:90, loss:0.038573745638132095
Message from 0:  	 Batch:91, loss:0.03804397210478783
Message from 0:  	 Batch:92, loss:0.04506440460681915
Message from 0:  	 Batch:93, loss:0.03902379423379898
Message from 0:  	 Batch:94, loss:0.057792630046606064
Message from 0:  	 Batch:95, loss:0.04128626361489296
Message from 0:  	 Batch:96, loss:0.041397616267204285
Message from 0:  	 Batch:97, loss:0.08235521614551544
Message from 0:  	 Batch:98, loss:0.044169820845127106
Message from 0:  	 Batch:99, loss:0.039416760206222534
Message from 0:  	 Batch:100, loss:0.04994668439030647
Message from 0:  	 Batch:101, loss:0.04139136150479317
Message from 0:  	 Batch:102, loss:0.04887455329298973
Message from 0:  	 Batch:103, loss:0.040494874119758606
Message from 0:  	 Batch:104, loss:0.03493516519665718
Message from 0:  	 Batch:105, loss:0.04588010907173157
Message from 0:  	 Batch:106, loss:0.049313291907310486
Message from 0:  	 Batch:107, loss:0.0400693416595459
Message from 0:  	 Batch:108, loss:0.04437842220067978
Message from 0:  	 Batch:109, loss:0.059203896671533585
Message from 0:  	 Batch:110, loss:0.03811046481132507
Message from 0:  	 Batch:111, loss:0.04546801745891571
Message from 0:  	 Batch:112, loss:0.05390195921063423
Message from 0:  	 Batch:113, loss:0.03950995206832886
Message from 0:  	 Batch:114, loss:0.043231941759586334
Message from 0:  	 Batch:115, loss:0.04315517470240593
Message from 0:  	 Batch:116, loss:0.04226646199822426
Message from 0:  	 Batch:117, loss:0.04406359791755676
Message from 0:  	 Batch:118, loss:0.04510337859392166
Message from 0:  	 Batch:119, loss:0.04482895880937576
Message from 0:  	 Batch:120, loss:0.04760782793164253
Message from 0:  	 Batch:121, loss:0.04035510867834091
Message from 0:  	 Batch:122, loss:0.05297736078500748
Message from 0:  	 Batch:123, loss:0.04127257689833641
Message from 0:  	 Batch:124, loss:0.057408154010772705
Message from 0:  	 Batch:125, loss:0.038640640676021576
Message from 0:  	 Batch:126, loss:0.041405823081731796
Message from 0:  	 Batch:127, loss:0.046972669661045074
Message from 0:  	 Batch:128, loss:0.06094835698604584
Message from 0:  	 Batch:129, loss:0.051388248801231384
Message from 0:  	 Batch:130, loss:0.03901664912700653
Message from 0:  	 Batch:131, loss:0.04184460639953613
Message from 0:  	 Batch:132, loss:0.04223659634590149
Message from 0:  	 Batch:133, loss:0.039843738079071045
Message from 0:  	 Batch:134, loss:0.06544853001832962
Message from 0:  	 Batch:135, loss:0.03887837380170822
Message from 0:  	 Batch:136, loss:0.05218034237623215
Message from 0:  	 Batch:137, loss:0.037927158176898956
Message from 0:  	 Batch:138, loss:0.04565666615962982
Message from 0:  	 Batch:139, loss:0.034668922424316406
Message from 0:  	 Batch:140, loss:0.09048429131507874
Message from 0:  	 Batch:141, loss:0.049328237771987915
Message from 0:  	 Batch:142, loss:0.042758334428071976
Message from 0:  	 Batch:143, loss:0.04520722106099129
Message from 0:  	 Batch:144, loss:0.03873790055513382
Message from 0:  	 Batch:145, loss:0.042371541261672974
Message from 0:  	 Batch:146, loss:0.040299151092767715
Message from 0:  	 Batch:147, loss:0.042221106588840485
Message from 0:  	 Batch:148, loss:0.03834269940853119
Message from 0:  	 Batch:149, loss:0.039137423038482666
Message from 0:  	 Batch:150, loss:0.04117348790168762
Message from 0:  	 Batch:151, loss:0.033639319241046906
Message from 0:  	 Batch:152, loss:0.04277923330664635
Message from 0:  	 Batch:153, loss:0.03748715668916702
Message from 0:  	 Batch:154, loss:0.041189972311258316
Message from 0:  	 Batch:155, loss:0.056923963129520416
Message from 0:  	 Batch:156, loss:0.05068502575159073
Message from 0:  	 Batch:157, loss:0.04386995732784271
Message from 0:  	 Batch:158, loss:0.04519528150558472
Message from 0:  	 Batch:159, loss:0.04256238043308258
Message from 0:  	 Batch:160, loss:0.0414259135723114
Message from 0:  	 Batch:161, loss:0.050378747284412384
Message from 0:  	 Batch:162, loss:0.03750227764248848
Message from 0:  	 Batch:163, loss:0.043263889849185944
Message from 0:  	 Batch:164, loss:0.04511847347021103
Message from 0:  	 Batch:165, loss:0.04918989539146423
Message from 0:  	 Batch:166, loss:0.04527147859334946
Message from 0:  	 Batch:167, loss:0.042147114872932434
Message from 0:  	 Batch:168, loss:0.07680918276309967
Message from 0:  	 Batch:169, loss:0.042744699865579605
Message from 0:  	 Batch:170, loss:0.039997197687625885
Message from 0:  	 Batch:171, loss:0.03516732528805733
Message from 0:  	 Batch:172, loss:0.05826820433139801
Message from 0:  	 Batch:173, loss:0.035800524055957794
Message from 0:  	 Batch:174, loss:0.0520426444709301
Message from 0:  	 Batch:175, loss:0.04089565575122833
Message from 0:  	 Batch:176, loss:0.03824133053421974
Message from 0:  	 Batch:177, loss:0.04082723706960678
Message from 0:  	 Batch:178, loss:0.03829779475927353
Message from 0:  	 Batch:179, loss:0.045906901359558105
Message from 0:  	 Batch:180, loss:0.03986096382141113
Message from 0:  	 Batch:181, loss:0.04092877730727196
Message from 0:  	 Batch:182, loss:0.03914092108607292
Message from 0:  	 Batch:183, loss:0.03928271681070328
Message from 0:  	 Batch:184, loss:0.043152257800102234
Message from 0:  	 Batch:185, loss:0.037953149527311325
Message from 0:  	 Batch:186, loss:0.05267921835184097
Message from 0:  	 Batch:187, loss:0.03919783979654312
Message from 0:  	 Batch:188, loss:0.04206157475709915
Message from 0:  	 Batch:189, loss:0.03932468220591545
Message from 0:  	 Batch:190, loss:0.061356380581855774
Message from 0:  	 Batch:191, loss:0.056792765855789185
Message from 0:  	 Batch:192, loss:0.04885093867778778
Message from 0:  	 Batch:193, loss:0.060192447155714035
Message from 0:  	 Batch:194, loss:0.03839750215411186
Message from 0:  	 Batch:195, loss:0.0506221167743206
Message from 0:  	 Batch:196, loss:0.03209203481674194
Message from 0:  	 Batch:197, loss:0.04623761028051376
Message from 0:  	 Batch:198, loss:0.0401429757475853
Message from 0:  	 Batch:199, loss:0.047099534422159195
Message from 0:  	 Batch:200, loss:0.0592498853802681
Message from 0:  	 Batch:201, loss:0.05712512508034706
Message from 0:  	 Batch:202, loss:0.04086863994598389
Message from 0:  	 Batch:203, loss:0.04241905361413956
Message from 0:  	 Batch:204, loss:0.03450613468885422
Message from 0:  	 Batch:205, loss:0.040510714054107666
Message from 0:  	 Batch:206, loss:0.04119686037302017
Message from 0:  	 Batch:207, loss:0.04098708927631378
Message from 0:  	 Batch:208, loss:0.041883647441864014
Message from 0:  	 Batch:209, loss:0.04896041378378868
Message from 0:  	 Batch:210, loss:0.07809769362211227
Message from 0:  	 Batch:211, loss:0.04164518415927887
Message from 0:  	 Batch:212, loss:0.06292404979467392
Message from 0:  	 Batch:213, loss:0.07498325407505035
Message from 0:  	 Batch:214, loss:0.04405108839273453
Message from 0:  	 Batch:215, loss:0.037560611963272095
Message from 0:  	 Batch:216, loss:0.04635016992688179
Message from 0:  	 Batch:217, loss:0.05744282156229019
Message from 0:  	 Batch:218, loss:0.042561501264572144
Message from 0:  	 Batch:219, loss:0.04864916577935219
Message from 0:  	 Batch:220, loss:0.044877588748931885
Message from 0:  	 Batch:221, loss:0.05840175598859787
Message from 0:  	 Batch:222, loss:0.031153012067079544
Message from 0:  	 Batch:223, loss:0.039151549339294434
Message from 0:  	 Batch:224, loss:0.0416947565972805
Message from 0:  	 Batch:225, loss:0.047048043459653854
Message from 0:  	 Batch:226, loss:0.0390961728990078
Message from 0:  	 Batch:227, loss:0.045303333550691605
Message from 0:  	 Batch:228, loss:0.040180012583732605
Message from 0:  	 Batch:229, loss:0.038012370467185974
Message from 0:  	 Batch:230, loss:0.035737939178943634
Message from 0:  	 Batch:231, loss:0.05786343291401863
Message from 0:  	 Batch:232, loss:0.03796626999974251
Message from 0:  	 Batch:233, loss:0.07530459761619568
Message from 0:  	 Batch:234, loss:0.03889775276184082
Message from 0:  	 Batch:235, loss:0.041089244186878204
Message from 0:  	 Batch:236, loss:0.05985255539417267
Message from 0:  	 Batch:237, loss:0.06187552213668823
Message from 0:  	 Batch:238, loss:0.04844330996274948
Message from 0:  	 Batch:239, loss:0.04349859803915024
Message from 0:  	 Batch:240, loss:0.038650788366794586
Message from 0:  	 Batch:241, loss:0.03962927311658859
Message from 0:  	 Batch:242, loss:0.04378613829612732
Message from 0:  	 Batch:243, loss:0.04862816259264946
Message from 0:  	 Batch:244, loss:0.06041513383388519
Message from 0:  	 Batch:245, loss:0.04847845807671547
Message from 0:  	 Batch:246, loss:0.04416649043560028
Message from 0:  	 Batch:247, loss:0.04188227653503418
Message from 0:  	 Batch:248, loss:0.046384550631046295
Message from 0:  	 Batch:249, loss:0.04174213111400604
Message from 0:  	 Batch:250, loss:0.04010938107967377
Message from 0:  	 Batch:251, loss:0.0443185493350029
Message from 0:  	 Batch:252, loss:0.053835779428482056
Message from 0:  	 Batch:253, loss:0.05078227072954178
Message from 0:  	 Batch:254, loss:0.04820423200726509
Message from 0:  	 Batch:255, loss:0.04832593724131584
Message from 0:  	 Batch:256, loss:0.04184907674789429
Message from 0:  	 Batch:257, loss:0.04056202620267868
Message from 0:  	 Batch:258, loss:0.037639468908309937
Message from 0:  	 Batch:259, loss:0.040894877165555954
Message from 0:  	 Batch:260, loss:0.04085961729288101
Message from 0:  	 Batch:261, loss:0.04985911399126053
Message from 0:  	 Batch:262, loss:0.0455436147749424
Message from 0:  	 Batch:263, loss:0.04362556338310242
Message from 0:  	 Batch:264, loss:0.042609915137290955
Message from 0:  	 Batch:265, loss:0.045906927436590195
Message from 0:  	 Batch:266, loss:0.037652648985385895
Message from 0:  	 Batch:267, loss:0.03884299099445343
Message from 0:  	 Batch:268, loss:0.04164343327283859
Message from 0:  	 Batch:269, loss:0.04331529513001442
Message from 0:  	 Batch:270, loss:0.04206894338130951
Message from 0:  	 Batch:271, loss:0.0472593829035759
Message from 0:  	 Batch:272, loss:0.040598638355731964
Message from 0:  	 Batch:273, loss:0.0529506579041481
Message from 0:  	 Batch:274, loss:0.07927126437425613
Message from 0:  	 Batch:275, loss:0.033407486975193024
Message from 0:  	 Batch:276, loss:0.04394499957561493
Message from 0:  	 Batch:277, loss:0.04526715353131294
Message from 0:  	 Batch:278, loss:0.04067366570234299
Message from 0:  	 Batch:279, loss:0.04613257199525833
Message from 0:  	 Batch:280, loss:0.054665692150592804
Message from 0:  	 Batch:281, loss:0.049040138721466064
Message from 0:  	 Batch:282, loss:0.03610144928097725
Message from 0:  	 Batch:283, loss:0.06406285613775253
Message from 0:  	 Batch:284, loss:0.06543230265378952
Message from 0:  	 Batch:285, loss:0.05762679502367973
Message from 0:  	 Batch:286, loss:0.05124286189675331
Message from 0:  	 Batch:287, loss:0.09037964791059494
Message from 0:  	 Batch:288, loss:0.04440481960773468
Message from 0:  	 Batch:289, loss:0.03875548392534256
Message from 0:  	 Batch:290, loss:0.0393868163228035
Message from 0:  	 Batch:291, loss:0.04263706132769585
Message from 0:  	 Batch:292, loss:0.05588490515947342
Message from 0:  	 Batch:293, loss:0.03837946802377701
Message from 0:  	 Batch:294, loss:0.04183424264192581
Message from 0:  	 Batch:295, loss:0.04122143238782883
Message from 0:  	 Batch:296, loss:0.056424155831336975
Message from 0:  	 Batch:297, loss:0.04051747918128967
Message from 0:  	 Batch:298, loss:0.04799138754606247
Message from 0:  	 Batch:299, loss:0.03222076594829559
Message from 0:  	 Batch:300, loss:0.04654021933674812
Message from 0:  	 Batch:301, loss:0.0602262020111084
Message from 0:  	 Batch:302, loss:0.04331447556614876
Message from 0:  	 Batch:303, loss:0.043469734489917755
Message from 0:  	 Batch:304, loss:0.04308943450450897
Message from 0:  	 Batch:305, loss:0.038868498057127
Message from 0:  	 Batch:306, loss:0.04555239528417587
Message from 0:  	 Batch:307, loss:0.06948451697826385
Message from 0:  	 Batch:308, loss:0.033739663660526276
Message from 0:  	 Batch:309, loss:0.03536659851670265
Message from 0:  	 Batch:310, loss:0.05370836704969406
Message from 0:  	 Batch:311, loss:0.03890193998813629
Message from 0:  	 Batch:312, loss:0.06246176362037659
Message from 0:  	 Batch:313, loss:0.03401027247309685
Message from 0:  	 Batch:314, loss:0.03933790326118469
Message from 0:  	 Batch:315, loss:0.04480171576142311
Message from 0:  	 Batch:316, loss:0.05443399399518967
Message from 0:  	 Batch:317, loss:0.04575813561677933
Message from 0:  	 Batch:318, loss:0.03412581607699394
Message from 0:  	 Batch:319, loss:0.053880780935287476
Message from 0:  	 Batch:320, loss:0.03758286312222481
Message from 0:  	 Batch:321, loss:0.04188203066587448
Message from 0:  	 Batch:322, loss:0.03950023651123047
Message from 0:  	 Batch:323, loss:0.03807322308421135
Message from 0:  	 Batch:324, loss:0.04156769812107086
Message from 0:  	 Batch:325, loss:0.04070105403661728
Message from 0:  	 Batch:326, loss:0.03584807366132736
Message from 0:  	 Batch:327, loss:0.043097492307424545
Message from 0:  	 Batch:328, loss:0.03815777599811554
Message from 0:  	 Batch:329, loss:0.03169558569788933
Message from 0:  	 Batch:330, loss:0.04121340438723564
Message from 0:  	 Batch:331, loss:0.037540141493082047
Message from 0:  	 Batch:332, loss:0.04192841798067093
Message from 0:  	 Batch:333, loss:0.05639887601137161
Message from 0:  	 Batch:334, loss:0.042807623744010925
Message from 0:  	 Batch:335, loss:0.039538733661174774
Message from 0:  	 Batch:336, loss:0.08973532915115356
Message from 0:  	 Batch:337, loss:0.044325631111860275
Message from 0:  	 Batch:338, loss:0.0412081703543663
Message from 0:  	 Batch:339, loss:0.04257877171039581
Message from 0:  	 Batch:340, loss:0.04820664972066879
Message from 0:  	 Batch:341, loss:0.04868112877011299
Message from 0:  	 Batch:342, loss:0.04042600095272064
Message from 0:  	 Batch:343, loss:0.05647798627614975
Message from 0:  	 Batch:344, loss:0.040350355207920074
Message from 0:  	 Batch:345, loss:0.04008439928293228
Message from 0:  	 Batch:346, loss:0.03844491019845009
Message from 0:  	 Batch:347, loss:0.04340779036283493
Message from 0:  	 Batch:348, loss:0.04080238193273544
Message from 0:  	 Batch:349, loss:0.03481430560350418
Message from 0:  	 Batch:350, loss:0.037992801517248154
Message from 0:  	 Batch:351, loss:0.041843876242637634
Message from 0:  	 Batch:352, loss:0.04153859615325928
Message from 0:  	 Batch:353, loss:0.042633913457393646
Message from 0:  	 Batch:354, loss:0.03967323899269104
Message from 0:  	 Batch:355, loss:0.042771030217409134
Message from 0:  	 Batch:356, loss:0.03797551244497299
Message from 0:  	 Batch:357, loss:0.055259011685848236
Message from 0:  	 Batch:358, loss:0.0367865189909935
Message from 0:  	 Batch:359, loss:0.058994144201278687
Message from 0:  	 Batch:360, loss:0.04001539200544357
Message from 0:  	 Batch:361, loss:0.05969945341348648
Message from 0:  	 Batch:362, loss:0.041738174855709076
Message from 0:  	 Batch:363, loss:0.04406047612428665
Message from 0:  	 Batch:364, loss:0.04204285889863968
Message from 0:  	 Batch:365, loss:0.04371847212314606
Message from 0:  	 Batch:366, loss:0.044800445437431335
Message from 0:  	 Batch:367, loss:0.04755774140357971
Message from 0:  	 Batch:368, loss:0.04863691329956055
Message from 0:  	 Batch:369, loss:0.04443475604057312
Message from 0:  	 Batch:370, loss:0.04722572863101959
Message from 0:  	 Batch:371, loss:0.049932871013879776
Message from 0:  	 Batch:372, loss:0.04143088310956955
Message from 0:  	 Batch:373, loss:0.040750667452812195
Message from 0:  	 Batch:374, loss:0.046027377247810364
Message from 0:  	 Batch:375, loss:0.04229827970266342
Message from 0:  	 Batch:376, loss:0.040743496268987656
Message from 0:  	 Batch:377, loss:0.05905413627624512
Message from 0:  	 Batch:378, loss:0.05361461639404297
Message from 0:  	 Batch:379, loss:0.04316629841923714
Message from 0:  	 Batch:380, loss:0.05016240477561951
Message from 0:  	 Batch:381, loss:0.07764223963022232
Message from 0:  	 Batch:382, loss:0.03838562220335007
Message from 0:  	 Batch:383, loss:0.042067259550094604
Message from 0:  	 Batch:384, loss:0.04218655824661255
Message from 0:  	 Batch:385, loss:0.038536496460437775
Message from 0:  	 Batch:386, loss:0.042376890778541565
Message from 0:  	 Batch:387, loss:0.044580765068531036
Message from 0:  	 Batch:388, loss:0.039572976529598236
Message from 0:  	 Batch:389, loss:0.04451935738325119
Message from 0:  	 Batch:390, loss:0.040519021451473236
Message from 0:  	 Batch:391, loss:0.03916054591536522
Message from 0:  	 Batch:392, loss:0.04774068295955658
Message from 0:  	 Batch:393, loss:0.0399869829416275
Message from 0:  	 Batch:394, loss:0.03556383401155472
Message from 0:  	 Batch:395, loss:0.03377044200897217
Message from 0:  	 Batch:396, loss:0.06160532310605049
Message from 0:  	 Batch:397, loss:0.04173402488231659
Message from 0:  	 Batch:398, loss:0.03010541945695877
Message from 0:  	 Batch:399, loss:0.046130452305078506
Message from 0:  	 Batch:400, loss:0.03421427309513092
Message from 0:  	 Batch:401, loss:0.04881858080625534
Message from 0:  	 Batch:402, loss:0.04165804386138916
Message from 0:  	 Batch:403, loss:0.05608856678009033
Message from 0:  	 Batch:404, loss:0.03985714912414551
Message from 0:  	 Batch:405, loss:0.03866350278258324
Message from 0:  	 Batch:406, loss:0.03932566940784454
Message from 0:  	 Batch:407, loss:0.06388451159000397
Message from 0:  	 Batch:408, loss:0.0754927471280098
Message from 0:  	 Batch:409, loss:0.030807215720415115
Message from 0:  	 Batch:410, loss:0.04300738871097565
Message from 0:  	 Batch:411, loss:0.044365428388118744
Message from 0:  	 Batch:412, loss:0.041841983795166016
Message from 0:  	 Batch:413, loss:0.05655090510845184
Message from 0:  	 Batch:414, loss:0.04171713441610336
Message from 0:  	 Batch:415, loss:0.037562884390354156
Message from 0:  	 Batch:416, loss:0.05430363118648529
Message from 0:  	 Batch:417, loss:0.030183719471096992
Message from 0:  	 Batch:418, loss:0.03871804103255272
Message from 0:  	 Batch:419, loss:0.03935423493385315
Message from 0:  	 Batch:420, loss:0.04143812507390976
Message from 0:  	 Batch:421, loss:0.04008004441857338
Message from 0:  	 Batch:422, loss:0.03620763123035431
Message from 0:  	 Batch:423, loss:0.04019252210855484
Message from 0:  	 Batch:424, loss:0.048080772161483765
Message from 0:  	 Batch:425, loss:0.04784451425075531
Message from 0:  	 Batch:426, loss:0.03994479775428772
Message from 0:  	 Batch:427, loss:0.03950231522321701
Message from 0:  	 Batch:428, loss:0.044878292828798294
Message from 0:  	 Batch:429, loss:0.0635375827550888
Message from 0:  	 Batch:430, loss:0.03736289590597153
Message from 0:  	 Batch:431, loss:0.040636323392391205
Message from 0:  	 Batch:432, loss:0.05620864778757095
Message from 0:  	 Batch:433, loss:0.051966018974781036
Message from 0:  	 Batch:434, loss:0.04067595303058624
Message from 0:  	 Batch:435, loss:0.038048699498176575
Message from 0:  	 Batch:436, loss:0.05292388051748276
Message from 0:  	 Batch:437, loss:0.038319580256938934
Message from 0:  	 Batch:438, loss:0.04876303672790527
Message from 0:  	 Batch:439, loss:0.03900971636176109
Message from 0:  	 Batch:440, loss:0.03977096825838089
Message from 0:  	 Batch:441, loss:0.06241803243756294
Message from 0:  	 Batch:442, loss:0.05482377111911774
Message from 0:  	 Batch:443, loss:0.04573739692568779
Message from 0:  	 Batch:444, loss:0.04314161092042923
Message from 0:  	 Batch:445, loss:0.03931519389152527
Message from 0:  	 Batch:446, loss:0.03620394319295883
Message from 0:  	 Batch:447, loss:0.0754241794347763
Message from 0:  	 Batch:448, loss:0.05409156531095505
Message from 0:  	 Batch:449, loss:0.044291555881500244
Message from 0:  	 Batch:450, loss:0.041662685573101044
Message from 0:  	 Batch:451, loss:0.03745552524924278
Message from 0:  	 Batch:452, loss:0.042579859495162964
Message from 0:  	 Batch:453, loss:0.04281294718384743
Message from 0:  	 Batch:454, loss:0.05064235255122185
Message from 0:  	 Batch:455, loss:0.03583701699972153
Message from 0:  	 Batch:456, loss:0.04211755469441414
Message from 0:  	 Batch:457, loss:0.061772048473358154
Message from 0:  	 Batch:458, loss:0.039271749556064606
Message from 0:  	 Batch:459, loss:0.043786339461803436
Message from 0:  	 Batch:460, loss:0.0374298170208931
Message from 0:  	 Batch:461, loss:0.04422517120838165
Message from 0:  	 Batch:462, loss:0.070282943546772
Message from 0:  	 Batch:463, loss:0.05529698729515076
Message from 0:  	 Batch:464, loss:0.03282172232866287
Message from 0:  	 Batch:465, loss:0.033753592520952225
Message from 0:  	 Batch:466, loss:0.03735627606511116
Message from 0:  	 Batch:467, loss:0.052969321608543396
Message from 0:  	 Batch:468, loss:0.03858993202447891
Message from 0:  	 Batch:469, loss:0.040840908885002136
Message from 0:  	 Batch:470, loss:0.06886321306228638
Message from 0:  	 Batch:471, loss:0.05460613965988159
Message from 0:  	 Batch:472, loss:0.04797505587339401
Message from 0:  	 Batch:473, loss:0.0324133038520813
Message from 0:  	 Batch:474, loss:0.04306385666131973
Message from 0:  	 Batch:475, loss:0.046542681753635406
Message from 0:  	 Batch:476, loss:0.03867127373814583
Message from 0:  	 Batch:477, loss:0.041540421545505524
Message from 0:  	 Batch:478, loss:0.03971409797668457
Message from 0:  	 Batch:479, loss:0.040590204298496246
Message from 0:  	 Batch:480, loss:0.04434431716799736
Message from 0:  	 Batch:481, loss:0.04044988751411438
Message from 0:  	 Batch:482, loss:0.05407794564962387
Message from 0:  	 Batch:483, loss:0.03922693431377411
Message from 0:  	 Batch:484, loss:0.09085966646671295
Message from 0:  	 Batch:485, loss:0.04155348241329193
Message from 0:  	 Batch:486, loss:0.04752366244792938
Message from 0:  	 Batch:487, loss:0.039885781705379486
Message from 0:  	 Batch:488, loss:0.035021357238292694
Message from 0:  	 Batch:489, loss:0.04182048887014389
Message from 0:  	 Batch:490, loss:0.041578248143196106
Message from 0:  	 Batch:491, loss:0.06258036196231842
Message from 0:  	 Batch:492, loss:0.04132520779967308
Message from 0:  	 Batch:493, loss:0.03342814743518829
Message from 0:  	 Batch:494, loss:0.046414803713560104
Message from 0:  	 Batch:495, loss:0.0430910587310791
Message from 0:  	 Batch:496, loss:0.05579076707363129
Message from 0:  	 Batch:497, loss:0.05429563671350479
Message from 0:  	 Batch:498, loss:0.042960699647665024
Message from 1:  	 Epoch:5, process-local average loss:0.04634524678811431
Message from 0:  	 Batch:499, loss:0.042267605662345886
Message from 0:  	 Epoch:5, process-local average loss:0.045397139929234984
/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
Traceback (most recent call last):
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/userhome/cs/ericcsr/anaconda3/envs/taichi/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/algorithms/solve.py", line 112, in <module>
    main()
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/algorithms/solve.py", line 89, in main
    learn_latent_focal(args,loss_fn)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/optimizer/focal_learn_latent.py", line 331, in learn_latent_focal
    dataloader = _update_focal_scheme(original_dataset,1000)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/optimizer/focal_learn_latent.py", line 232, in _update_focal_scheme
    subdataset = dataset.getSubset(size)
  File "/userhome/cs/ericcsr/chensirui/PlasticineLab/plb/neurals/pcdataloader.py", line 50, in getSubset
    sub_target_x = self.target_x[subset_idx]
numpy.core._exceptions.MemoryError: Unable to allocate 1.28 GiB for an array with shape (1000, 7, 8192, 3) and data type float64
